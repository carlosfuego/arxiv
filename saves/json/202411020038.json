[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.24222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24222v1",
                "updated": "2024-10-31T17:59:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "Robust Gaussian Processes via Relevance Pursuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Gaussian Processes via Relevance Pursuit"
                },
                "summary": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range."
                },
                "authors": [
                    {
                        "name": "Sebastian Ament"
                    },
                    {
                        "name": "Elizabeth Santorella"
                    },
                    {
                        "name": "David Eriksson"
                    },
                    {
                        "name": "Ben Letham"
                    },
                    {
                        "name": "Maximilian Balandat"
                    },
                    {
                        "name": "Eytan Bakshy"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Bakshy"
                },
                "author": "Eytan Bakshy",
                "arxiv_comment": "NeurIPS 2024 Article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24223v1",
                "updated": "2024-10-31T17:59:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "URAvatar: Universal Relightable Gaussian Codec Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URAvatar: Universal Relightable Gaussian Codec Avatars"
                },
                "summary": "We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Chen Cao"
                    },
                    {
                        "name": "Gabriel Schwartz"
                    },
                    {
                        "name": "Rawal Khirodkar"
                    },
                    {
                        "name": "Christian Richardt"
                    },
                    {
                        "name": "Tomas Simon"
                    },
                    {
                        "name": "Yaser Sheikh"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "SIGGRAPH Asia 2024. Website:\n  https://junxuan-li.github.io/urgca-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24218v1",
                "updated": "2024-10-31T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    52,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use"
                },
                "summary": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL"
                },
                "authors": [
                    {
                        "name": "Jiajun Xi"
                    },
                    {
                        "name": "Yinong He"
                    },
                    {
                        "name": "Jianing Yang"
                    },
                    {
                        "name": "Yinpei Dai"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24207v1",
                "updated": "2024-10-31T17:58:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    58,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:58:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    58,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images"
                },
                "summary": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/."
                },
                "authors": [
                    {
                        "name": "Botao Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Haofei Xu"
                    },
                    {
                        "name": "Xueting Li"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Songyou Peng"
                    }
                ],
                "author_detail": {
                    "name": "Songyou Peng"
                },
                "author": "Songyou Peng",
                "arxiv_comment": "Project page: https://noposplat.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24199v1",
                "updated": "2024-10-31T17:55:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    27,
                    3,
                    305,
                    0
                ],
                "title": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation"
                },
                "summary": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes."
                },
                "authors": [
                    {
                        "name": "Mohamed Elgaar"
                    },
                    {
                        "name": "Hadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Hadi Amiri"
                },
                "author": "Hadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24198v1",
                "updated": "2024-10-31T17:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "SelfCodeAlign: Self-Alignment for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCodeAlign: Self-Alignment for Code Generation"
                },
                "summary": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Zachary Mueller"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24197v1",
                "updated": "2024-10-31T17:55:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:55:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "Generative modelling for mass-mapping with fast uncertainty\n  quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modelling for mass-mapping with fast uncertainty\n  quantification"
                },
                "summary": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second."
                },
                "authors": [
                    {
                        "name": "Jessica J. Whitney"
                    },
                    {
                        "name": "Tobías I. Liaudat"
                    },
                    {
                        "name": "Matthew A. Price"
                    },
                    {
                        "name": "Matthijs Mars"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24190v1",
                "updated": "2024-10-31T17:51:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"
                },
                "summary": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions."
                },
                "authors": [
                    {
                        "name": "Yujin Potter"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04312v2",
                "updated": "2024-10-31T17:47:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    47,
                    54,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-06T17:56:40Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    56,
                    40,
                    3,
                    158,
                    0
                ],
                "title": "ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise\n  Optimization"
                },
                "summary": "Text-to-Image (T2I) models have made significant advancements in recent\nyears, but they still struggle to accurately capture intricate details\nspecified in complex compositional prompts. While fine-tuning T2I models with\nreward objectives has shown promise, it suffers from \"reward hacking\" and may\nnot generalize well to unseen prompt distributions. In this work, we propose\nReward-based Noise Optimization (ReNO), a novel approach that enhances T2I\nmodels at inference by optimizing the initial noise based on the signal from\none or multiple human preference reward models. Remarkably, solving this\noptimization problem with gradient ascent for 50 iterations yields impressive\nresults on four different one-step models across two competitive benchmarks,\nT2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,\nReNO-enhanced one-step models consistently surpass the performance of all\ncurrent open-source Text-to-Image models. Extensive user studies demonstrate\nthat our model is preferred nearly twice as often compared to the popular SDXL\nmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.\nMoreover, given the same computational resources, a ReNO-optimized one-step\nmodel outperforms widely-used open-source models such as SDXL and\nPixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in\nenhancing T2I model performance at inference time. Code is available at\nhttps://github.com/ExplainableML/ReNO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) models have made significant advancements in recent\nyears, but they still struggle to accurately capture intricate details\nspecified in complex compositional prompts. While fine-tuning T2I models with\nreward objectives has shown promise, it suffers from \"reward hacking\" and may\nnot generalize well to unseen prompt distributions. In this work, we propose\nReward-based Noise Optimization (ReNO), a novel approach that enhances T2I\nmodels at inference by optimizing the initial noise based on the signal from\none or multiple human preference reward models. Remarkably, solving this\noptimization problem with gradient ascent for 50 iterations yields impressive\nresults on four different one-step models across two competitive benchmarks,\nT2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,\nReNO-enhanced one-step models consistently surpass the performance of all\ncurrent open-source Text-to-Image models. Extensive user studies demonstrate\nthat our model is preferred nearly twice as often compared to the popular SDXL\nmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.\nMoreover, given the same computational resources, a ReNO-optimized one-step\nmodel outperforms widely-used open-source models such as SDXL and\nPixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in\nenhancing T2I model performance at inference time. Code is available at\nhttps://github.com/ExplainableML/ReNO."
                },
                "authors": [
                    {
                        "name": "Luca Eyring"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Alexey Dosovitskiy"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24175v1",
                "updated": "2024-10-31T17:42:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:42:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13046v2",
                "updated": "2024-10-31T17:39:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    39,
                    34,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-19T17:59:48Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    17,
                    59,
                    48,
                    4,
                    110,
                    0
                ],
                "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context"
                },
                "summary": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24169v1",
                "updated": "2024-10-31T17:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    35,
                    57,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    35,
                    57,
                    3,
                    305,
                    0
                ],
                "title": "The Importance of Being Scalable: Improving the Speed and Accuracy of\n  Neural Network Interatomic Potentials Across Chemical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Importance of Being Scalable: Improving the Speed and Accuracy of\n  Neural Network Interatomic Potentials Across Chemical Domains"
                },
                "summary": "Scaling has been critical in improving model performance and generalization\nin machine learning. It involves how a model's performance changes with\nincreases in model size or input data, as well as how efficiently computational\nresources are utilized to support this growth. Despite successes in other\nareas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)\nremains limited. NNIPs act as surrogate models for ab initio quantum mechanical\ncalculations. The dominant paradigm here is to incorporate many physical domain\nconstraints into the model, such as rotational equivariance. We contend that\nthese complex constraints inhibit the scaling ability of NNIPs, and are likely\nto lead to performance plateaus in the long run. In this work, we take an\nalternative approach and start by systematically studying NNIP scaling\nstrategies. Our findings indicate that scaling the model through attention\nmechanisms is efficient and improves model expressivity. These insights\nmotivate us to develop an NNIP architecture designed for scalability: the\nEfficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages a\nmulti-head self-attention formulation within graph neural networks, applying\nattention at the neighbor-level representations. Implemented with\nhighly-optimized attention GPU kernels, EScAIP achieves substantial gains in\nefficiency--at least 10x faster inference, 5x less memory usage--compared to\nexisting NNIPs. EScAIP also achieves state-of-the-art performance on a wide\nrange of datasets including catalysts (OC20 and OC22), molecules (SPICE), and\nmaterials (MPTrj). We emphasize that our approach should be thought of as a\nphilosophy rather than a specific model, representing a proof-of-concept for\ndeveloping general-purpose NNIPs that achieve better expressivity through\nscaling, and continue to scale efficiently with increased computational\nresources and training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling has been critical in improving model performance and generalization\nin machine learning. It involves how a model's performance changes with\nincreases in model size or input data, as well as how efficiently computational\nresources are utilized to support this growth. Despite successes in other\nareas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)\nremains limited. NNIPs act as surrogate models for ab initio quantum mechanical\ncalculations. The dominant paradigm here is to incorporate many physical domain\nconstraints into the model, such as rotational equivariance. We contend that\nthese complex constraints inhibit the scaling ability of NNIPs, and are likely\nto lead to performance plateaus in the long run. In this work, we take an\nalternative approach and start by systematically studying NNIP scaling\nstrategies. Our findings indicate that scaling the model through attention\nmechanisms is efficient and improves model expressivity. These insights\nmotivate us to develop an NNIP architecture designed for scalability: the\nEfficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages a\nmulti-head self-attention formulation within graph neural networks, applying\nattention at the neighbor-level representations. Implemented with\nhighly-optimized attention GPU kernels, EScAIP achieves substantial gains in\nefficiency--at least 10x faster inference, 5x less memory usage--compared to\nexisting NNIPs. EScAIP also achieves state-of-the-art performance on a wide\nrange of datasets including catalysts (OC20 and OC22), molecules (SPICE), and\nmaterials (MPTrj). We emphasize that our approach should be thought of as a\nphilosophy rather than a specific model, representing a proof-of-concept for\ndeveloping general-purpose NNIPs that achieve better expressivity through\nscaling, and continue to scale efficiently with increased computational\nresources and training data."
                },
                "authors": [
                    {
                        "name": "Eric Qu"
                    },
                    {
                        "name": "Aditi S. Krishnapriyan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi S. Krishnapriyan"
                },
                "author": "Aditi S. Krishnapriyan",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22382v2",
                "updated": "2024-10-31T17:12:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T12:54:55Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    54,
                    55,
                    1,
                    303,
                    0
                ],
                "title": "Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference"
                },
                "summary": "Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees."
                },
                "authors": [
                    {
                        "name": "Chris Lam"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lam"
                },
                "author": "Chris Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24155v1",
                "updated": "2024-10-31T17:12:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:12:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24152v1",
                "updated": "2024-10-31T17:10:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:10:01Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "title": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Wei Zhan"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    }
                ],
                "author_detail": {
                    "name": "Masayoshi Tomizuka"
                },
                "author": "Masayoshi Tomizuka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17947v2",
                "updated": "2024-10-31T17:08:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    8,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-25T21:47:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    21,
                    47,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias"
                },
                "summary": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl ."
                },
                "authors": [
                    {
                        "name": "Venkata S Govindarajan"
                    },
                    {
                        "name": "Matianyu Zang"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "David Beaver"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "Accepted to Findings@EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01079v3",
                "updated": "2024-10-31T16:59:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    59,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-01T08:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    34,
                    40,
                    0,
                    183,
                    0
                ],
                "title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)"
                },
                "summary": "We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data."
                },
                "authors": [
                    {
                        "name": "Jerry Yao-Chieh Hu"
                    },
                    {
                        "name": "Weimin Wu"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "Accepted at NeurIPS 2024. v3 updated to camera-ready version with\n  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12794v3",
                "updated": "2024-10-31T16:58:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    58,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-01-23T14:29:17Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    14,
                    29,
                    17,
                    1,
                    23,
                    0
                ],
                "title": "Benchmarking LLMs via Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs via Uncertainty Quantification"
                },
                "summary": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "30 pages, accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03636v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03636v4",
                "updated": "2024-10-31T16:54:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    54,
                    30,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-05T22:16:19Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    22,
                    16,
                    19,
                    2,
                    157,
                    0
                ],
                "title": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages"
                },
                "summary": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness."
                },
                "authors": [
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Haley Lepe"
                    },
                    {
                        "name": "Sahil Bhatia"
                    },
                    {
                        "name": "Karim Elmaaroufi"
                    },
                    {
                        "name": "George Varghese"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    }
                ],
                "author_detail": {
                    "name": "Sanjit A. Seshia"
                },
                "author": "Sanjit A. Seshia",
                "arxiv_comment": "14 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03636v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03636v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04823v2",
                "updated": "2024-10-31T16:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    48,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-07T10:48:45Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    10,
                    48,
                    45,
                    4,
                    159,
                    0
                ],
                "title": "BERTs are Generative In-Context Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BERTs are Generative In-Context Learners"
                },
                "summary": "While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives."
                },
                "authors": [
                    {
                        "name": "David Samuel"
                    }
                ],
                "author_detail": {
                    "name": "David Samuel"
                },
                "author": "David Samuel",
                "arxiv_comment": "26 pages, NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24117v1",
                "updated": "2024-10-31T16:46:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Repository-Level Compositional Code Translation and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-Level Compositional Code Translation and Validation"
                },
                "summary": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Mrigank Pawagi"
                    },
                    {
                        "name": "Muhammad Salman Abid"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07307v3",
                "updated": "2024-10-31T16:39:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    39,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-11T21:12:21Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    21,
                    12,
                    21,
                    6,
                    42,
                    0
                ],
                "title": "Self-Calibrating Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Calibrating Conformal Prediction"
                },
                "summary": "In machine learning, model calibration and predictive inference are essential\nfor producing reliable predictions and quantifying uncertainty to support\ndecision-making. Recognizing the complementary roles of point and interval\npredictions, we introduce Self-Calibrating Conformal Prediction, a method that\ncombines Venn-Abers calibration and conformal prediction to deliver calibrated\npoint predictions alongside prediction intervals with finite-sample validity\nconditional on these predictions. To achieve this, we extend the original\nVenn-Abers procedure from binary classification to regression. Our theoretical\nframework supports analyzing conformal prediction methods that involve\ncalibrating model predictions and subsequently constructing conditionally valid\nprediction intervals on the same data, where the conditioning set or conformity\nscores may depend on the calibrated predictions. Real-data experiments show\nthat our method improves interval efficiency through model calibration and\noffers a practical alternative to feature-conditional validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, model calibration and predictive inference are essential\nfor producing reliable predictions and quantifying uncertainty to support\ndecision-making. Recognizing the complementary roles of point and interval\npredictions, we introduce Self-Calibrating Conformal Prediction, a method that\ncombines Venn-Abers calibration and conformal prediction to deliver calibrated\npoint predictions alongside prediction intervals with finite-sample validity\nconditional on these predictions. To achieve this, we extend the original\nVenn-Abers procedure from binary classification to regression. Our theoretical\nframework supports analyzing conformal prediction methods that involve\ncalibrating model predictions and subsequently constructing conditionally valid\nprediction intervals on the same data, where the conditioning set or conformity\nscores may depend on the calibrated predictions. Real-data experiments show\nthat our method improves interval efficiency through model calibration and\noffers a practical alternative to feature-conditional validity."
                },
                "authors": [
                    {
                        "name": "Lars van der Laan"
                    },
                    {
                        "name": "Ahmed M. Alaa"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Alaa"
                },
                "author": "Ahmed M. Alaa",
                "arxiv_comment": "Accepted to Neurips 2024. Preprint previously titled Self-Consistent\n  Conformal Prediction.\n  https://openreview.net/forum?id=BJ6HkT7qIk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15892v3",
                "updated": "2024-10-31T16:36:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    36,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-22T01:52:30Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    52,
                    30,
                    0,
                    204,
                    0
                ],
                "title": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training"
                },
                "summary": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24096v1",
                "updated": "2024-10-31T16:28:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:28:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning"
                },
                "summary": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform."
                },
                "authors": [
                    {
                        "name": "Nabil Omi"
                    },
                    {
                        "name": "Hosein Hasanbeig"
                    },
                    {
                        "name": "Hiteshi Sharma"
                    },
                    {
                        "name": "Sriram K. Rajamani"
                    },
                    {
                        "name": "Siddhartha Sen"
                    }
                ],
                "author_detail": {
                    "name": "Siddhartha Sen"
                },
                "author": "Siddhartha Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24087v1",
                "updated": "2024-10-31T16:20:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    20,
                    4,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:20:04Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    20,
                    4,
                    3,
                    305,
                    0
                ],
                "title": "In-Context Fine-Tuning for Time-Series Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Fine-Tuning for Time-Series Foundation Models"
                },
                "summary": "Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Das"
                    },
                    {
                        "name": "Matthew Faw"
                    },
                    {
                        "name": "Rajat Sen"
                    },
                    {
                        "name": "Yichen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhou"
                },
                "author": "Yichen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24079v1",
                "updated": "2024-10-31T16:16:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    16,
                    18,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:16:18Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    16,
                    18,
                    3,
                    305,
                    0
                ],
                "title": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models"
                },
                "summary": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences."
                },
                "authors": [
                    {
                        "name": "Jinlin Lai"
                    },
                    {
                        "name": "Daniel Sheldon"
                    },
                    {
                        "name": "Justin Domke"
                    }
                ],
                "author_detail": {
                    "name": "Justin Domke"
                },
                "author": "Justin Domke",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18760v3",
                "updated": "2024-10-31T16:12:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    12,
                    16,
                    3,
                    305,
                    0
                ],
                "published": "2023-11-30T18:02:44Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    2,
                    44,
                    3,
                    334,
                    0
                ],
                "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaskBench: Benchmarking Large Language Models for Task Automation"
                },
                "summary": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents."
                },
                "authors": [
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19238v2",
                "updated": "2024-10-31T16:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    6,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-27T15:01:53Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    1,
                    53,
                    3,
                    179,
                    0
                ],
                "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Fine-Grained Values and Opinions in Large Language Models"
                },
                "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24069v1",
                "updated": "2024-10-31T16:05:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    5,
                    21,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:05:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    5,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Simulation-based inference of the 2D ex-situ stellar mass fraction\n  distribution of galaxies using variational autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference of the 2D ex-situ stellar mass fraction\n  distribution of galaxies using variational autoencoders"
                },
                "summary": "Galaxies grow through star formation (in-situ) and accretion (ex-situ) of\nother galaxies. Reconstructing the relative contribution of these two growth\nchannels is crucial for constraining the processes of galaxy formation in a\ncosmological context. In this on-going work, we utilize a conditional\nvariational autoencoder along with a normalizing flow - trained on a\nstate-of-the-art cosmological simulation - in an attempt to infer the posterior\ndistribution of the 2D ex-situ stellar mass distribution of galaxies solely\nfrom observable two-dimensional maps of their stellar mass, kinematics, age and\nmetallicity. Such maps are typically obtained from large Integral Field Unit\nSurveys such as MaNGA. We find that the average posterior provides an estimate\nof the resolved accretion histories of galaxies with a mean ~10% error per\npixel. We show that the use of a normalizing flow to conditionally sample the\nlatent space results in a smaller reconstruction error. Due to the\nprobabilistic nature of our architecture, the uncertainty of our predictions\ncan also be quantified. To our knowledge, this is the first attempt to infer\nthe 2D ex-situ fraction maps from observable maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxies grow through star formation (in-situ) and accretion (ex-situ) of\nother galaxies. Reconstructing the relative contribution of these two growth\nchannels is crucial for constraining the processes of galaxy formation in a\ncosmological context. In this on-going work, we utilize a conditional\nvariational autoencoder along with a normalizing flow - trained on a\nstate-of-the-art cosmological simulation - in an attempt to infer the posterior\ndistribution of the 2D ex-situ stellar mass distribution of galaxies solely\nfrom observable two-dimensional maps of their stellar mass, kinematics, age and\nmetallicity. Such maps are typically obtained from large Integral Field Unit\nSurveys such as MaNGA. We find that the average posterior provides an estimate\nof the resolved accretion histories of galaxies with a mean ~10% error per\npixel. We show that the use of a normalizing flow to conditionally sample the\nlatent space results in a smaller reconstruction error. Due to the\nprobabilistic nature of our architecture, the uncertainty of our predictions\ncan also be quantified. To our knowledge, this is the first attempt to infer\nthe 2D ex-situ fraction maps from observable maps."
                },
                "authors": [
                    {
                        "name": "Eirini Angeloudi"
                    },
                    {
                        "name": "Marc Huertas-Company"
                    },
                    {
                        "name": "Jesús Falcón-Barroso"
                    },
                    {
                        "name": "Regina Sarmiento"
                    },
                    {
                        "name": "Daniel Walo-Martín"
                    },
                    {
                        "name": "Annalisa Pillepich"
                    },
                    {
                        "name": "Jesús Vega Ferrero"
                    }
                ],
                "author_detail": {
                    "name": "Jesús Vega Ferrero"
                },
                "author": "Jesús Vega Ferrero",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v2",
                "updated": "2024-10-31T16:01:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    1,
                    59,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4"
                },
                "summary": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02119v3",
                "updated": "2024-10-31T15:57:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    57,
                    42,
                    3,
                    305,
                    0
                ],
                "published": "2023-12-04T18:49:23Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    49,
                    23,
                    0,
                    338,
                    0
                ],
                "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
                },
                "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard."
                },
                "authors": [
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    },
                    {
                        "name": "Paul Kassianik"
                    },
                    {
                        "name": "Blaine Nelson"
                    },
                    {
                        "name": "Hyrum Anderson"
                    },
                    {
                        "name": "Yaron Singer"
                    },
                    {
                        "name": "Amin Karbasi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Karbasi"
                },
                "author": "Amin Karbasi",
                "arxiv_comment": "Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01093v2",
                "updated": "2024-10-31T15:56:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    56,
                    8,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-02T01:45:18Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    1,
                    45,
                    18,
                    4,
                    33,
                    0
                ],
                "title": "Need a Small Specialized Language Model? Plan Early!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Need a Small Specialized Language Model? Plan Early!"
                },
                "summary": "Large language models are versatile tools but are not suitable for small\ninference budgets. Small models have more efficient inference, but their lower\ncapacity means that their performance can be good only if one limits their\nscope to a specialized domain. This paper explores how to get good specialized\nsmall language models using a large, generic, pretraining set and a limited\namount of specialized data. We consider two scenarios, depending on whether (i)\none can afford pretraining a model for each specialization task, or (ii) one\nwants to cheaply adapt a single pretrained model for each task. In the first\nscenario, we propose an effective solution based on importance sampling: we\nresample the pretraining set to imitate the specialization data and train a\nsmall model on it. In the second scenario, we propose a novel architecture,\nprojected networks (PN). PN is a large network whose parameters can be linearly\nprojected into a small network for specialization. For both scenarios, we\ndemonstrate the empirical effectiveness of our solutions across various\ndomains, training set sizes, and training budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are versatile tools but are not suitable for small\ninference budgets. Small models have more efficient inference, but their lower\ncapacity means that their performance can be good only if one limits their\nscope to a specialized domain. This paper explores how to get good specialized\nsmall language models using a large, generic, pretraining set and a limited\namount of specialized data. We consider two scenarios, depending on whether (i)\none can afford pretraining a model for each specialization task, or (ii) one\nwants to cheaply adapt a single pretrained model for each task. In the first\nscenario, we propose an effective solution based on importance sampling: we\nresample the pretraining set to imitate the specialization data and train a\nsmall model on it. In the second scenario, we propose a novel architecture,\nprojected networks (PN). PN is a large network whose parameters can be linearly\nprojected into a small network for specialization. For both scenarios, we\ndemonstrate the empirical effectiveness of our solutions across various\ndomains, training set sizes, and training budgets."
                },
                "authors": [
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Angelos Katharopoulos"
                    },
                    {
                        "name": "Pierre Ablin"
                    },
                    {
                        "name": "Awni Hannun"
                    }
                ],
                "author_detail": {
                    "name": "Awni Hannun"
                },
                "author": "Awni Hannun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16978v2",
                "updated": "2024-10-31T15:50:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    50,
                    21,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-27T09:21:40Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    9,
                    21,
                    40,
                    0,
                    148,
                    0
                ],
                "title": "OSLO: One-Shot Label-Only Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSLO: One-Shot Label-Only Membership Inference Attacks"
                },
                "summary": "We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO."
                },
                "authors": [
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "To appear at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24054v1",
                "updated": "2024-10-31T15:48:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    48,
                    34,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:48:34Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    48,
                    34,
                    3,
                    305,
                    0
                ],
                "title": "EigenVI: score-based variational inference with orthogonal function\n  expansions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenVI: score-based variational inference with orthogonal function\n  expansions"
                },
                "summary": "We develop EigenVI, an eigenvalue-based approach for black-box variational\ninference (BBVI). EigenVI constructs its variational approximations from\northogonal function expansions. For distributions over $\\mathbb{R}^D$, the\nlowest order term in these expansions provides a Gaussian variational\napproximation, while higher-order terms provide a systematic way to model\nnon-Gaussianity. These approximations are flexible enough to model complex\ndistributions (multimodal, asymmetric), but they are simple enough that one can\ncalculate their low-order moments and draw samples from them. EigenVI can also\nmodel other types of random variables (e.g., nonnegative, bounded) by\nconstructing variational approximations from different families of orthogonal\nfunctions. Within these families, EigenVI computes the variational\napproximation that best matches the score function of the target distribution\nby minimizing a stochastic estimate of the Fisher divergence. Notably, this\noptimization reduces to solving a minimum eigenvalue problem, so that EigenVI\neffectively sidesteps the iterative gradient-based optimizations that are\nrequired for many other BBVI algorithms. (Gradient-based methods can be\nsensitive to learning rates, termination criteria, and other tunable\nhyperparameters.) We use EigenVI to approximate a variety of target\ndistributions, including a benchmark suite of Bayesian models from posteriordb.\nOn these distributions, we find that EigenVI is more accurate than existing\nmethods for Gaussian BBVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop EigenVI, an eigenvalue-based approach for black-box variational\ninference (BBVI). EigenVI constructs its variational approximations from\northogonal function expansions. For distributions over $\\mathbb{R}^D$, the\nlowest order term in these expansions provides a Gaussian variational\napproximation, while higher-order terms provide a systematic way to model\nnon-Gaussianity. These approximations are flexible enough to model complex\ndistributions (multimodal, asymmetric), but they are simple enough that one can\ncalculate their low-order moments and draw samples from them. EigenVI can also\nmodel other types of random variables (e.g., nonnegative, bounded) by\nconstructing variational approximations from different families of orthogonal\nfunctions. Within these families, EigenVI computes the variational\napproximation that best matches the score function of the target distribution\nby minimizing a stochastic estimate of the Fisher divergence. Notably, this\noptimization reduces to solving a minimum eigenvalue problem, so that EigenVI\neffectively sidesteps the iterative gradient-based optimizations that are\nrequired for many other BBVI algorithms. (Gradient-based methods can be\nsensitive to learning rates, termination criteria, and other tunable\nhyperparameters.) We use EigenVI to approximate a variety of target\ndistributions, including a benchmark suite of Bayesian models from posteriordb.\nOn these distributions, we find that EigenVI is more accurate than existing\nmethods for Gaussian BBVI."
                },
                "authors": [
                    {
                        "name": "Diana Cai"
                    },
                    {
                        "name": "Chirag Modi"
                    },
                    {
                        "name": "Charles C. Margossian"
                    },
                    {
                        "name": "Robert M. Gower"
                    },
                    {
                        "name": "David M. Blei"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "arxiv_comment": "25 pages, 9 figures. Advances in Neural Information Processing\n  Systems (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24049v1",
                "updated": "2024-10-31T15:45:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:45:23Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
                },
                "summary": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs."
                },
                "authors": [
                    {
                        "name": "Muhammed Saeed"
                    },
                    {
                        "name": "Elgizouli Mohamed"
                    },
                    {
                        "name": "Mukhtar Mohamed"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Shady Shehata"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Abdul-Mageed"
                },
                "author": "Muhammad Abdul-Mageed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24034v1",
                "updated": "2024-10-31T15:32:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:32:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Handwriting Recognition in Historical Documents with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwriting Recognition in Historical Documents with Multimodal LLM"
                },
                "summary": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio"
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    }
                ],
                "author_detail": {
                    "name": "Lucian Li"
                },
                "author": "Lucian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24032v1",
                "updated": "2024-10-31T15:30:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks"
                },
                "summary": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration."
                },
                "authors": [
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24028v1",
                "updated": "2024-10-31T15:28:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    28,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:28:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    28,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with\n  Generalized Affinity Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with\n  Generalized Affinity Control"
                },
                "summary": "The rise of mobile devices equipped with numerous sensors, such as LiDAR and\ncameras, has spurred the adoption of multi-modal deep intelligence for\ndistributed sensing tasks, such as smart cabins and driving assistance.\nHowever, the arrival times of mobile sensory data vary due to modality size and\nnetwork dynamics, which can lead to delays (if waiting for slower data) or\naccuracy decline (if inference proceeds without waiting). Moreover, the\ndiversity and dynamic nature of mobile systems exacerbate this challenge. In\nresponse, we present a shift to \\textit{opportunistic} inference for\nasynchronous distributed multi-modal data, enabling inference as soon as\npartial data arrives. While existing methods focus on optimizing modality\nconsistency and complementarity, known as modal affinity, they lack a\n\\textit{computational} approach to control this affinity in open-world mobile\nenvironments. AdaFlow pioneers the formulation of structured cross-modality\naffinity in mobile contexts using a hierarchical analysis-based normalized\nmatrix. This approach accommodates the diversity and dynamics of modalities,\ngeneralizing across different types and numbers of inputs. Employing an\naffinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible\ndata imputation, adapting to various modalities and downstream tasks without\nretraining. Experiments show that AdaFlow significantly reduces inference\nlatency by up to 79.9\\% and enhances accuracy by up to 61.9\\%, outperforming\nstatus quo approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of mobile devices equipped with numerous sensors, such as LiDAR and\ncameras, has spurred the adoption of multi-modal deep intelligence for\ndistributed sensing tasks, such as smart cabins and driving assistance.\nHowever, the arrival times of mobile sensory data vary due to modality size and\nnetwork dynamics, which can lead to delays (if waiting for slower data) or\naccuracy decline (if inference proceeds without waiting). Moreover, the\ndiversity and dynamic nature of mobile systems exacerbate this challenge. In\nresponse, we present a shift to \\textit{opportunistic} inference for\nasynchronous distributed multi-modal data, enabling inference as soon as\npartial data arrives. While existing methods focus on optimizing modality\nconsistency and complementarity, known as modal affinity, they lack a\n\\textit{computational} approach to control this affinity in open-world mobile\nenvironments. AdaFlow pioneers the formulation of structured cross-modality\naffinity in mobile contexts using a hierarchical analysis-based normalized\nmatrix. This approach accommodates the diversity and dynamics of modalities,\ngeneralizing across different types and numbers of inputs. Employing an\naffinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible\ndata imputation, adapting to various modalities and downstream tasks without\nretraining. Experiments show that AdaFlow significantly reduces inference\nlatency by up to 79.9\\% and enhances accuracy by up to 61.9\\%, outperforming\nstatus quo approaches."
                },
                "authors": [
                    {
                        "name": "Fenmin Wu"
                    },
                    {
                        "name": "Sicong Liu"
                    },
                    {
                        "name": "Kehao Zhu"
                    },
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Zhiwen Yu"
                    },
                    {
                        "name": "Hongkai Wen"
                    },
                    {
                        "name": "Xiangrui Xu"
                    },
                    {
                        "name": "Lehao Wang"
                    },
                    {
                        "name": "Xiangyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Liu"
                },
                "author": "Xiangyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24024v1",
                "updated": "2024-10-31T15:25:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:25:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents"
                },
                "summary": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Siyi Cheng"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Shudan Zhang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24021v1",
                "updated": "2024-10-31T15:21:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:21:27Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "title": "Detecting text level intellectual influence with knowledge graph\n  embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text level intellectual influence with knowledge graph\n  embeddings"
                },
                "summary": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights."
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    },
                    {
                        "name": "Eryclis Silva"
                    }
                ],
                "author_detail": {
                    "name": "Eryclis Silva"
                },
                "author": "Eryclis Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24020v1",
                "updated": "2024-10-31T15:21:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:21:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Cross-correlation of Luminous Red Galaxies with ML-selected AGN in\n  HSC-SSP II: AGN classification and clustering with DESI spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-correlation of Luminous Red Galaxies with ML-selected AGN in\n  HSC-SSP II: AGN classification and clustering with DESI spectroscopy"
                },
                "summary": "An unresolved question in studies of active galactic nuclei (AGN) is whether\ntheir different classes probe different evolutionary stages of black hole--host\ngalaxy interaction. We present the projected two-point cross-correlation\nfunction between a sample of Dark Energy Spectroscopic Instrument\n(DESI)-matched AGN selected from Hyper Suprime-Cam Subaru Strategic Program\n(HSC-SSP) optical + Wide-field Infrared Survey Explorer ($WISE$) mid-IR\nphotometry, and DESI-designated luminous red galaxies, for $z\\in 0.5-1.0$. The\ntotal overlap area is 43.4 deg$^2$, including $\\sim27,000$ spectroscopic LRGs\nin our redshift range. We visually classified 1,991 matched HSC-DESI objects in\nour redshift range, spectroscopically confirming that 1,517 ($76\\%$) of them\nare AGN. Of these 1,517 objects, $73\\%$ are broad-line AGN, $27\\%$ are obscured\nAGN. We infer that the parent HSC+$WISE$ AGN catalog has a number density of at\nleast $\\sim 240$ deg$^{-2}$, confirming it is one of the most complete\noptical/infrared AGN catalog to date. We investigate the AGN clustering as a\nfunction of the spectroscopic classification and infer the halo mass for each\nsample. The inferred average mass of the halos $\\langle M_h\\rangle$ that host\nunobscured broad-line AGN ($M_h \\approx 10^{13.4}h^{-1}M_\\odot$) is $\\sim\n5.5\\times$ larger than the halos that host obscured AGN ($M_h \\approx\n10^{12.6}\\, h^{-1}M_\\odot$), at $2.8\\sigma$ significance, in the same sense as\nour prior work based on photometric redshifts. This suggests that we may relax\nour concerns about systematic shifts in the inferred redshift distribution\nproducing this halo mass difference. While we do not yet find statistically\nsignificant spectroscopic evidence that unobscured AGN reside in more massive\nhalos than their obscured counterparts, further analyses are necessary to\ndistinguish if more complex evolutionary histories are needed to model these\nAGN populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An unresolved question in studies of active galactic nuclei (AGN) is whether\ntheir different classes probe different evolutionary stages of black hole--host\ngalaxy interaction. We present the projected two-point cross-correlation\nfunction between a sample of Dark Energy Spectroscopic Instrument\n(DESI)-matched AGN selected from Hyper Suprime-Cam Subaru Strategic Program\n(HSC-SSP) optical + Wide-field Infrared Survey Explorer ($WISE$) mid-IR\nphotometry, and DESI-designated luminous red galaxies, for $z\\in 0.5-1.0$. The\ntotal overlap area is 43.4 deg$^2$, including $\\sim27,000$ spectroscopic LRGs\nin our redshift range. We visually classified 1,991 matched HSC-DESI objects in\nour redshift range, spectroscopically confirming that 1,517 ($76\\%$) of them\nare AGN. Of these 1,517 objects, $73\\%$ are broad-line AGN, $27\\%$ are obscured\nAGN. We infer that the parent HSC+$WISE$ AGN catalog has a number density of at\nleast $\\sim 240$ deg$^{-2}$, confirming it is one of the most complete\noptical/infrared AGN catalog to date. We investigate the AGN clustering as a\nfunction of the spectroscopic classification and infer the halo mass for each\nsample. The inferred average mass of the halos $\\langle M_h\\rangle$ that host\nunobscured broad-line AGN ($M_h \\approx 10^{13.4}h^{-1}M_\\odot$) is $\\sim\n5.5\\times$ larger than the halos that host obscured AGN ($M_h \\approx\n10^{12.6}\\, h^{-1}M_\\odot$), at $2.8\\sigma$ significance, in the same sense as\nour prior work based on photometric redshifts. This suggests that we may relax\nour concerns about systematic shifts in the inferred redshift distribution\nproducing this halo mass difference. While we do not yet find statistically\nsignificant spectroscopic evidence that unobscured AGN reside in more massive\nhalos than their obscured counterparts, further analyses are necessary to\ndistinguish if more complex evolutionary histories are needed to model these\nAGN populations."
                },
                "authors": [
                    {
                        "name": "Rodrigo Córdova Rosado"
                    },
                    {
                        "name": "Andy D. Goulding"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Nickolas Kokron"
                    },
                    {
                        "name": "Michael A. Strauss"
                    },
                    {
                        "name": "ChangHoon Hahn"
                    },
                    {
                        "name": "Grayson C. Petter"
                    },
                    {
                        "name": "Ryan C. Hickox"
                    }
                ],
                "author_detail": {
                    "name": "Ryan C. Hickox"
                },
                "author": "Ryan C. Hickox",
                "arxiv_comment": "27 pages, 12 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24015v1",
                "updated": "2024-10-31T15:17:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    17,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:17:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    17,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Unveiling Synthetic Faces: How Synthetic Datasets Can Expose Real\n  Identities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Synthetic Faces: How Synthetic Datasets Can Expose Real\n  Identities"
                },
                "summary": "Synthetic data generation is gaining increasing popularity in different\ncomputer vision applications. Existing state-of-the-art face recognition models\nare trained using large-scale face datasets, which are crawled from the\nInternet and raise privacy and ethical concerns. To address such concerns,\nseveral works have proposed generating synthetic face datasets to train face\nrecognition models. However, these methods depend on generative models, which\nare trained on real face images. In this work, we design a simple yet effective\nmembership inference attack to systematically study if any of the existing\nsynthetic face recognition datasets leak any information from the real data\nused to train the generator model. We provide an extensive study on 6\nstate-of-the-art synthetic face recognition datasets, and show that in all\nthese synthetic datasets, several samples from the original real dataset are\nleaked. To our knowledge, this paper is the first work which shows the leakage\nfrom training data of generator models into the generated synthetic face\nrecognition datasets. Our study demonstrates privacy pitfalls in synthetic face\nrecognition datasets and paves the way for future studies on generating\nresponsible synthetic face datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation is gaining increasing popularity in different\ncomputer vision applications. Existing state-of-the-art face recognition models\nare trained using large-scale face datasets, which are crawled from the\nInternet and raise privacy and ethical concerns. To address such concerns,\nseveral works have proposed generating synthetic face datasets to train face\nrecognition models. However, these methods depend on generative models, which\nare trained on real face images. In this work, we design a simple yet effective\nmembership inference attack to systematically study if any of the existing\nsynthetic face recognition datasets leak any information from the real data\nused to train the generator model. We provide an extensive study on 6\nstate-of-the-art synthetic face recognition datasets, and show that in all\nthese synthetic datasets, several samples from the original real dataset are\nleaked. To our knowledge, this paper is the first work which shows the leakage\nfrom training data of generator models into the generated synthetic face\nrecognition datasets. Our study demonstrates privacy pitfalls in synthetic face\nrecognition datasets and paves the way for future studies on generating\nresponsible synthetic face datasets."
                },
                "authors": [
                    {
                        "name": "Hatef Otroshi Shahreza"
                    },
                    {
                        "name": "Sébastien Marcel"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Marcel"
                },
                "author": "Sébastien Marcel",
                "arxiv_comment": "Accepted in NeurIPS 2024 Workshop on New Frontiers in Adversarial\n  Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15852v2",
                "updated": "2024-10-31T14:43:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    43,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-23T14:04:48Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    14,
                    4,
                    48,
                    5,
                    83,
                    0
                ],
                "title": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents"
                },
                "summary": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Husn"
                    },
                    {
                        "name": "Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen"
                },
                "arxiv_affiliation": "Peter",
                "author": "Chen",
                "arxiv_comment": "ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23987v1",
                "updated": "2024-10-31T14:40:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    40,
                    48,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:40:48Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    40,
                    48,
                    3,
                    305,
                    0
                ],
                "title": "Task-Aware Unified Source Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware Unified Source Separation"
                },
                "summary": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts."
                },
                "authors": [
                    {
                        "name": "Kohei Saijo"
                    },
                    {
                        "name": "Janek Ebbers"
                    },
                    {
                        "name": "François G. Germain"
                    },
                    {
                        "name": "Gordon Wichern"
                    },
                    {
                        "name": "Jonathan Le Roux"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Le Roux"
                },
                "author": "Jonathan Le Roux",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v2",
                "updated": "2024-10-31T14:38:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    38,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17638v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17638v3",
                "updated": "2024-10-31T14:35:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    35,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2023-10-26T17:53:24Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    17,
                    53,
                    24,
                    3,
                    299,
                    0
                ],
                "title": "Generative Fractional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Fractional Diffusion Models"
                },
                "summary": "We introduce the first continuous-time score-based generative model that\nleverages fractional diffusion processes for its underlying dynamics. Although\ndiffusion models have excelled at capturing data distributions, they still\nsuffer from various limitations such as slow convergence, mode-collapse on\nimbalanced data, and lack of diversity. These issues are partially linked to\nthe use of light-tailed Brownian motion (BM) with independent increments. In\nthis paper, we replace BM with an approximation of its non-Markovian\ncounterpart, fractional Brownian motion (fBM), characterized by correlated\nincrements and Hurst index $H \\in (0,1)$, where $H=0.5$ recovers the classical\nBM. To ensure tractable inference and learning, we employ a recently\npopularized Markov approximation of fBM (MA-fBM) and derive its reverse-time\nmodel, resulting in generative fractional diffusion models (GFDM). We\ncharacterize the forward dynamics using a continuous reparameterization trick\nand propose augmented score matching to efficiently learn the score function,\nwhich is partly known in closed form, at minimal added cost. The ability to\ndrive our diffusion model via MA-fBM offers flexibility and control. $H \\leq\n0.5$ enters the regime of rough paths whereas $H>0.5$ regularizes diffusion\npaths and invokes long-term memory. The Markov approximation allows added\ncontrol by varying the number of Markov processes linearly combined to\napproximate fBM. Our evaluations on real image datasets demonstrate that GFDM\nachieves greater pixel-wise diversity and enhanced image quality, as indicated\nby a lower FID, offering a promising alternative to traditional diffusion\nmodels",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the first continuous-time score-based generative model that\nleverages fractional diffusion processes for its underlying dynamics. Although\ndiffusion models have excelled at capturing data distributions, they still\nsuffer from various limitations such as slow convergence, mode-collapse on\nimbalanced data, and lack of diversity. These issues are partially linked to\nthe use of light-tailed Brownian motion (BM) with independent increments. In\nthis paper, we replace BM with an approximation of its non-Markovian\ncounterpart, fractional Brownian motion (fBM), characterized by correlated\nincrements and Hurst index $H \\in (0,1)$, where $H=0.5$ recovers the classical\nBM. To ensure tractable inference and learning, we employ a recently\npopularized Markov approximation of fBM (MA-fBM) and derive its reverse-time\nmodel, resulting in generative fractional diffusion models (GFDM). We\ncharacterize the forward dynamics using a continuous reparameterization trick\nand propose augmented score matching to efficiently learn the score function,\nwhich is partly known in closed form, at minimal added cost. The ability to\ndrive our diffusion model via MA-fBM offers flexibility and control. $H \\leq\n0.5$ enters the regime of rough paths whereas $H>0.5$ regularizes diffusion\npaths and invokes long-term memory. The Markov approximation allows added\ncontrol by varying the number of Markov processes linearly combined to\napproximate fBM. Our evaluations on real image datasets demonstrate that GFDM\nachieves greater pixel-wise diversity and enhanced image quality, as indicated\nby a lower FID, offering a promising alternative to traditional diffusion\nmodels"
                },
                "authors": [
                    {
                        "name": "Gabriel Nobis"
                    },
                    {
                        "name": "Maximilian Springenberg"
                    },
                    {
                        "name": "Marco Aversa"
                    },
                    {
                        "name": "Michael Detzel"
                    },
                    {
                        "name": "Rembert Daems"
                    },
                    {
                        "name": "Roderick Murray-Smith"
                    },
                    {
                        "name": "Shinichi Nakajima"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Tolga Birdal"
                    },
                    {
                        "name": "Manfred Opper"
                    },
                    {
                        "name": "Christoph Knochenhauer"
                    },
                    {
                        "name": "Luis Oala"
                    },
                    {
                        "name": "Wojciech Samek"
                    }
                ],
                "author_detail": {
                    "name": "Wojciech Samek"
                },
                "author": "Wojciech Samek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17638v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17638v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; F.4.1; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19534v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19534v4",
                "updated": "2024-10-31T14:32:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    32,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-29T21:29:44Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    21,
                    29,
                    44,
                    2,
                    150,
                    0
                ],
                "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Learning Algorithms Do Not Learn Preference Rankings"
                },
                "summary": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Lily H. Zhang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "arxiv_comment": "NeurIPS 2024 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19534v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19534v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23975v1",
                "updated": "2024-10-31T14:30:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    30,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:30:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    30,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Average Controlled and Average Natural Micro Direct Effects in Summary\n  Causal Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Average Controlled and Average Natural Micro Direct Effects in Summary\n  Causal Graphs"
                },
                "summary": "In this paper, we investigate the identifiability of average controlled\ndirect effects and average natural direct effects in causal systems represented\nby summary causal graphs, which are abstractions of full causal graphs, often\nused in dynamic systems where cycles and omitted temporal information\ncomplicate causal inference. Unlike in the traditional linear setting, where\ndirect effects are typically easier to identify and estimate, non-parametric\ndirect effects, which are crucial for handling real-world complexities,\nparticularly in epidemiological contexts where relationships between variables\n(e.g, genetic, environmental, and behavioral factors) are often non-linear, are\nmuch harder to define and identify. In particular, we give sufficient\nconditions for identifying average controlled micro direct effect and average\nnatural micro direct effect from summary causal graphs in the presence of\nhidden confounding. Furthermore, we show that the conditions given for the\naverage controlled micro direct effect become also necessary in the setting\nwhere there is no hidden confounding and where we are only interested in\nidentifiability by adjustment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the identifiability of average controlled\ndirect effects and average natural direct effects in causal systems represented\nby summary causal graphs, which are abstractions of full causal graphs, often\nused in dynamic systems where cycles and omitted temporal information\ncomplicate causal inference. Unlike in the traditional linear setting, where\ndirect effects are typically easier to identify and estimate, non-parametric\ndirect effects, which are crucial for handling real-world complexities,\nparticularly in epidemiological contexts where relationships between variables\n(e.g, genetic, environmental, and behavioral factors) are often non-linear, are\nmuch harder to define and identify. In particular, we give sufficient\nconditions for identifying average controlled micro direct effect and average\nnatural micro direct effect from summary causal graphs in the presence of\nhidden confounding. Furthermore, we show that the conditions given for the\naverage controlled micro direct effect become also necessary in the setting\nwhere there is no hidden confounding and where we are only interested in\nidentifiability by adjustment."
                },
                "authors": [
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06246v2",
                "updated": "2024-10-31T14:24:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    24,
                    45,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-10T13:23:00Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    13,
                    23,
                    0,
                    0,
                    162,
                    0
                ],
                "title": "Data-Efficient Learning with Neural Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Learning with Neural Programs"
                },
                "summary": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner."
                },
                "authors": [
                    {
                        "name": "Alaia Solko-Breslin"
                    },
                    {
                        "name": "Seewon Choi"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Neelay Velingker"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23968v1",
                "updated": "2024-10-31T14:22:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    22,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:22:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    22,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable\n  Robot Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable\n  Robot Task Planning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have helped facilitate\nexciting progress for robotic planning in real, open-world environments. 3D\nscene graphs (3DSGs) offer a promising environment representation for grounding\nsuch LLM-based planners as they are compact and semantically rich. However, as\nthe robot's environment scales (e.g., number of entities tracked) and the\ncomplexity of scene graph information increases (e.g., maintaining more\nattributes), providing the 3DSG as-is to an LLM-based planner quickly becomes\ninfeasible due to input token count limits and attentional biases present in\nLLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods\nthat retrieve query-relevant document chunks for LLM question and answering, we\nadapt the paradigm for our embodied domain. Specifically, we propose a 3D scene\nsubgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based\nplanner with for executing natural language robotic tasks. Notably, our\nretrieved subgraphs adapt to changes in the environment as well as changes in\ntask-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's\nability to significantly reduce input token counts (by an order of magnitude)\nand planning time (up to 70% reduction in average time per planning step) while\nimproving success rates on AI2Thor simulated household tasks with a single-arm,\nmobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with\na manipulator to highlight the performance benefits for robot deployment at the\nedge in real environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have helped facilitate\nexciting progress for robotic planning in real, open-world environments. 3D\nscene graphs (3DSGs) offer a promising environment representation for grounding\nsuch LLM-based planners as they are compact and semantically rich. However, as\nthe robot's environment scales (e.g., number of entities tracked) and the\ncomplexity of scene graph information increases (e.g., maintaining more\nattributes), providing the 3DSG as-is to an LLM-based planner quickly becomes\ninfeasible due to input token count limits and attentional biases present in\nLLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods\nthat retrieve query-relevant document chunks for LLM question and answering, we\nadapt the paradigm for our embodied domain. Specifically, we propose a 3D scene\nsubgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based\nplanner with for executing natural language robotic tasks. Notably, our\nretrieved subgraphs adapt to changes in the environment as well as changes in\ntask-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's\nability to significantly reduce input token counts (by an order of magnitude)\nand planning time (up to 70% reduction in average time per planning step) while\nimproving success rates on AI2Thor simulated household tasks with a single-arm,\nmobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with\na manipulator to highlight the performance benefits for robot deployment at the\nedge in real environments."
                },
                "authors": [
                    {
                        "name": "Meghan Booker"
                    },
                    {
                        "name": "Grayson Byrd"
                    },
                    {
                        "name": "Bethany Kemp"
                    },
                    {
                        "name": "Aurora Schmidt"
                    },
                    {
                        "name": "Corban Rivera"
                    }
                ],
                "author_detail": {
                    "name": "Corban Rivera"
                },
                "author": "Corban Rivera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14808v2",
                "updated": "2024-10-31T14:19:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    19,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-23T17:18:46Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    18,
                    46,
                    3,
                    144,
                    0
                ],
                "title": "Implicit Personalization in Language Models: A Systematic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Personalization in Language Models: A Systematic Study"
                },
                "summary": "Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData."
                },
                "authors": [
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Nils Heil"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Yahang Qi"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21076v2",
                "updated": "2024-10-31T14:16:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    16,
                    36,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-28T14:40:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    40,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows"
                },
                "summary": "We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community."
                },
                "authors": [
                    {
                        "name": "Alicja Polanska"
                    },
                    {
                        "name": "Thibeau Wouters"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    },
                    {
                        "name": "Kaze K. W. Wong"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_comment": "accepted to NeurIPS 2024 workshop on Machine Learning and the\n  Physical Sciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10476v2",
                "updated": "2024-10-31T14:15:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    15,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-14T13:10:45Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?"
                },
                "summary": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub."
                },
                "authors": [
                    {
                        "name": "Gabriel Roccabruna"
                    },
                    {
                        "name": "Massimo Rizzoli"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23956v1",
                "updated": "2024-10-31T14:09:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    9,
                    50,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:09:50Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    9,
                    50,
                    3,
                    305,
                    0
                ],
                "title": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language"
                },
                "summary": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM."
                },
                "authors": [
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Maurice Weber"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Yihong Chen"
                    },
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    }
                ],
                "author_detail": {
                    "name": "Pontus Stenetorp"
                },
                "author": "Pontus Stenetorp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23948v1",
                "updated": "2024-10-31T14:03:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    3,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:03:37Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    3,
                    37,
                    3,
                    305,
                    0
                ],
                "title": "Transformers to Predict the Applicability of Symbolic Integration\n  Routines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers to Predict the Applicability of Symbolic Integration\n  Routines"
                },
                "summary": "Symbolic integration is a fundamental problem in mathematics: we consider how\nmachine learning may be used to optimise this task in a Computer Algebra System\n(CAS). We train transformers that predict whether a particular integration\nmethod will be successful, and compare against the existing human-made\nheuristics (called guards) that perform this task in a leading CAS. We find the\ntransformer can outperform these guards, gaining up to 30% accuracy and 70%\nprecision. We further show that the inference time of the transformer is\ninconsequential which shows that it is well-suited to include as a guard in a\nCAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions\nthat the transformer is making. If guided by a subject-matter expert, the\ntechnique can explain some of the predictions based on the input tokens, which\ncan lead to further optimisations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic integration is a fundamental problem in mathematics: we consider how\nmachine learning may be used to optimise this task in a Computer Algebra System\n(CAS). We train transformers that predict whether a particular integration\nmethod will be successful, and compare against the existing human-made\nheuristics (called guards) that perform this task in a leading CAS. We find the\ntransformer can outperform these guards, gaining up to 30% accuracy and 70%\nprecision. We further show that the inference time of the transformer is\ninconsequential which shows that it is well-suited to include as a guard in a\nCAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions\nthat the transformer is making. If guided by a subject-matter expert, the\ntechnique can explain some of the predictions based on the input tokens, which\ncan lead to further optimisations."
                },
                "authors": [
                    {
                        "name": "Rashid Barket"
                    },
                    {
                        "name": "Uzma Shafiq"
                    },
                    {
                        "name": "Matthew England"
                    },
                    {
                        "name": "Juergen Gerhard"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Gerhard"
                },
                "author": "Juergen Gerhard",
                "arxiv_comment": "10 pages, 5 figures, to be published in NeurIPS 2024 MATH-AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16724v2",
                "updated": "2024-10-31T14:03:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    3,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-23T12:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    12,
                    38,
                    48,
                    1,
                    205,
                    0
                ],
                "title": "Structure-aware Domain Knowledge Injection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-aware Domain Knowledge Injection for Large Language Models"
                },
                "summary": "This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus requirement to a mere\n0.3%, while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes of human\nstudents, particularly how structured domain knowledge from textbooks is\nassimilated and subsequently applied to tackle real-world challenges through\nspecific exercises. Based on this, we propose a novel two-stage strategy for\nknowledge injection and alignment: Structure-aware Continual Pre-Training\n(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we\nautomatically extract the domain knowledge taxonomy and reorganize the training\ncorpora, enabling LLMs to effectively link textual segments to targeted\nknowledge points within the taxonomy. In the SSFT phase, we explicitly prompt\nmodels to elucidate the underlying knowledge structure in their outputs,\nleveraging the structured domain insight to address practical problems. Our\nultimate method has undergone extensive evaluations across model architectures\nand scales, using closed-book question-answering tasks on LongBench and\nMMedBench datasets. Remarkably, our method demonstrates the potential of\ncomparable improvement against the state-of-the-art MMedLM2 on MMedBench, while\nsignificantly reducing the training costs to 5%. This breakthrough paves the\nway for scaling up our StructTuning for stronger domain-specific LLMs with\ncomprehensive data utilization. Code is available at\nhttps://github.com/alibaba/struxgpt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus requirement to a mere\n0.3%, while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes of human\nstudents, particularly how structured domain knowledge from textbooks is\nassimilated and subsequently applied to tackle real-world challenges through\nspecific exercises. Based on this, we propose a novel two-stage strategy for\nknowledge injection and alignment: Structure-aware Continual Pre-Training\n(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we\nautomatically extract the domain knowledge taxonomy and reorganize the training\ncorpora, enabling LLMs to effectively link textual segments to targeted\nknowledge points within the taxonomy. In the SSFT phase, we explicitly prompt\nmodels to elucidate the underlying knowledge structure in their outputs,\nleveraging the structured domain insight to address practical problems. Our\nultimate method has undergone extensive evaluations across model architectures\nand scales, using closed-book question-answering tasks on LongBench and\nMMedBench datasets. Remarkably, our method demonstrates the potential of\ncomparable improvement against the state-of-the-art MMedLM2 on MMedBench, while\nsignificantly reducing the training costs to 5%. This breakthrough paves the\nway for scaling up our StructTuning for stronger domain-specific LLMs with\ncomprehensive data utilization. Code is available at\nhttps://github.com/alibaba/struxgpt."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Ze Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Rongxin Jiang"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Yaowu Chen"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Preprint. Code is available at https://github.com/alibaba/struxgpt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19073v2",
                "updated": "2024-10-31T13:59:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    59,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-27T10:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    10,
                    43,
                    4,
                    3,
                    179,
                    0
                ],
                "title": "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries"
                },
                "summary": "Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions."
                },
                "authors": [
                    {
                        "name": "Irina Saparina"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "NeurIPS 2024 D&B Track Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07146v2",
                "updated": "2024-10-31T13:54:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    54,
                    35,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-11T09:49:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    49,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling"
                },
                "summary": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via\n$\\operatorname{softmax}$, utilizing context-aware memory reading and adaptive\nforgetting to improve memory capacity while maintaining compact recurrent state\nsize. This design greatly enhances both training and inference efficiency\nthrough GLA's hardware-efficient training algorithm and reduced state size.\nAdditionally, retaining the $\\operatorname{softmax}$ operation is particularly\nbeneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings,\nreducing the need for extensive training from scratch. Extensive experiments\nconfirm GSA's superior performance in scenarios requiring in-context recall and\nin T2R settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via\n$\\operatorname{softmax}$, utilizing context-aware memory reading and adaptive\nforgetting to improve memory capacity while maintaining compact recurrent state\nsize. This design greatly enhances both training and inference efficiency\nthrough GLA's hardware-efficient training algorithm and reduced state size.\nAdditionally, retaining the $\\operatorname{softmax}$ operation is particularly\nbeneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings,\nreducing the need for extensive training from scratch. Extensive experiments\nconfirm GSA's superior performance in scenarios requiring in-context recall and\nin T2R settings."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Yiqiao Wang"
                    },
                    {
                        "name": "Bolun Wang"
                    },
                    {
                        "name": "Freda Shi"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Wei Bi"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Guohong Fu"
                    }
                ],
                "author_detail": {
                    "name": "Guohong Fu"
                },
                "author": "Guohong Fu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2008.11140v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2008.11140v7",
                "updated": "2024-10-31T13:52:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    52,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2020-08-25T16:11:37Z",
                "published_parsed": [
                    2020,
                    8,
                    25,
                    16,
                    11,
                    37,
                    1,
                    238,
                    0
                ],
                "title": "Inference for parameters identified by conditional moment restrictions\n  using a generalized Bierens maximum statistic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for parameters identified by conditional moment restrictions\n  using a generalized Bierens maximum statistic"
                },
                "summary": "Many economic panel and dynamic models, such as rational behavior and Euler\nequations, imply that the parameters of interest are identified by conditional\nmoment restrictions. We introduce a novel inference method without any prior\ninformation about which conditioning instruments are weak or irrelevant.\nBuilding on Bierens (1990), we propose penalized maximum statistics and combine\nbootstrap inference with model selection. Our method optimizes asymptotic power\nby solving a data-dependent max-min problem for tuning parameter selection.\nExtensive Monte Carlo experiments, based on an empirical example, demonstrate\nthe extent to which our inference procedure is superior to those available in\nthe literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many economic panel and dynamic models, such as rational behavior and Euler\nequations, imply that the parameters of interest are identified by conditional\nmoment restrictions. We introduce a novel inference method without any prior\ninformation about which conditioning instruments are weak or irrelevant.\nBuilding on Bierens (1990), we propose penalized maximum statistics and combine\nbootstrap inference with model selection. Our method optimizes asymptotic power\nby solving a data-dependent max-min problem for tuning parameter selection.\nExtensive Monte Carlo experiments, based on an empirical example, demonstrate\nthe extent to which our inference procedure is superior to those available in\nthe literature."
                },
                "authors": [
                    {
                        "name": "Xiaohong Chen"
                    },
                    {
                        "name": "Sokbae Lee"
                    },
                    {
                        "name": "Myung Hwan Seo"
                    },
                    {
                        "name": "Myunghyun Song"
                    }
                ],
                "author_detail": {
                    "name": "Myunghyun Song"
                },
                "author": "Myunghyun Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2008.11140v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2008.11140v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22086v2",
                "updated": "2024-10-31T13:50:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    50,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate"
                },
                "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training."
                },
                "authors": [
                    {
                        "name": "Zhiqi Bu"
                    },
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Bhanukiran Vinzamuri"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23933v1",
                "updated": "2024-10-31T13:47:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    47,
                    10,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:47:10Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    47,
                    10,
                    3,
                    305,
                    0
                ],
                "title": "Language Models can Self-Lengthen to Generate Long Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Self-Lengthen to Generate Long Texts"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23918v1",
                "updated": "2024-10-31T13:26:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    26,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:26:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    26,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments"
                },
                "summary": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack."
                },
                "authors": [
                    {
                        "name": "Xinghao Wang"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23913v1",
                "updated": "2024-10-31T13:19:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    19,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:19:31Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    19,
                    31,
                    3,
                    305,
                    0
                ],
                "title": "Efficient Inference and Computation of Optimal Alternatives for\n  Preference Languages Based On Lexicographic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference and Computation of Optimal Alternatives for\n  Preference Languages Based On Lexicographic Models"
                },
                "summary": "We analyse preference inference, through consistency, for general preference\nlanguages based on lexicographic models. We identify a property, which we call\nstrong compositionality, that applies for many natural kinds of preference\nstatement, and that allows a greedy algorithm for determining consistency of a\nset of preference statements. We also consider different natural definitions of\noptimality, and their relations to each other, for general preference languages\nbased on lexicographic models. Based on our framework, we show that testing\nconsistency, and thus inference, is polynomial for a specific preference\nlanguage LpqT, which allows strict and non-strict statements, comparisons\nbetween outcomes and between partial tuples, both ceteris paribus and strong\nstatements, and their combination. Computing different kinds of optimal sets is\nalso shown to be polynomial; this is backed up by our experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse preference inference, through consistency, for general preference\nlanguages based on lexicographic models. We identify a property, which we call\nstrong compositionality, that applies for many natural kinds of preference\nstatement, and that allows a greedy algorithm for determining consistency of a\nset of preference statements. We also consider different natural definitions of\noptimality, and their relations to each other, for general preference languages\nbased on lexicographic models. Based on our framework, we show that testing\nconsistency, and thus inference, is polynomial for a specific preference\nlanguage LpqT, which allows strict and non-strict statements, comparisons\nbetween outcomes and between partial tuples, both ceteris paribus and strong\nstatements, and their combination. Computing different kinds of optimal sets is\nalso shown to be polynomial; this is backed up by our experimental results."
                },
                "authors": [
                    {
                        "name": "Nic Wilson"
                    },
                    {
                        "name": "Anne-Marie George"
                    }
                ],
                "author_detail": {
                    "name": "Anne-Marie George"
                },
                "author": "Anne-Marie George",
                "arxiv_comment": "Longer Version of IJCAI'17 publication\n  https://www.ijcai.org/proceedings/2017/0182.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23912v1",
                "updated": "2024-10-31T13:17:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    17,
                    53,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:17:53Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    17,
                    53,
                    3,
                    305,
                    0
                ],
                "title": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner"
                },
                "summary": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Yu-Ting Lee"
                    },
                    {
                        "name": "Hui-Ying Shih"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20087v2",
                "updated": "2024-10-31T13:10:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    10,
                    12,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-28T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    55,
                    24,
                    4,
                    180,
                    0
                ],
                "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgressGym: Alignment with a Millennium of Moral Progress"
                },
                "summary": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively."
                },
                "authors": [
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xuchuan Huang"
                    },
                    {
                        "name": "Jasmine Xinze Li"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16434v2",
                "updated": "2024-10-31T13:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-23T12:33:58Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    12,
                    33,
                    58,
                    1,
                    205,
                    0
                ],
                "title": "Enhancing LLM's Cognition via Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM's Cognition via Structurization"
                },
                "summary": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rongxin Jiang"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Yaowu Chen"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "This paper has been accepted by NeurIPS 2024. Code is available at\n  https://github.com/alibaba/struxgpt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23904v1",
                "updated": "2024-10-31T13:06:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:06:29Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection"
                },
                "summary": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04559v3",
                "updated": "2024-10-31T12:59:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    59,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-07T03:37:19Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    37,
                    19,
                    2,
                    38,
                    0
                ],
                "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Model Agents Simulate Human Trust Behavior?"
                },
                "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Feiran Jia"
                    },
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "David Jurgens"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Guohao Li"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Li"
                },
                "author": "Guohao Li",
                "arxiv_comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20745v2",
                "updated": "2024-10-31T12:54:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    54,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-28T05:25:47Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    25,
                    47,
                    0,
                    302,
                    0
                ],
                "title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models"
                },
                "summary": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/."
                },
                "authors": [
                    {
                        "name": "Yilun Jin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Pratik Jayarao"
                    },
                    {
                        "name": "Mao Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Ritesh Sarkhel"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Haodong Wang"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Wenju Xu"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Bing Yin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yin"
                },
                "author": "Bing Yin",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos\n  in Figure 9",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23894v1",
                "updated": "2024-10-31T12:53:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    53,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:53:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    53,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models"
                },
                "summary": "Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines."
                },
                "authors": [
                    {
                        "name": "Pooria Madani"
                    }
                ],
                "author_detail": {
                    "name": "Pooria Madani"
                },
                "author": "Pooria Madani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23890v1",
                "updated": "2024-10-31T12:52:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    52,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:52:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    52,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages"
                },
                "summary": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies."
                },
                "authors": [
                    {
                        "name": "Séamus Lankford"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.02370,\n  arXiv:2403.01580",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23884v1",
                "updated": "2024-10-31T12:48:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:48:58Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "title": "Failure Modes of LLMs for Causal Reasoning on Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Modes of LLMs for Causal Reasoning on Narratives"
                },
                "summary": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Khurram Yamin"
                    },
                    {
                        "name": "Shantanu Gupta"
                    },
                    {
                        "name": "Gaurav R. Ghosal"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Bryan Wilder"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wilder"
                },
                "author": "Bryan Wilder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23882v1",
                "updated": "2024-10-31T12:45:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    45,
                    50,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:45:50Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    45,
                    50,
                    3,
                    305,
                    0
                ],
                "title": "In-Context Learned Equalization in Cell-Free Massive MIMO via\n  State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learned Equalization in Cell-Free Massive MIMO via\n  State-Space Models"
                },
                "summary": "Sequence models have demonstrated the ability to perform tasks like channel\nequalization and symbol detection by automatically adapting to current channel\nconditions. This is done without requiring any explicit optimization and by\nleveraging not only short pilot sequences but also contextual information such\nas long-term channel statistics. The operating principle underlying automatic\nadaptation is in-context learning (ICL), an emerging property of sequence\nmodels. Prior art adopted transformer-based sequence models, which, however,\nhave a computational complexity scaling quadratically with the context length\ndue to batch processing. Recently, state-space models (SSMs) have emerged as a\nmore efficient alternative, affording a linear inference complexity in the\ncontext size. This work explores the potential of SSMs for ICL-based\nequalization in cell-free massive MIMO systems. Results show that selective\nSSMs achieve comparable performance to transformer-based models while requiring\napproximately eight times fewer parameters and five times fewer floating-point\noperations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models have demonstrated the ability to perform tasks like channel\nequalization and symbol detection by automatically adapting to current channel\nconditions. This is done without requiring any explicit optimization and by\nleveraging not only short pilot sequences but also contextual information such\nas long-term channel statistics. The operating principle underlying automatic\nadaptation is in-context learning (ICL), an emerging property of sequence\nmodels. Prior art adopted transformer-based sequence models, which, however,\nhave a computational complexity scaling quadratically with the context length\ndue to batch processing. Recently, state-space models (SSMs) have emerged as a\nmore efficient alternative, affording a linear inference complexity in the\ncontext size. This work explores the potential of SSMs for ICL-based\nequalization in cell-free massive MIMO systems. Results show that selective\nSSMs achieve comparable performance to transformer-based models while requiring\napproximately eight times fewer parameters and five times fewer floating-point\noperations."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23881v1",
                "updated": "2024-10-31T12:44:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    44,
                    7,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:44:07Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    44,
                    7,
                    3,
                    305,
                    0
                ],
                "title": "DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge"
                },
                "summary": "The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines."
                },
                "authors": [
                    {
                        "name": "Daniel May"
                    },
                    {
                        "name": "Alessandro Tundo"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23879v1",
                "updated": "2024-10-31T12:40:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    40,
                    38,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    40,
                    38,
                    3,
                    305,
                    0
                ],
                "title": "Investigating Bias in Political Search Query Suggestions by Relative\n  Comparison with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Bias in Political Search Query Suggestions by Relative\n  Comparison with LLMs"
                },
                "summary": "Search query suggestions affect users' interactions with search engines,\nwhich then influences the information they encounter. Thus, bias in search\nquery suggestions can lead to exposure to biased search results and can impact\nopinion formation. This is especially critical in the political domain.\nDetecting and quantifying bias in web search engines is difficult due to its\ntopic dependency, complexity, and subjectivity. The lack of context and\nphrasality of query suggestions emphasizes this problem. In a multi-step\napproach, we combine the benefits of large language models, pairwise\ncomparison, and Elo-based scoring to identify and quantify bias in English\nsearch query suggestions. We apply our approach to the U.S. political news\ndomain and compare bias in Google and Bing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search query suggestions affect users' interactions with search engines,\nwhich then influences the information they encounter. Thus, bias in search\nquery suggestions can lead to exposure to biased search results and can impact\nopinion formation. This is especially critical in the political domain.\nDetecting and quantifying bias in web search engines is difficult due to its\ntopic dependency, complexity, and subjectivity. The lack of context and\nphrasality of query suggestions emphasizes this problem. In a multi-step\napproach, we combine the benefits of large language models, pairwise\ncomparison, and Elo-based scoring to identify and quantify bias in English\nsearch query suggestions. We apply our approach to the U.S. political news\ndomain and compare bias in Google and Bing."
                },
                "authors": [
                    {
                        "name": "Fabian Haak"
                    },
                    {
                        "name": "Björn Engelmann"
                    },
                    {
                        "name": "Christin Katharina Kreutz"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3630744.3658415",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3630744.3658415",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23875v1",
                "updated": "2024-10-31T12:37:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    37,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:37:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    37,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model\n  on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model\n  on Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG."
                },
                "authors": [
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Panrong Tong"
                    },
                    {
                        "name": "Zhongming Jin"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v2",
                "updated": "2024-10-31T12:34:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    34,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5\\% of extraversion-related test instances, while\nMistral-7B-Instruct did so in 92.5\\% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analysing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5\\% of extraversion-related test instances, while\nMistral-7B-Instruct did so in 92.5\\% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analysing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NeurIPS 2024 Workshop on Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02130v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02130v5",
                "updated": "2024-10-31T12:27:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-03T12:19:47Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    12,
                    19,
                    47,
                    5,
                    34,
                    0
                ],
                "title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset."
                },
                "authors": [
                    {
                        "name": "Yanbin Wei"
                    },
                    {
                        "name": "Shuai Fu"
                    },
                    {
                        "name": "Weisen Jiang"
                    },
                    {
                        "name": "Zejian Zhang"
                    },
                    {
                        "name": "Zhixiong Zeng"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "James T. Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "NeurIPS 2024; Project Page: v-graph.github.io; Code:\n  https://github.com/WEIYanbin1999/GITA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02130v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02130v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03941v2",
                "updated": "2024-10-31T12:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    30,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-06T12:18:54Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    12,
                    18,
                    54,
                    1,
                    37,
                    0
                ],
                "title": "Discovery of the Hidden World with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of the Hidden World with Large Language Models"
                },
                "summary": "Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "James Cheng"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "arxiv_comment": "NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72\n  figures; Project page: https://causalcoat.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12404v3",
                "updated": "2024-10-31T12:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-15T17:49:16Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    49,
                    16,
                    0,
                    106,
                    0
                ],
                "title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/"
                },
                "authors": [
                    {
                        "name": "Jinhee Kim"
                    },
                    {
                        "name": "Taesung Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06255v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06255v4",
                "updated": "2024-10-31T12:24:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    24,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-09T09:09:39Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    9,
                    9,
                    39,
                    4,
                    40,
                    0
                ],
                "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning"
                },
                "summary": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT."
                },
                "authors": [
                    {
                        "name": "Yichuan Mo"
                    },
                    {
                        "name": "Yuji Wang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06255v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06255v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23866v1",
                "updated": "2024-10-31T12:20:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    20,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:20:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    20,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Evaluating and Improving ChatGPT-Based Expansion of Abbreviations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving ChatGPT-Based Expansion of Abbreviations"
                },
                "summary": "Source code identifiers often contain abbreviations. Such abbreviations may\nreduce the readability of the source code, which in turn hinders the\nmaintenance of the software applications. To this end, accurate and automated\napproaches to expanding abbreviations in source code are desirable and\nabbreviation expansion has been intensively investigated. However, to the best\nof our knowledge, most existing approaches are heuristics, and none of them has\neven employed deep learning techniques, let alone the most advanced large\nlanguage models (LLMs). LLMs have demonstrated cutting-edge performance in\nvarious software engineering tasks, and thus it has the potential to expand\nabbreviation automatically. To this end, in this paper, we present the first\nempirical study on LLM-based abbreviation expansion. Our evaluation results on\na public benchmark suggest that ChatGPT is substantially less accurate than the\nstate-of-the-art approach, reducing precision and recall by 28.2\\% and 27.8\\%,\nrespectively. We manually analyzed the failed cases, and discovered the root\ncauses for the failures: 1) Lack of contexts and 2) Inability to recognize\nabbreviations. In response to the first cause, we investigated the effect of\nvarious contexts and found surrounding source code is the best selection. In\nresponse to the second cause, we designed an iterative approach that identifies\nand explicitly marks missed abbreviations in prompts. Finally, we proposed a\npost-condition checking to exclude incorrect expansions that violate\ncommonsense. All such measures together make ChatGPT-based abbreviation\nexpansion comparable to the state of the art while avoiding expensive source\ncode parsing and deep analysis that are indispensable for state-of-the-art\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code identifiers often contain abbreviations. Such abbreviations may\nreduce the readability of the source code, which in turn hinders the\nmaintenance of the software applications. To this end, accurate and automated\napproaches to expanding abbreviations in source code are desirable and\nabbreviation expansion has been intensively investigated. However, to the best\nof our knowledge, most existing approaches are heuristics, and none of them has\neven employed deep learning techniques, let alone the most advanced large\nlanguage models (LLMs). LLMs have demonstrated cutting-edge performance in\nvarious software engineering tasks, and thus it has the potential to expand\nabbreviation automatically. To this end, in this paper, we present the first\nempirical study on LLM-based abbreviation expansion. Our evaluation results on\na public benchmark suggest that ChatGPT is substantially less accurate than the\nstate-of-the-art approach, reducing precision and recall by 28.2\\% and 27.8\\%,\nrespectively. We manually analyzed the failed cases, and discovered the root\ncauses for the failures: 1) Lack of contexts and 2) Inability to recognize\nabbreviations. In response to the first cause, we investigated the effect of\nvarious contexts and found surrounding source code is the best selection. In\nresponse to the second cause, we designed an iterative approach that identifies\nand explicitly marks missed abbreviations in prompts. Finally, we proposed a\npost-condition checking to exclude incorrect expansions that violate\ncommonsense. All such measures together make ChatGPT-based abbreviation\nexpansion comparable to the state of the art while avoiding expensive source\ncode parsing and deep analysis that are indispensable for state-of-the-art\napproaches."
                },
                "authors": [
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23861v1",
                "updated": "2024-10-31T12:11:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    11,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:11:17Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    11,
                    17,
                    3,
                    305,
                    0
                ],
                "title": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23856v1",
                "updated": "2024-10-31T12:07:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    7,
                    44,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:07:44Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    7,
                    44,
                    3,
                    305,
                    0
                ],
                "title": "Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Rong Tao"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Yiwen Luo"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23855v1",
                "updated": "2024-10-31T12:05:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    5,
                    21,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:05:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    5,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "RAGraph: A General Retrieval-Augmented Graph Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGraph: A General Retrieval-Augmented Graph Learning Framework"
                },
                "summary": "Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability."
                },
                "authors": [
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Rihong Qiu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yuchen Fang"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23852v1",
                "updated": "2024-10-31T12:02:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    2,
                    7,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:02:07Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    2,
                    7,
                    3,
                    305,
                    0
                ],
                "title": "Estimation and Inference in Dyadic Network Formation Models with\n  Nontransferable Utilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Inference in Dyadic Network Formation Models with\n  Nontransferable Utilities"
                },
                "summary": "This paper studies estimation and inference in a dyadic network formation\nmodel with observed covariates, unobserved heterogeneity, and nontransferable\nutilities. With the presence of the high dimensional fixed effects, the maximum\nlikelihood estimator is numerically difficult to compute and suffers from the\nincidental parameter bias. We propose an easy-to-compute one-step estimator for\nthe homophily parameter of interest, which is further refined to achieve\n$\\sqrt{N}$-consistency via split-network jackknife and efficiency by the\nbootstrap aggregating (bagging) technique. We establish consistency for the\nestimator of the fixed effects and prove asymptotic normality for the\nunconditional average partial effects. Simulation studies show that our method\nworks well with finite samples, and an empirical application using the\nrisk-sharing data from Nyakatoke highlights the importance of employing proper\nstatistical inferential procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies estimation and inference in a dyadic network formation\nmodel with observed covariates, unobserved heterogeneity, and nontransferable\nutilities. With the presence of the high dimensional fixed effects, the maximum\nlikelihood estimator is numerically difficult to compute and suffers from the\nincidental parameter bias. We propose an easy-to-compute one-step estimator for\nthe homophily parameter of interest, which is further refined to achieve\n$\\sqrt{N}$-consistency via split-network jackknife and efficiency by the\nbootstrap aggregating (bagging) technique. We establish consistency for the\nestimator of the fixed effects and prove asymptotic normality for the\nunconditional average partial effects. Simulation studies show that our method\nworks well with finite samples, and an empirical application using the\nrisk-sharing data from Nyakatoke highlights the importance of employing proper\nstatistical inferential procedures."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Zhentao Shi"
                    },
                    {
                        "name": "Yapeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yapeng Zheng"
                },
                "author": "Yapeng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23851v1",
                "updated": "2024-10-31T12:01:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    1,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    1,
                    51,
                    3,
                    305,
                    0
                ],
                "title": "Leveraging Large Language Models for Medical Information Extraction and\n  Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Medical Information Extraction and\n  Query Generation"
                },
                "summary": "This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings."
                },
                "authors": [
                    {
                        "name": "Georgios Peikos"
                    },
                    {
                        "name": "Pranav Kasela"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11965v2",
                "updated": "2024-10-31T11:52:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    52,
                    57,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-17T18:00:02Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    0,
                    2,
                    0,
                    169,
                    0
                ],
                "title": "Be careful in multi-messenger inference of the Hubble constant: A path\n  forward for robust inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Be careful in multi-messenger inference of the Hubble constant: A path\n  forward for robust inference"
                },
                "summary": "Multi-messenger observations of coalescing binary neutron stars (BNSs) are a\ndirect probe of the expansion history of the universe and carry the potential\nto shed light on the disparity between low- and high-redshift measurements of\nthe Hubble constant $H_0$. To measure the value of $H_0$ with such observations\nrequires pristine inference of the luminosity distance and the true source\nredshift with minimal impact from systematics. In this analysis, we carry out\njoint inference on mock gravitational wave (GW) signals and their\nelectromagnetic (EM) afterglows from BNS coalescences and find that the\ninclination angle inferred from the afterglow light curve and apparent\nsuperluminal motion can be precise, but need not be accurate and is subject to\nsystematic uncertainty that could be as large as $1.5\\sigma$. This produces a\ndisparity between the EM and GW inferred inclination angles, which if not\ncarefully treated when combining observations can bias the inferred value of\n$H_0$. We also find that already small misalignments of $3^{\\circ}-6^{\\circ}$\nbetween the inherent system inclinations for the GW and EM emission can bias\nthe inference by $\\mathcal{O}(1-2\\sigma)$ if not taken into account. As\nmulti-messenger BNS observations are rare, we must make the most out of a small\nnumber of events and harness the increased precision, while avoiding reduced\naccuracy. We demonstrate how to mitigate these potential sources of bias by\njointly inferring the mismatch between the GW- and EM-based inclination angles\nand $H_0$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-messenger observations of coalescing binary neutron stars (BNSs) are a\ndirect probe of the expansion history of the universe and carry the potential\nto shed light on the disparity between low- and high-redshift measurements of\nthe Hubble constant $H_0$. To measure the value of $H_0$ with such observations\nrequires pristine inference of the luminosity distance and the true source\nredshift with minimal impact from systematics. In this analysis, we carry out\njoint inference on mock gravitational wave (GW) signals and their\nelectromagnetic (EM) afterglows from BNS coalescences and find that the\ninclination angle inferred from the afterglow light curve and apparent\nsuperluminal motion can be precise, but need not be accurate and is subject to\nsystematic uncertainty that could be as large as $1.5\\sigma$. This produces a\ndisparity between the EM and GW inferred inclination angles, which if not\ncarefully treated when combining observations can bias the inferred value of\n$H_0$. We also find that already small misalignments of $3^{\\circ}-6^{\\circ}$\nbetween the inherent system inclinations for the GW and EM emission can bias\nthe inference by $\\mathcal{O}(1-2\\sigma)$ if not taken into account. As\nmulti-messenger BNS observations are rare, we must make the most out of a small\nnumber of events and harness the increased precision, while avoiding reduced\naccuracy. We demonstrate how to mitigate these potential sources of bias by\njointly inferring the mismatch between the GW- and EM-based inclination angles\nand $H_0$."
                },
                "authors": [
                    {
                        "name": "Michael Müller"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "Geoffrey Ryan"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ryan"
                },
                "author": "Geoffrey Ryan",
                "arxiv_comment": "21 pages, 10 figures, accepted in ApJL on October 24, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23844v1",
                "updated": "2024-10-31T11:50:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    50,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:50:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    50,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Knowledge Editing Based on Free-Text in LLMs"
                },
                "summary": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance."
                },
                "authors": [
                    {
                        "name": "Xiusheng Huang"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23841v1",
                "updated": "2024-10-31T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:47:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models"
                },
                "summary": "Instruction-following capabilities in large language models (LLMs) have\nsignificantly progressed, enabling more complex user interactions through\ndetailed prompts. However, retrieval systems have not matched these advances,\nmost of them still relies on traditional lexical and semantic matching\ntechniques that fail to fully capture user intent. Recent efforts have\nintroduced instruction-aware retrieval models, but these primarily focus on\nintrinsic content relevance, which neglects the importance of customized\npreferences for broader document-level attributes. This study evaluates the\ninstruction-following capabilities of various retrieval models beyond content\nrelevance, including LLM-based dense retrieval and reranking models. We develop\nInfoSearch, a novel retrieval evaluation benchmark spanning six document-level\nattributes: Audience, Keyword, Format, Language, Length, and Source, and\nintroduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and\nWeighted Instruction Sensitivity Evaluation (WISE) to accurately assess the\nmodels' responsiveness to instructions. Our findings reveal that while\nreranking models generally surpass retrieval models in instruction following,\nthey still face challenges in handling certain attributes. Moreover, although\ninstruction fine-tuning and increased model size lead to better performance,\nmost models fall short of achieving comprehensive instruction compliance as\nassessed by our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following capabilities in large language models (LLMs) have\nsignificantly progressed, enabling more complex user interactions through\ndetailed prompts. However, retrieval systems have not matched these advances,\nmost of them still relies on traditional lexical and semantic matching\ntechniques that fail to fully capture user intent. Recent efforts have\nintroduced instruction-aware retrieval models, but these primarily focus on\nintrinsic content relevance, which neglects the importance of customized\npreferences for broader document-level attributes. This study evaluates the\ninstruction-following capabilities of various retrieval models beyond content\nrelevance, including LLM-based dense retrieval and reranking models. We develop\nInfoSearch, a novel retrieval evaluation benchmark spanning six document-level\nattributes: Audience, Keyword, Format, Language, Length, and Source, and\nintroduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and\nWeighted Instruction Sensitivity Evaluation (WISE) to accurately assess the\nmodels' responsiveness to instructions. Our findings reveal that while\nreranking models generally surpass retrieval models in instruction following,\nthey still face challenges in handling certain attributes. Moreover, although\ninstruction fine-tuning and increased model size lead to better performance,\nmost models fall short of achieving comprehensive instruction compliance as\nassessed by our benchmark."
                },
                "authors": [
                    {
                        "name": "Jianqun Zhou"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Qianqian Zheng"
                    },
                    {
                        "name": "Zeyuan Shang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23838v1",
                "updated": "2024-10-31T11:42:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    42,
                    34,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:42:34Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    42,
                    34,
                    3,
                    305,
                    0
                ],
                "title": "Zero-inflated stochastic block modeling of efficiency-security tradeoffs\n  in weighted criminal networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-inflated stochastic block modeling of efficiency-security tradeoffs\n  in weighted criminal networks"
                },
                "summary": "Criminal networks arise from the unique attempt to balance a need of\nestablishing frequent ties among affiliates to facilitate the coordination of\nillegal activities, with the necessity to sparsify the overall connectivity\narchitecture to hide from law enforcement. This efficiency-security tradeoff is\nalso combined with the creation of groups of redundant criminals that exhibit\nsimilar connectivity patterns, thus guaranteeing resilient network\narchitectures. State-of-the-art models for such data are not designed to infer\nthese unique structures. In contrast to such solutions we develop a\ncomputationally-tractable Bayesian zero-inflated Poisson stochastic block model\n(ZIP-SBM), which identifies groups of redundant criminals with similar\nconnectivity patterns, and infers both overt and covert block interactions\nwithin and across such groups. This is accomplished by modeling weighted ties\n(corresponding to counts of interactions among pairs of criminals) via\nzero-inflated Poisson distributions with block-specific parameters that\nquantify complex patterns in the excess of zero ties in each block (security)\nrelative to the distribution of the observed weighted ties within that block\n(efficiency). The performance of ZIP-SBM is illustrated in simulations and in a\nstudy of summits co-attendances in a complex Mafia organization, where we\nunveil efficiency-security structures adopted by the criminal organization that\nwere hidden to previous analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal networks arise from the unique attempt to balance a need of\nestablishing frequent ties among affiliates to facilitate the coordination of\nillegal activities, with the necessity to sparsify the overall connectivity\narchitecture to hide from law enforcement. This efficiency-security tradeoff is\nalso combined with the creation of groups of redundant criminals that exhibit\nsimilar connectivity patterns, thus guaranteeing resilient network\narchitectures. State-of-the-art models for such data are not designed to infer\nthese unique structures. In contrast to such solutions we develop a\ncomputationally-tractable Bayesian zero-inflated Poisson stochastic block model\n(ZIP-SBM), which identifies groups of redundant criminals with similar\nconnectivity patterns, and infers both overt and covert block interactions\nwithin and across such groups. This is accomplished by modeling weighted ties\n(corresponding to counts of interactions among pairs of criminals) via\nzero-inflated Poisson distributions with block-specific parameters that\nquantify complex patterns in the excess of zero ties in each block (security)\nrelative to the distribution of the observed weighted ties within that block\n(efficiency). The performance of ZIP-SBM is illustrated in simulations and in a\nstudy of summits co-attendances in a complex Mafia organization, where we\nunveil efficiency-security structures adopted by the criminal organization that\nwere hidden to previous analyses."
                },
                "authors": [
                    {
                        "name": "Chaoyi Lu"
                    },
                    {
                        "name": "Daniele Durante"
                    },
                    {
                        "name": "Nial Friel"
                    }
                ],
                "author_detail": {
                    "name": "Nial Friel"
                },
                "author": "Nial Friel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17557v2",
                "updated": "2024-10-31T11:37:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    37,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-25T13:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    13,
                    50,
                    56,
                    1,
                    177,
                    0
                ],
                "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale"
                },
                "summary": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments."
                },
                "authors": [
                    {
                        "name": "Guilherme Penedo"
                    },
                    {
                        "name": "Hynek Kydlíček"
                    },
                    {
                        "name": "Loubna Ben allal"
                    },
                    {
                        "name": "Anton Lozhkov"
                    },
                    {
                        "name": "Margaret Mitchell"
                    },
                    {
                        "name": "Colin Raffel"
                    },
                    {
                        "name": "Leandro Von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02370v4",
                "updated": "2024-10-31T11:37:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    37,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-04T06:59:21Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    6,
                    59,
                    21,
                    6,
                    35,
                    0
                ],
                "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTimes: Autoregressive Time Series Forecasters via Large Language\n  Models"
                },
                "summary": "Foundation models of time series have not been fully developed due to the\nlimited availability of time series corpora and the underexploration of\nscalable pre-training. Based on the similar sequential formulation of time\nseries and natural language, increasing research demonstrates the feasibility\nof leveraging large language models (LLM) for time series. Nevertheless, the\ninherent autoregressive property and decoder-only architecture of LLMs have not\nbeen fully considered, resulting in insufficient utilization of LLM abilities.\nTo fully revitalize the general-purpose token transition and multi-step\ngeneration capability of large language models, we propose AutoTimes to\nrepurpose LLMs as autoregressive time series forecasters, which projects time\nseries into the embedding space of language tokens and autoregressively\ngenerates future predictions with arbitrary lengths. Compatible with any\ndecoder-only LLMs, the consequent forecaster exhibits the flexibility of the\nlookback length and scalability with larger LLMs. Further, we formulate time\nseries as prompts, extending the context for prediction beyond the lookback\nwindow, termed in-context forecasting. By introducing LLM-embedded textual\ntimestamps, AutoTimes can utilize chronological information to align\nmultivariate time series. Empirically, AutoTimes achieves state-of-the-art with\n0.1% trainable parameters and over $5\\times$ training/inference speedup\ncompared to advanced LLM-based forecasters. Code is available at this\nrepository: https://github.com/thuml/AutoTimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models of time series have not been fully developed due to the\nlimited availability of time series corpora and the underexploration of\nscalable pre-training. Based on the similar sequential formulation of time\nseries and natural language, increasing research demonstrates the feasibility\nof leveraging large language models (LLM) for time series. Nevertheless, the\ninherent autoregressive property and decoder-only architecture of LLMs have not\nbeen fully considered, resulting in insufficient utilization of LLM abilities.\nTo fully revitalize the general-purpose token transition and multi-step\ngeneration capability of large language models, we propose AutoTimes to\nrepurpose LLMs as autoregressive time series forecasters, which projects time\nseries into the embedding space of language tokens and autoregressively\ngenerates future predictions with arbitrary lengths. Compatible with any\ndecoder-only LLMs, the consequent forecaster exhibits the flexibility of the\nlookback length and scalability with larger LLMs. Further, we formulate time\nseries as prompts, extending the context for prediction beyond the lookback\nwindow, termed in-context forecasting. By introducing LLM-embedded textual\ntimestamps, AutoTimes can utilize chronological information to align\nmultivariate time series. Empirically, AutoTimes achieves state-of-the-art with\n0.1% trainable parameters and over $5\\times$ training/inference speedup\ncompared to advanced LLM-based forecasters. Code is available at this\nrepository: https://github.com/thuml/AutoTimes."
                },
                "authors": [
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Guo Qin"
                    },
                    {
                        "name": "Xiangdong Huang"
                    },
                    {
                        "name": "Jianmin Wang"
                    },
                    {
                        "name": "Mingsheng Long"
                    }
                ],
                "author_detail": {
                    "name": "Mingsheng Long"
                },
                "author": "Mingsheng Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23836v1",
                "updated": "2024-10-31T11:32:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    32,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:32:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    32,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts"
                },
                "summary": "This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes."
                },
                "authors": [
                    {
                        "name": "Xiang Deng"
                    },
                    {
                        "name": "Youxin Pang"
                    },
                    {
                        "name": "Xiaochen Zhao"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Lizhen Wang"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Shi Yan"
                    },
                    {
                        "name": "Hongwen Zhang"
                    },
                    {
                        "name": "Yebin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yebin Liu"
                },
                "author": "Yebin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10048v2",
                "updated": "2024-10-31T11:29:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    29,
                    38,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-15T18:00:05Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    18,
                    0,
                    5,
                    0,
                    106,
                    0
                ],
                "title": "Classifying binary black holes from Population III stars with the\n  Einstein Telescope: A machine-learning approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifying binary black holes from Population III stars with the\n  Einstein Telescope: A machine-learning approach"
                },
                "summary": "Third-generation (3G) gravitational-wave detectors such as the Einstein\nTelescope (ET) will observe binary black hole (BBH) mergers at redshifts up to\n$z\\sim 100$. However, an unequivocal determination of the origin of\nhigh-redshift sources will remain uncertain because of the low signal-to-noise\nratio (S/N) and poor estimate of their luminosity distance. This study proposes\na machine-learning approach to infer the origins of high-redshift BBHs. We\nspecifically differentiate those arising from Population III (Pop. III) stars,\nwhich probably are the first progenitors of star-born BBH mergers in the\nUniverse, and those originated from Population I-II (Pop. I-II) stars. We\nconsidered a wide range of models that encompass the current uncertainties on\nPop. III BBH mergers. We then estimated the parameter errors of the detected\nsources with ET using the Fisher information-matrix formalism, followed by a\nclassification using XGBoost, which is a machine-learning algorithm based on\ndecision trees. For a set of mock observed BBHs, we provide the probability\nthat they belong to the Pop. III class while considering the parameter errors\nof each source. In our fiducial model, we accurately identify $\\gtrsim 10\\%$ of\nthe detected BBHs that originate from Pop. III stars with a precision $>90\\%$.\nOur study demonstrates that machine-learning enables us to achieve some pivotal\naspects of the ET science case by exploring the origin of individual\nhigh-redshift GW observations. We set the basis for further studies, which will\nintegrate additional simulated populations and account for further\nuncertainties in the population modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Third-generation (3G) gravitational-wave detectors such as the Einstein\nTelescope (ET) will observe binary black hole (BBH) mergers at redshifts up to\n$z\\sim 100$. However, an unequivocal determination of the origin of\nhigh-redshift sources will remain uncertain because of the low signal-to-noise\nratio (S/N) and poor estimate of their luminosity distance. This study proposes\na machine-learning approach to infer the origins of high-redshift BBHs. We\nspecifically differentiate those arising from Population III (Pop. III) stars,\nwhich probably are the first progenitors of star-born BBH mergers in the\nUniverse, and those originated from Population I-II (Pop. I-II) stars. We\nconsidered a wide range of models that encompass the current uncertainties on\nPop. III BBH mergers. We then estimated the parameter errors of the detected\nsources with ET using the Fisher information-matrix formalism, followed by a\nclassification using XGBoost, which is a machine-learning algorithm based on\ndecision trees. For a set of mock observed BBHs, we provide the probability\nthat they belong to the Pop. III class while considering the parameter errors\nof each source. In our fiducial model, we accurately identify $\\gtrsim 10\\%$ of\nthe detected BBHs that originate from Pop. III stars with a precision $>90\\%$.\nOur study demonstrates that machine-learning enables us to achieve some pivotal\naspects of the ET science case by exploring the origin of individual\nhigh-redshift GW observations. We set the basis for further studies, which will\nintegrate additional simulated populations and account for further\nuncertainties in the population modeling."
                },
                "authors": [
                    {
                        "name": "Filippo Santoliquido"
                    },
                    {
                        "name": "Ulyana Dupletsa"
                    },
                    {
                        "name": "Jacopo Tissino"
                    },
                    {
                        "name": "Marica Branchesi"
                    },
                    {
                        "name": "Francesco Iacovelli"
                    },
                    {
                        "name": "Giuliano Iorio"
                    },
                    {
                        "name": "Michela Mapelli"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Jan Harms"
                    },
                    {
                        "name": "Mario Pasquato"
                    }
                ],
                "author_detail": {
                    "name": "Mario Pasquato"
                },
                "author": "Mario Pasquato",
                "arxiv_doi": "10.1051/0004-6361/202450381",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450381",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Astronomy & Astrophysics. 15 pages, 9 Figures and 6\n  tables. Comments are welcome",
                "arxiv_journal_ref": "A&A 690, A362 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v2",
                "updated": "2024-10-31T11:16:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    16,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "arxiv_comment": "24 pages, 10 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09881v2",
                "updated": "2024-10-31T11:14:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    14,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-08-19T10:46:19Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    46,
                    19,
                    0,
                    232,
                    0
                ],
                "title": "Uncertainty Quantification of Surrogate Models using Conformal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification of Surrogate Models using Conformal\n  Prediction"
                },
                "summary": "Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models of physical systems do not quantify their\nuncertainty, rendering their predictions unreliable, requiring further\nvalidation. Though Bayesian approximations offer some solace in estimating the\nerror associated with these models, they cannot provide guarantees, and the\nquality of their inferences depends on the availability of prior information\nand good approximations to posteriors for complex problems. This is\nparticularly pertinent to multi-variable or spatio-temporal problems. Our work\nconstructs and formalises a conformal prediction framework that satisfies\nmarginal coverage for spatio-temporal predictions in a model-agnostic manner,\nrequiring near-zero computational costs. We provide an extensive empirical\nstudy of the application of the framework to ascertain valid error bars that\nprovide guaranteed coverage across the surrogate model's domain of operation.\nThe application scope of our work extends across a large range of\nspatio-temporal models, from solving partial differential equations to weather\nforecasting. Through the applications, the paper looks at providing\nstatistically valid error bars for deterministic models, as well as crafting\nguarantees to the error bars of probabilistic models. Our conformal prediction\nformalisation provides guaranteed coverage of the surrogate model, regardless\nof model architecture, and its training regime and is unbothered by the curse\nof dimensionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models of physical systems do not quantify their\nuncertainty, rendering their predictions unreliable, requiring further\nvalidation. Though Bayesian approximations offer some solace in estimating the\nerror associated with these models, they cannot provide guarantees, and the\nquality of their inferences depends on the availability of prior information\nand good approximations to posteriors for complex problems. This is\nparticularly pertinent to multi-variable or spatio-temporal problems. Our work\nconstructs and formalises a conformal prediction framework that satisfies\nmarginal coverage for spatio-temporal predictions in a model-agnostic manner,\nrequiring near-zero computational costs. We provide an extensive empirical\nstudy of the application of the framework to ascertain valid error bars that\nprovide guaranteed coverage across the surrogate model's domain of operation.\nThe application scope of our work extends across a large range of\nspatio-temporal models, from solving partial differential equations to weather\nforecasting. Through the applications, the paper looks at providing\nstatistically valid error bars for deterministic models, as well as crafting\nguarantees to the error bars of probabilistic models. Our conformal prediction\nformalisation provides guaranteed coverage of the surrogate model, regardless\nof model architecture, and its training regime and is unbothered by the curse\nof dimensionality."
                },
                "authors": [
                    {
                        "name": "Vignesh Gopakumar"
                    },
                    {
                        "name": "Ander Gray"
                    },
                    {
                        "name": "Joel Oskarsson"
                    },
                    {
                        "name": "Lorenzo Zanisi"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "Daniel Giles"
                    },
                    {
                        "name": "Matt Kusner"
                    },
                    {
                        "name": "Marc Peter Deisenroth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Peter Deisenroth"
                },
                "author": "Marc Peter Deisenroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08126v2",
                "updated": "2024-10-31T11:11:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    11,
                    18,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-10T17:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Mars: Situated Inductive Reasoning in an Open-World Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mars: Situated Inductive Reasoning in an Open-World Environment"
                },
                "summary": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Song-chun Zhu"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted by NeurIPS 2024 Track Datasets and Benchmarks. Project page:\n  https://marscrafter.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23822v1",
                "updated": "2024-10-31T11:07:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:07:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding"
                },
                "summary": "Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review."
                },
                "authors": [
                    {
                        "name": "Jinlong He"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Shenjun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Shenjun Zhong"
                },
                "author": "Shenjun Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11393v2",
                "updated": "2024-10-31T11:07:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents"
                },
                "summary": "In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents."
                },
                "authors": [
                    {
                        "name": "Amine Ben Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_comment": "36 pages, 19 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.24198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24198v1",
                "updated": "2024-10-31T17:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "SelfCodeAlign: Self-Alignment for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCodeAlign: Self-Alignment for Code Generation"
                },
                "summary": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Zachary Mueller"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24190v1",
                "updated": "2024-10-31T17:51:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"
                },
                "summary": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions."
                },
                "authors": [
                    {
                        "name": "Yujin Potter"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06711v2",
                "updated": "2024-10-31T17:48:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    48,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-08-25T13:14:59Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    13,
                    14,
                    59,
                    6,
                    238,
                    0
                ],
                "title": "Quantized neural network for complex hologram generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized neural network for complex hologram generation"
                },
                "summary": "Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency."
                },
                "authors": [
                    {
                        "name": "Yutaka Endo"
                    },
                    {
                        "name": "Minoru Oikawa"
                    },
                    {
                        "name": "Timothy D. Wilkinson"
                    },
                    {
                        "name": "Tomoyoshi Shimobaba"
                    },
                    {
                        "name": "Tomoyoshi Ito"
                    }
                ],
                "author_detail": {
                    "name": "Tomoyoshi Ito"
                },
                "author": "Tomoyoshi Ito",
                "arxiv_doi": "10.1364/AO.538096",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1364/AO.538096",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures",
                "arxiv_journal_ref": "Appl. Opt. 64, A12-A18 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24175v1",
                "updated": "2024-10-31T17:42:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:42:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13046v2",
                "updated": "2024-10-31T17:39:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    39,
                    34,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-19T17:59:48Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    17,
                    59,
                    48,
                    4,
                    110,
                    0
                ],
                "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context"
                },
                "summary": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03555v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03555v4",
                "updated": "2024-10-31T17:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    36,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-06T15:25:48Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    25,
                    48,
                    0,
                    127,
                    0
                ],
                "title": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges"
                },
                "summary": "Open-radio access network (O-RAN) seeks to establish principles of openness,\nprogrammability, automation, intelligence, and hardware-software disaggregation\nwith interoperable interfaces. It advocates for multi-vendorism and\nmulti-stakeholderism within a cloudified and virtualized wireless\ninfrastructure, aimed at enhancing the deployment, operation, and maintenance\nof RAN architecture. This enhancement promises increased flexibility,\nperformance optimization, service innovation, energy efficiency, and cost\nefficiency in fifth-generation (5G), sixth-generation (6G), and future\nnetworks. One of the key features of the O-RAN architecture is its support for\nnetwork slicing, which entails interaction with other slicing domains within a\nmobile network, notably the transport network (TN) domain and the core network\n(CN) domain, to realize end-to-end (E2E) network slicing. The study of this\nfeature requires exploring the stances and contributions of diverse standards\ndevelopment organizations (SDOs). In this context, we note that despite the\nongoing industrial deployments and standardization efforts, the research and\nstandardization communities have yet to comprehensively address network slicing\nin O-RAN. To address this gap, this survey paper provides a comprehensive\nexploration of network slicing in O-RAN through an in-depth review of\nspecification documents from O-RAN Alliance and research papers from leading\nindustry and academic institutions. The paper commences with an overview of the\nongoing standardization efforts and open-source contributions associated with\nO-RAN, subsequently delving into the latest O-RAN architecture with an emphasis\non its slicing aspects. Further, the paper explores deployment scenarios for\nnetwork slicing within O-RAN, examining options for the deployment and\norchestration of O-RAN and TN network slice subnets...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-radio access network (O-RAN) seeks to establish principles of openness,\nprogrammability, automation, intelligence, and hardware-software disaggregation\nwith interoperable interfaces. It advocates for multi-vendorism and\nmulti-stakeholderism within a cloudified and virtualized wireless\ninfrastructure, aimed at enhancing the deployment, operation, and maintenance\nof RAN architecture. This enhancement promises increased flexibility,\nperformance optimization, service innovation, energy efficiency, and cost\nefficiency in fifth-generation (5G), sixth-generation (6G), and future\nnetworks. One of the key features of the O-RAN architecture is its support for\nnetwork slicing, which entails interaction with other slicing domains within a\nmobile network, notably the transport network (TN) domain and the core network\n(CN) domain, to realize end-to-end (E2E) network slicing. The study of this\nfeature requires exploring the stances and contributions of diverse standards\ndevelopment organizations (SDOs). In this context, we note that despite the\nongoing industrial deployments and standardization efforts, the research and\nstandardization communities have yet to comprehensively address network slicing\nin O-RAN. To address this gap, this survey paper provides a comprehensive\nexploration of network slicing in O-RAN through an in-depth review of\nspecification documents from O-RAN Alliance and research papers from leading\nindustry and academic institutions. The paper commences with an overview of the\nongoing standardization efforts and open-source contributions associated with\nO-RAN, subsequently delving into the latest O-RAN architecture with an emphasis\non its slicing aspects. Further, the paper explores deployment scenarios for\nnetwork slicing within O-RAN, examining options for the deployment and\norchestration of O-RAN and TN network slice subnets..."
                },
                "authors": [
                    {
                        "name": "Khurshid Alam"
                    },
                    {
                        "name": "Mohammad Asif Habibi"
                    },
                    {
                        "name": "Matthias Tammen"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Marco Di Renzo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Xavier Costa-Pérez"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Ashutosh Dutta"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "46 pages, 12 figures, 4 tables, submitted to the IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03555v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03555v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24155v1",
                "updated": "2024-10-31T17:12:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:12:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24152v1",
                "updated": "2024-10-31T17:10:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:10:01Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "title": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Wei Zhan"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    }
                ],
                "author_detail": {
                    "name": "Masayoshi Tomizuka"
                },
                "author": "Masayoshi Tomizuka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17947v2",
                "updated": "2024-10-31T17:08:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    8,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-25T21:47:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    21,
                    47,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias"
                },
                "summary": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl ."
                },
                "authors": [
                    {
                        "name": "Venkata S Govindarajan"
                    },
                    {
                        "name": "Matianyu Zang"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "David Beaver"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "Accepted to Findings@EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12794v3",
                "updated": "2024-10-31T16:58:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    58,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-01-23T14:29:17Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    14,
                    29,
                    17,
                    1,
                    23,
                    0
                ],
                "title": "Benchmarking LLMs via Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs via Uncertainty Quantification"
                },
                "summary": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "30 pages, accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03636v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03636v4",
                "updated": "2024-10-31T16:54:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    54,
                    30,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-05T22:16:19Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    22,
                    16,
                    19,
                    2,
                    157,
                    0
                ],
                "title": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages"
                },
                "summary": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness."
                },
                "authors": [
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Haley Lepe"
                    },
                    {
                        "name": "Sahil Bhatia"
                    },
                    {
                        "name": "Karim Elmaaroufi"
                    },
                    {
                        "name": "George Varghese"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    }
                ],
                "author_detail": {
                    "name": "Sanjit A. Seshia"
                },
                "author": "Sanjit A. Seshia",
                "arxiv_comment": "14 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03636v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03636v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24117v1",
                "updated": "2024-10-31T16:46:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Repository-Level Compositional Code Translation and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-Level Compositional Code Translation and Validation"
                },
                "summary": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Mrigank Pawagi"
                    },
                    {
                        "name": "Muhammad Salman Abid"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15892v3",
                "updated": "2024-10-31T16:36:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    36,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-22T01:52:30Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    52,
                    30,
                    0,
                    204,
                    0
                ],
                "title": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training"
                },
                "summary": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24096v1",
                "updated": "2024-10-31T16:28:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:28:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning"
                },
                "summary": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform."
                },
                "authors": [
                    {
                        "name": "Nabil Omi"
                    },
                    {
                        "name": "Hosein Hasanbeig"
                    },
                    {
                        "name": "Hiteshi Sharma"
                    },
                    {
                        "name": "Sriram K. Rajamani"
                    },
                    {
                        "name": "Siddhartha Sen"
                    }
                ],
                "author_detail": {
                    "name": "Siddhartha Sen"
                },
                "author": "Siddhartha Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17446v2",
                "updated": "2024-10-31T16:16:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    16,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-26T00:38:18Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    38,
                    18,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability"
                },
                "summary": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets."
                },
                "authors": [
                    {
                        "name": "Ming Xiang"
                    },
                    {
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "name": "Edmund Yeh"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "Lili Su"
                    }
                ],
                "author_detail": {
                    "name": "Lili Su"
                },
                "author": "Lili Su",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18760v3",
                "updated": "2024-10-31T16:12:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    12,
                    16,
                    3,
                    305,
                    0
                ],
                "published": "2023-11-30T18:02:44Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    2,
                    44,
                    3,
                    334,
                    0
                ],
                "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaskBench: Benchmarking Large Language Models for Task Automation"
                },
                "summary": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents."
                },
                "authors": [
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19238v2",
                "updated": "2024-10-31T16:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    6,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-27T15:01:53Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    1,
                    53,
                    3,
                    179,
                    0
                ],
                "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Fine-Grained Values and Opinions in Large Language Models"
                },
                "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v2",
                "updated": "2024-10-31T16:01:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    1,
                    59,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4"
                },
                "summary": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02119v3",
                "updated": "2024-10-31T15:57:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    57,
                    42,
                    3,
                    305,
                    0
                ],
                "published": "2023-12-04T18:49:23Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    49,
                    23,
                    0,
                    338,
                    0
                ],
                "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
                },
                "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard."
                },
                "authors": [
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    },
                    {
                        "name": "Paul Kassianik"
                    },
                    {
                        "name": "Blaine Nelson"
                    },
                    {
                        "name": "Hyrum Anderson"
                    },
                    {
                        "name": "Yaron Singer"
                    },
                    {
                        "name": "Amin Karbasi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Karbasi"
                },
                "author": "Amin Karbasi",
                "arxiv_comment": "Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24049v1",
                "updated": "2024-10-31T15:45:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:45:23Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
                },
                "summary": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs."
                },
                "authors": [
                    {
                        "name": "Muhammed Saeed"
                    },
                    {
                        "name": "Elgizouli Mohamed"
                    },
                    {
                        "name": "Mukhtar Mohamed"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Shady Shehata"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Abdul-Mageed"
                },
                "author": "Muhammad Abdul-Mageed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24039v1",
                "updated": "2024-10-31T15:35:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    35,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:35:41Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    35,
                    41,
                    3,
                    305,
                    0
                ],
                "title": "Efficient Satellite-Ground Interconnection Design for Low-orbit\n  Mega-Constellation Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Satellite-Ground Interconnection Design for Low-orbit\n  Mega-Constellation Topology"
                },
                "summary": "The low-orbit mega-constellation network (LMCN) is an important part of the\nspace-air-ground integrated network system. An effective satellite-ground\ninterconnection design can result in a stable constellation topology for LMCNs.\nA naive solution is accessing the satellite with the longest remaining service\ntime (LRST), which is widely used in previous designs. The Coordinated\nSatellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm,\ncoordinates the establishment of ground-satellite links (GSLs). Compared with\nexisting solutions, it reduces latency by 19% and jitter by 70% on average.\nHowever, CSGI only supports the scenario where terminals access only one\nsatellite and cannot fully utilize the multi-access capabilities of terminals.\nAdditionally, CSGI's high computational complexity poses deployment challenges.\nTo overcome these problems, we propose the Classification-based Longest\nRemaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario\nwith multi-access capabilities. It adds optional paths during routing with low\ncomputational complexity, improving end-to-end communications quality. We\nconduct our 1000s simulation from Brazil to Lithuania on the open-source\nplatform Hypatia. Experiment results show that compared with CSGI, C-LRST\nreduces the latency and increases the throughput by approximately 60% and 40%,\nrespectively. In addition, C-LRST's GSL switching number is 14, whereas CSGI is\n23. C-LRST has better link stability than CSGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-orbit mega-constellation network (LMCN) is an important part of the\nspace-air-ground integrated network system. An effective satellite-ground\ninterconnection design can result in a stable constellation topology for LMCNs.\nA naive solution is accessing the satellite with the longest remaining service\ntime (LRST), which is widely used in previous designs. The Coordinated\nSatellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm,\ncoordinates the establishment of ground-satellite links (GSLs). Compared with\nexisting solutions, it reduces latency by 19% and jitter by 70% on average.\nHowever, CSGI only supports the scenario where terminals access only one\nsatellite and cannot fully utilize the multi-access capabilities of terminals.\nAdditionally, CSGI's high computational complexity poses deployment challenges.\nTo overcome these problems, we propose the Classification-based Longest\nRemaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario\nwith multi-access capabilities. It adds optional paths during routing with low\ncomputational complexity, improving end-to-end communications quality. We\nconduct our 1000s simulation from Brazil to Lithuania on the open-source\nplatform Hypatia. Experiment results show that compared with CSGI, C-LRST\nreduces the latency and increases the throughput by approximately 60% and 40%,\nrespectively. In addition, C-LRST's GSL switching number is 14, whereas CSGI is\n23. C-LRST has better link stability than CSGI."
                },
                "authors": [
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Jiazhi Wu"
                    },
                    {
                        "name": "Quanwei Lin"
                    },
                    {
                        "name": "Handong Luo"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Kun Qiu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24034v1",
                "updated": "2024-10-31T15:32:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:32:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Handwriting Recognition in Historical Documents with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwriting Recognition in Historical Documents with Multimodal LLM"
                },
                "summary": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio"
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    }
                ],
                "author_detail": {
                    "name": "Lucian Li"
                },
                "author": "Lucian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24032v1",
                "updated": "2024-10-31T15:30:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks"
                },
                "summary": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration."
                },
                "authors": [
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24024v1",
                "updated": "2024-10-31T15:25:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:25:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents"
                },
                "summary": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Siyi Cheng"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Shudan Zhang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24021v1",
                "updated": "2024-10-31T15:21:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:21:27Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "title": "Detecting text level intellectual influence with knowledge graph\n  embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text level intellectual influence with knowledge graph\n  embeddings"
                },
                "summary": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights."
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    },
                    {
                        "name": "Eryclis Silva"
                    }
                ],
                "author_detail": {
                    "name": "Eryclis Silva"
                },
                "author": "Eryclis Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24013v1",
                "updated": "2024-10-31T15:14:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    14,
                    15,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:14:15Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    14,
                    15,
                    3,
                    305,
                    0
                ],
                "title": "Distributing Intelligence in 6G Programmable Data Planes for Effective\n  In-Network Deployment of an Active Intrusion Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing Intelligence in 6G Programmable Data Planes for Effective\n  In-Network Deployment of an Active Intrusion Detection System"
                },
                "summary": "The problem of attacks on new generation network infrastructures is becoming\nincreasingly relevant, given the widening of the attack surface of these\nnetworks resulting from the greater number of devices that will access them in\nthe future (sensors, actuators, vehicles, household appliances, etc.).\nApproaches to the design of intrusion detection systems must evolve and go\nbeyond the traditional concept of perimeter control to build on new paradigms\nthat exploit the typical characteristics of future 5G and 6G networks, such as\nin-network computing and intelligent programmable data planes. The aim of this\nresearch is to propose a disruptive paradigm in which devices in a typical data\nplane of a future programmable network have %classification and anomaly\ndetection capabilities and cooperate in a fully distributed fashion to act as\nan ML-enabled Active Intrusion Detection System \"embedded\" into the network.\nThe reported proof-of-concept experiments demonstrate that the proposed\nparadigm allows working effectively and with a good level of precision while\noccupying overall less CPU and RAM resources of the devices involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of attacks on new generation network infrastructures is becoming\nincreasingly relevant, given the widening of the attack surface of these\nnetworks resulting from the greater number of devices that will access them in\nthe future (sensors, actuators, vehicles, household appliances, etc.).\nApproaches to the design of intrusion detection systems must evolve and go\nbeyond the traditional concept of perimeter control to build on new paradigms\nthat exploit the typical characteristics of future 5G and 6G networks, such as\nin-network computing and intelligent programmable data planes. The aim of this\nresearch is to propose a disruptive paradigm in which devices in a typical data\nplane of a future programmable network have %classification and anomaly\ndetection capabilities and cooperate in a fully distributed fashion to act as\nan ML-enabled Active Intrusion Detection System \"embedded\" into the network.\nThe reported proof-of-concept experiments demonstrate that the proposed\nparadigm allows working effectively and with a good level of precision while\noccupying overall less CPU and RAM resources of the devices involved."
                },
                "authors": [
                    {
                        "name": "Mattia G. Spina"
                    },
                    {
                        "name": "Floriano De Rango"
                    },
                    {
                        "name": "Edoardo Scalzo"
                    },
                    {
                        "name": "Francesca Guerriero"
                    },
                    {
                        "name": "Antonio Iera"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Iera"
                },
                "author": "Antonio Iera",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.15656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.15656v3",
                "updated": "2024-10-31T15:01:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    1,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2022-11-28T18:59:02Z",
                "published_parsed": [
                    2022,
                    11,
                    28,
                    18,
                    59,
                    2,
                    0,
                    332,
                    0
                ],
                "title": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation"
                },
                "summary": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion."
                },
                "authors": [
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Weihao Gu"
                    },
                    {
                        "name": "Xianjing Zhang"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xieyuanli Chen"
                },
                "author": "Xieyuanli Chen",
                "arxiv_comment": "ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.15656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.15656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15852v2",
                "updated": "2024-10-31T14:43:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    43,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-23T14:04:48Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    14,
                    4,
                    48,
                    5,
                    83,
                    0
                ],
                "title": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents"
                },
                "summary": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Husn"
                    },
                    {
                        "name": "Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen"
                },
                "arxiv_affiliation": "Peter",
                "author": "Chen",
                "arxiv_comment": "ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v2",
                "updated": "2024-10-31T14:38:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    38,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22637v2",
                "updated": "2024-10-31T14:35:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    35,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T02:04:23Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    4,
                    23,
                    2,
                    304,
                    0
                ],
                "title": "Consistency Diffusion Bridge Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Diffusion Bridge Models"
                },
                "summary": "Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space."
                },
                "authors": [
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Fan Bao"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19534v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19534v4",
                "updated": "2024-10-31T14:32:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    32,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-29T21:29:44Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    21,
                    29,
                    44,
                    2,
                    150,
                    0
                ],
                "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Learning Algorithms Do Not Learn Preference Rankings"
                },
                "summary": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Lily H. Zhang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "arxiv_comment": "NeurIPS 2024 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19534v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19534v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06246v2",
                "updated": "2024-10-31T14:24:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    24,
                    45,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-10T13:23:00Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    13,
                    23,
                    0,
                    0,
                    162,
                    0
                ],
                "title": "Data-Efficient Learning with Neural Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Learning with Neural Programs"
                },
                "summary": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner."
                },
                "authors": [
                    {
                        "name": "Alaia Solko-Breslin"
                    },
                    {
                        "name": "Seewon Choi"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Neelay Velingker"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23968v1",
                "updated": "2024-10-31T14:22:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    22,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:22:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    22,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable\n  Robot Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable\n  Robot Task Planning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have helped facilitate\nexciting progress for robotic planning in real, open-world environments. 3D\nscene graphs (3DSGs) offer a promising environment representation for grounding\nsuch LLM-based planners as they are compact and semantically rich. However, as\nthe robot's environment scales (e.g., number of entities tracked) and the\ncomplexity of scene graph information increases (e.g., maintaining more\nattributes), providing the 3DSG as-is to an LLM-based planner quickly becomes\ninfeasible due to input token count limits and attentional biases present in\nLLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods\nthat retrieve query-relevant document chunks for LLM question and answering, we\nadapt the paradigm for our embodied domain. Specifically, we propose a 3D scene\nsubgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based\nplanner with for executing natural language robotic tasks. Notably, our\nretrieved subgraphs adapt to changes in the environment as well as changes in\ntask-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's\nability to significantly reduce input token counts (by an order of magnitude)\nand planning time (up to 70% reduction in average time per planning step) while\nimproving success rates on AI2Thor simulated household tasks with a single-arm,\nmobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with\na manipulator to highlight the performance benefits for robot deployment at the\nedge in real environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have helped facilitate\nexciting progress for robotic planning in real, open-world environments. 3D\nscene graphs (3DSGs) offer a promising environment representation for grounding\nsuch LLM-based planners as they are compact and semantically rich. However, as\nthe robot's environment scales (e.g., number of entities tracked) and the\ncomplexity of scene graph information increases (e.g., maintaining more\nattributes), providing the 3DSG as-is to an LLM-based planner quickly becomes\ninfeasible due to input token count limits and attentional biases present in\nLLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods\nthat retrieve query-relevant document chunks for LLM question and answering, we\nadapt the paradigm for our embodied domain. Specifically, we propose a 3D scene\nsubgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based\nplanner with for executing natural language robotic tasks. Notably, our\nretrieved subgraphs adapt to changes in the environment as well as changes in\ntask-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's\nability to significantly reduce input token counts (by an order of magnitude)\nand planning time (up to 70% reduction in average time per planning step) while\nimproving success rates on AI2Thor simulated household tasks with a single-arm,\nmobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with\na manipulator to highlight the performance benefits for robot deployment at the\nedge in real environments."
                },
                "authors": [
                    {
                        "name": "Meghan Booker"
                    },
                    {
                        "name": "Grayson Byrd"
                    },
                    {
                        "name": "Bethany Kemp"
                    },
                    {
                        "name": "Aurora Schmidt"
                    },
                    {
                        "name": "Corban Rivera"
                    }
                ],
                "author_detail": {
                    "name": "Corban Rivera"
                },
                "author": "Corban Rivera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10476v2",
                "updated": "2024-10-31T14:15:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    15,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-14T13:10:45Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    10,
                    45,
                    0,
                    288,
                    0
                ],
                "title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?"
                },
                "summary": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub."
                },
                "authors": [
                    {
                        "name": "Gabriel Roccabruna"
                    },
                    {
                        "name": "Massimo Rizzoli"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23956v1",
                "updated": "2024-10-31T14:09:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    9,
                    50,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T14:09:50Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    9,
                    50,
                    3,
                    305,
                    0
                ],
                "title": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language"
                },
                "summary": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM."
                },
                "authors": [
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Maurice Weber"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Yihong Chen"
                    },
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    }
                ],
                "author_detail": {
                    "name": "Pontus Stenetorp"
                },
                "author": "Pontus Stenetorp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16724v2",
                "updated": "2024-10-31T14:03:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    3,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-23T12:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    12,
                    38,
                    48,
                    1,
                    205,
                    0
                ],
                "title": "Structure-aware Domain Knowledge Injection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-aware Domain Knowledge Injection for Large Language Models"
                },
                "summary": "This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus requirement to a mere\n0.3%, while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes of human\nstudents, particularly how structured domain knowledge from textbooks is\nassimilated and subsequently applied to tackle real-world challenges through\nspecific exercises. Based on this, we propose a novel two-stage strategy for\nknowledge injection and alignment: Structure-aware Continual Pre-Training\n(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we\nautomatically extract the domain knowledge taxonomy and reorganize the training\ncorpora, enabling LLMs to effectively link textual segments to targeted\nknowledge points within the taxonomy. In the SSFT phase, we explicitly prompt\nmodels to elucidate the underlying knowledge structure in their outputs,\nleveraging the structured domain insight to address practical problems. Our\nultimate method has undergone extensive evaluations across model architectures\nand scales, using closed-book question-answering tasks on LongBench and\nMMedBench datasets. Remarkably, our method demonstrates the potential of\ncomparable improvement against the state-of-the-art MMedLM2 on MMedBench, while\nsignificantly reducing the training costs to 5%. This breakthrough paves the\nway for scaling up our StructTuning for stronger domain-specific LLMs with\ncomprehensive data utilization. Code is available at\nhttps://github.com/alibaba/struxgpt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly reduces the training corpus requirement to a mere\n0.3%, while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes of human\nstudents, particularly how structured domain knowledge from textbooks is\nassimilated and subsequently applied to tackle real-world challenges through\nspecific exercises. Based on this, we propose a novel two-stage strategy for\nknowledge injection and alignment: Structure-aware Continual Pre-Training\n(SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we\nautomatically extract the domain knowledge taxonomy and reorganize the training\ncorpora, enabling LLMs to effectively link textual segments to targeted\nknowledge points within the taxonomy. In the SSFT phase, we explicitly prompt\nmodels to elucidate the underlying knowledge structure in their outputs,\nleveraging the structured domain insight to address practical problems. Our\nultimate method has undergone extensive evaluations across model architectures\nand scales, using closed-book question-answering tasks on LongBench and\nMMedBench datasets. Remarkably, our method demonstrates the potential of\ncomparable improvement against the state-of-the-art MMedLM2 on MMedBench, while\nsignificantly reducing the training costs to 5%. This breakthrough paves the\nway for scaling up our StructTuning for stronger domain-specific LLMs with\ncomprehensive data utilization. Code is available at\nhttps://github.com/alibaba/struxgpt."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Ze Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Rongxin Jiang"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Yaowu Chen"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Preprint. Code is available at https://github.com/alibaba/struxgpt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19073v2",
                "updated": "2024-10-31T13:59:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    59,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-27T10:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    10,
                    43,
                    4,
                    3,
                    179,
                    0
                ],
                "title": "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries"
                },
                "summary": "Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions."
                },
                "authors": [
                    {
                        "name": "Irina Saparina"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "NeurIPS 2024 D&B Track Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22086v2",
                "updated": "2024-10-31T13:50:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    50,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate"
                },
                "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training."
                },
                "authors": [
                    {
                        "name": "Zhiqi Bu"
                    },
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Bhanukiran Vinzamuri"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23933v1",
                "updated": "2024-10-31T13:47:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    47,
                    10,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:47:10Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    47,
                    10,
                    3,
                    305,
                    0
                ],
                "title": "Language Models can Self-Lengthen to Generate Long Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Self-Lengthen to Generate Long Texts"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23918v1",
                "updated": "2024-10-31T13:26:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    26,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:26:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    26,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments"
                },
                "summary": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack."
                },
                "authors": [
                    {
                        "name": "Xinghao Wang"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23912v1",
                "updated": "2024-10-31T13:17:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    17,
                    53,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:17:53Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    17,
                    53,
                    3,
                    305,
                    0
                ],
                "title": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner"
                },
                "summary": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Yu-Ting Lee"
                    },
                    {
                        "name": "Hui-Ying Shih"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20087v2",
                "updated": "2024-10-31T13:10:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    10,
                    12,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-28T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    55,
                    24,
                    4,
                    180,
                    0
                ],
                "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgressGym: Alignment with a Millennium of Moral Progress"
                },
                "summary": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively."
                },
                "authors": [
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xuchuan Huang"
                    },
                    {
                        "name": "Jasmine Xinze Li"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16434v2",
                "updated": "2024-10-31T13:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-23T12:33:58Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    12,
                    33,
                    58,
                    1,
                    205,
                    0
                ],
                "title": "Enhancing LLM's Cognition via Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM's Cognition via Structurization"
                },
                "summary": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including a series of\nauto-regressive LLMs as well as BERT-like masking models) on a diverse set of\nNLP tasks (e.g., context-based question-answering, exhaustive hallucination\nevaluation, and passage-level dense retrieval). Empirical results show\nconsistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost the open-sourced LLaMA2-70B model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code is available\nat https://github.com/alibaba/struxgpt."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rongxin Jiang"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Yaowu Chen"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "This paper has been accepted by NeurIPS 2024. Code is available at\n  https://github.com/alibaba/struxgpt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23904v1",
                "updated": "2024-10-31T13:06:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:06:29Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection"
                },
                "summary": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23902v1",
                "updated": "2024-10-31T13:05:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    5,
                    39,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T13:05:39Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    5,
                    39,
                    3,
                    305,
                    0
                ],
                "title": "Responsible Retrieval Augmented Generation for Climate Decision Making\n  from Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible Retrieval Augmented Generation for Climate Decision Making\n  from Documents"
                },
                "summary": "Climate decision making is constrained by the complexity and inaccessibility\nof key information within lengthy, technical, and multi-lingual documents.\nGenerative AI technologies offer a promising route for improving the\naccessibility of information contained within these documents, but suffer from\nlimitations. These include (1) a tendency to hallucinate or mis-represent\ninformation, (2) difficulty in steering or guaranteeing properties of generated\noutput, and (3) reduced performance in specific technical domains. To address\nthese challenges, we introduce a novel evaluation framework with\ndomain-specific dimensions tailored for climate-related documents. We then\napply this framework to evaluate Retrieval-Augmented Generation (RAG)\napproaches and assess retrieval- and generation-quality within a prototype tool\nthat answers questions about individual climate law and policy documents. In\naddition, we publish a human-annotated dataset and scalable automated\nevaluation tools, with the aim of facilitating broader adoption and robust\nassessment of these systems in the climate domain. Our findings highlight the\nkey components of responsible deployment of RAG to enhance decision-making,\nwhile also providing insights into user experience (UX) considerations for\nsafely deploying such systems to build trust with users in high-risk domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate decision making is constrained by the complexity and inaccessibility\nof key information within lengthy, technical, and multi-lingual documents.\nGenerative AI technologies offer a promising route for improving the\naccessibility of information contained within these documents, but suffer from\nlimitations. These include (1) a tendency to hallucinate or mis-represent\ninformation, (2) difficulty in steering or guaranteeing properties of generated\noutput, and (3) reduced performance in specific technical domains. To address\nthese challenges, we introduce a novel evaluation framework with\ndomain-specific dimensions tailored for climate-related documents. We then\napply this framework to evaluate Retrieval-Augmented Generation (RAG)\napproaches and assess retrieval- and generation-quality within a prototype tool\nthat answers questions about individual climate law and policy documents. In\naddition, we publish a human-annotated dataset and scalable automated\nevaluation tools, with the aim of facilitating broader adoption and robust\nassessment of these systems in the climate domain. Our findings highlight the\nkey components of responsible deployment of RAG to enhance decision-making,\nwhile also providing insights into user experience (UX) considerations for\nsafely deploying such systems to build trust with users in high-risk domains."
                },
                "authors": [
                    {
                        "name": "Matyas Juhasz"
                    },
                    {
                        "name": "Kalyan Dutia"
                    },
                    {
                        "name": "Henry Franks"
                    },
                    {
                        "name": "Conor Delahunty"
                    },
                    {
                        "name": "Patrick Fawbert Mills"
                    },
                    {
                        "name": "Harrison Pim"
                    }
                ],
                "author_detail": {
                    "name": "Harrison Pim"
                },
                "author": "Harrison Pim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04559v3",
                "updated": "2024-10-31T12:59:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    59,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-07T03:37:19Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    37,
                    19,
                    2,
                    38,
                    0
                ],
                "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Model Agents Simulate Human Trust Behavior?"
                },
                "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Feiran Jia"
                    },
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "David Jurgens"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Guohao Li"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Li"
                },
                "author": "Guohao Li",
                "arxiv_comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20745v2",
                "updated": "2024-10-31T12:54:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    54,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-28T05:25:47Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    25,
                    47,
                    0,
                    302,
                    0
                ],
                "title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models"
                },
                "summary": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/."
                },
                "authors": [
                    {
                        "name": "Yilun Jin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Pratik Jayarao"
                    },
                    {
                        "name": "Mao Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Ritesh Sarkhel"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Haodong Wang"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Wenju Xu"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Bing Yin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yin"
                },
                "author": "Bing Yin",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos\n  in Figure 9",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23894v1",
                "updated": "2024-10-31T12:53:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    53,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:53:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    53,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models"
                },
                "summary": "Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines."
                },
                "authors": [
                    {
                        "name": "Pooria Madani"
                    }
                ],
                "author_detail": {
                    "name": "Pooria Madani"
                },
                "author": "Pooria Madani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23890v1",
                "updated": "2024-10-31T12:52:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    52,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:52:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    52,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages"
                },
                "summary": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies."
                },
                "authors": [
                    {
                        "name": "Séamus Lankford"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.02370,\n  arXiv:2403.01580",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23884v1",
                "updated": "2024-10-31T12:48:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:48:58Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "title": "Failure Modes of LLMs for Causal Reasoning on Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Modes of LLMs for Causal Reasoning on Narratives"
                },
                "summary": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Khurram Yamin"
                    },
                    {
                        "name": "Shantanu Gupta"
                    },
                    {
                        "name": "Gaurav R. Ghosal"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Bryan Wilder"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wilder"
                },
                "author": "Bryan Wilder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23881v1",
                "updated": "2024-10-31T12:44:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    44,
                    7,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:44:07Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    44,
                    7,
                    3,
                    305,
                    0
                ],
                "title": "DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge"
                },
                "summary": "The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines."
                },
                "authors": [
                    {
                        "name": "Daniel May"
                    },
                    {
                        "name": "Alessandro Tundo"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23879v1",
                "updated": "2024-10-31T12:40:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    40,
                    38,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    40,
                    38,
                    3,
                    305,
                    0
                ],
                "title": "Investigating Bias in Political Search Query Suggestions by Relative\n  Comparison with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Bias in Political Search Query Suggestions by Relative\n  Comparison with LLMs"
                },
                "summary": "Search query suggestions affect users' interactions with search engines,\nwhich then influences the information they encounter. Thus, bias in search\nquery suggestions can lead to exposure to biased search results and can impact\nopinion formation. This is especially critical in the political domain.\nDetecting and quantifying bias in web search engines is difficult due to its\ntopic dependency, complexity, and subjectivity. The lack of context and\nphrasality of query suggestions emphasizes this problem. In a multi-step\napproach, we combine the benefits of large language models, pairwise\ncomparison, and Elo-based scoring to identify and quantify bias in English\nsearch query suggestions. We apply our approach to the U.S. political news\ndomain and compare bias in Google and Bing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search query suggestions affect users' interactions with search engines,\nwhich then influences the information they encounter. Thus, bias in search\nquery suggestions can lead to exposure to biased search results and can impact\nopinion formation. This is especially critical in the political domain.\nDetecting and quantifying bias in web search engines is difficult due to its\ntopic dependency, complexity, and subjectivity. The lack of context and\nphrasality of query suggestions emphasizes this problem. In a multi-step\napproach, we combine the benefits of large language models, pairwise\ncomparison, and Elo-based scoring to identify and quantify bias in English\nsearch query suggestions. We apply our approach to the U.S. political news\ndomain and compare bias in Google and Bing."
                },
                "authors": [
                    {
                        "name": "Fabian Haak"
                    },
                    {
                        "name": "Björn Engelmann"
                    },
                    {
                        "name": "Christin Katharina Kreutz"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3630744.3658415",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3630744.3658415",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23875v1",
                "updated": "2024-10-31T12:37:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    37,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:37:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    37,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model\n  on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model\n  on Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG."
                },
                "authors": [
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Panrong Tong"
                    },
                    {
                        "name": "Zhongming Jin"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v2",
                "updated": "2024-10-31T12:34:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    34,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5\\% of extraversion-related test instances, while\nMistral-7B-Instruct did so in 92.5\\% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analysing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5\\% of extraversion-related test instances, while\nMistral-7B-Instruct did so in 92.5\\% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analysing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NeurIPS 2024 Workshop on Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02130v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02130v5",
                "updated": "2024-10-31T12:27:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-03T12:19:47Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    12,
                    19,
                    47,
                    5,
                    34,
                    0
                ],
                "title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures. Though LLMs can process graph information in a textual\nformat, they overlook the rich vision modality, which is an intuitive way for\nhumans to comprehend structural information and conduct general graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., $\\textit{visual graph}$) are still\nunexplored. To fill the gap, we innovatively propose an end-to-end framework,\ncalled $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual\nIntegr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into\ngeneral graph reasoning. Besides, we establish $\\textbf{G}$raph-based\n$\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering\n(GVLQA) dataset from existing graph data, which is the first vision-language\ndataset for general graph reasoning purposes. Extensive experiments on the\nGVLQA dataset and five real-world datasets show that GITA outperforms\nmainstream LLMs in terms of general graph reasoning capabilities. Moreover, We\nhighlight the effectiveness of the layout augmentation on visual graphs and\npretraining on the GVLQA dataset."
                },
                "authors": [
                    {
                        "name": "Yanbin Wei"
                    },
                    {
                        "name": "Shuai Fu"
                    },
                    {
                        "name": "Weisen Jiang"
                    },
                    {
                        "name": "Zejian Zhang"
                    },
                    {
                        "name": "Zhixiong Zeng"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "James T. Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "NeurIPS 2024; Project Page: v-graph.github.io; Code:\n  https://github.com/WEIYanbin1999/GITA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02130v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02130v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03941v2",
                "updated": "2024-10-31T12:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    30,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-06T12:18:54Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    12,
                    18,
                    54,
                    1,
                    37,
                    0
                ],
                "title": "Discovery of the Hidden World with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of the Hidden World with Large Language Models"
                },
                "summary": "Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "James Cheng"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "arxiv_comment": "NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72\n  figures; Project page: https://causalcoat.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12404v3",
                "updated": "2024-10-31T12:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    27,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-15T17:49:16Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    49,
                    16,
                    0,
                    106,
                    0
                ],
                "title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/"
                },
                "authors": [
                    {
                        "name": "Jinhee Kim"
                    },
                    {
                        "name": "Taesung Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06255v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06255v4",
                "updated": "2024-10-31T12:24:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    24,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-09T09:09:39Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    9,
                    9,
                    39,
                    4,
                    40,
                    0
                ],
                "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning"
                },
                "summary": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT."
                },
                "authors": [
                    {
                        "name": "Yichuan Mo"
                    },
                    {
                        "name": "Yuji Wang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06255v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06255v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23866v1",
                "updated": "2024-10-31T12:20:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    20,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:20:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    20,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Evaluating and Improving ChatGPT-Based Expansion of Abbreviations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving ChatGPT-Based Expansion of Abbreviations"
                },
                "summary": "Source code identifiers often contain abbreviations. Such abbreviations may\nreduce the readability of the source code, which in turn hinders the\nmaintenance of the software applications. To this end, accurate and automated\napproaches to expanding abbreviations in source code are desirable and\nabbreviation expansion has been intensively investigated. However, to the best\nof our knowledge, most existing approaches are heuristics, and none of them has\neven employed deep learning techniques, let alone the most advanced large\nlanguage models (LLMs). LLMs have demonstrated cutting-edge performance in\nvarious software engineering tasks, and thus it has the potential to expand\nabbreviation automatically. To this end, in this paper, we present the first\nempirical study on LLM-based abbreviation expansion. Our evaluation results on\na public benchmark suggest that ChatGPT is substantially less accurate than the\nstate-of-the-art approach, reducing precision and recall by 28.2\\% and 27.8\\%,\nrespectively. We manually analyzed the failed cases, and discovered the root\ncauses for the failures: 1) Lack of contexts and 2) Inability to recognize\nabbreviations. In response to the first cause, we investigated the effect of\nvarious contexts and found surrounding source code is the best selection. In\nresponse to the second cause, we designed an iterative approach that identifies\nand explicitly marks missed abbreviations in prompts. Finally, we proposed a\npost-condition checking to exclude incorrect expansions that violate\ncommonsense. All such measures together make ChatGPT-based abbreviation\nexpansion comparable to the state of the art while avoiding expensive source\ncode parsing and deep analysis that are indispensable for state-of-the-art\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code identifiers often contain abbreviations. Such abbreviations may\nreduce the readability of the source code, which in turn hinders the\nmaintenance of the software applications. To this end, accurate and automated\napproaches to expanding abbreviations in source code are desirable and\nabbreviation expansion has been intensively investigated. However, to the best\nof our knowledge, most existing approaches are heuristics, and none of them has\neven employed deep learning techniques, let alone the most advanced large\nlanguage models (LLMs). LLMs have demonstrated cutting-edge performance in\nvarious software engineering tasks, and thus it has the potential to expand\nabbreviation automatically. To this end, in this paper, we present the first\nempirical study on LLM-based abbreviation expansion. Our evaluation results on\na public benchmark suggest that ChatGPT is substantially less accurate than the\nstate-of-the-art approach, reducing precision and recall by 28.2\\% and 27.8\\%,\nrespectively. We manually analyzed the failed cases, and discovered the root\ncauses for the failures: 1) Lack of contexts and 2) Inability to recognize\nabbreviations. In response to the first cause, we investigated the effect of\nvarious contexts and found surrounding source code is the best selection. In\nresponse to the second cause, we designed an iterative approach that identifies\nand explicitly marks missed abbreviations in prompts. Finally, we proposed a\npost-condition checking to exclude incorrect expansions that violate\ncommonsense. All such measures together make ChatGPT-based abbreviation\nexpansion comparable to the state of the art while avoiding expensive source\ncode parsing and deep analysis that are indispensable for state-of-the-art\napproaches."
                },
                "authors": [
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23861v1",
                "updated": "2024-10-31T12:11:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    11,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:11:17Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    11,
                    17,
                    3,
                    305,
                    0
                ],
                "title": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23856v1",
                "updated": "2024-10-31T12:07:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    7,
                    44,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:07:44Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    7,
                    44,
                    3,
                    305,
                    0
                ],
                "title": "Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Rong Tao"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Yiwen Luo"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23851v1",
                "updated": "2024-10-31T12:01:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    1,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T12:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    12,
                    1,
                    51,
                    3,
                    305,
                    0
                ],
                "title": "Leveraging Large Language Models for Medical Information Extraction and\n  Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Medical Information Extraction and\n  Query Generation"
                },
                "summary": "This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings."
                },
                "authors": [
                    {
                        "name": "Georgios Peikos"
                    },
                    {
                        "name": "Pranav Kasela"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23844v1",
                "updated": "2024-10-31T11:50:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    50,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:50:24Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    50,
                    24,
                    3,
                    305,
                    0
                ],
                "title": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Knowledge Editing Based on Free-Text in LLMs"
                },
                "summary": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance."
                },
                "authors": [
                    {
                        "name": "Xiusheng Huang"
                    },
                    {
                        "name": "Yequan Wang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23841v1",
                "updated": "2024-10-31T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:47:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models"
                },
                "summary": "Instruction-following capabilities in large language models (LLMs) have\nsignificantly progressed, enabling more complex user interactions through\ndetailed prompts. However, retrieval systems have not matched these advances,\nmost of them still relies on traditional lexical and semantic matching\ntechniques that fail to fully capture user intent. Recent efforts have\nintroduced instruction-aware retrieval models, but these primarily focus on\nintrinsic content relevance, which neglects the importance of customized\npreferences for broader document-level attributes. This study evaluates the\ninstruction-following capabilities of various retrieval models beyond content\nrelevance, including LLM-based dense retrieval and reranking models. We develop\nInfoSearch, a novel retrieval evaluation benchmark spanning six document-level\nattributes: Audience, Keyword, Format, Language, Length, and Source, and\nintroduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and\nWeighted Instruction Sensitivity Evaluation (WISE) to accurately assess the\nmodels' responsiveness to instructions. Our findings reveal that while\nreranking models generally surpass retrieval models in instruction following,\nthey still face challenges in handling certain attributes. Moreover, although\ninstruction fine-tuning and increased model size lead to better performance,\nmost models fall short of achieving comprehensive instruction compliance as\nassessed by our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following capabilities in large language models (LLMs) have\nsignificantly progressed, enabling more complex user interactions through\ndetailed prompts. However, retrieval systems have not matched these advances,\nmost of them still relies on traditional lexical and semantic matching\ntechniques that fail to fully capture user intent. Recent efforts have\nintroduced instruction-aware retrieval models, but these primarily focus on\nintrinsic content relevance, which neglects the importance of customized\npreferences for broader document-level attributes. This study evaluates the\ninstruction-following capabilities of various retrieval models beyond content\nrelevance, including LLM-based dense retrieval and reranking models. We develop\nInfoSearch, a novel retrieval evaluation benchmark spanning six document-level\nattributes: Audience, Keyword, Format, Language, Length, and Source, and\nintroduce novel metrics -- Strict Instruction Compliance Ratio (SICR) and\nWeighted Instruction Sensitivity Evaluation (WISE) to accurately assess the\nmodels' responsiveness to instructions. Our findings reveal that while\nreranking models generally surpass retrieval models in instruction following,\nthey still face challenges in handling certain attributes. Moreover, although\ninstruction fine-tuning and increased model size lead to better performance,\nmost models fall short of achieving comprehensive instruction compliance as\nassessed by our benchmark."
                },
                "authors": [
                    {
                        "name": "Jianqun Zhou"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Qianqian Zheng"
                    },
                    {
                        "name": "Zeyuan Shang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17557v2",
                "updated": "2024-10-31T11:37:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    37,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-25T13:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    13,
                    50,
                    56,
                    1,
                    177,
                    0
                ],
                "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale"
                },
                "summary": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments."
                },
                "authors": [
                    {
                        "name": "Guilherme Penedo"
                    },
                    {
                        "name": "Hynek Kydlíček"
                    },
                    {
                        "name": "Loubna Ben allal"
                    },
                    {
                        "name": "Anton Lozhkov"
                    },
                    {
                        "name": "Margaret Mitchell"
                    },
                    {
                        "name": "Colin Raffel"
                    },
                    {
                        "name": "Leandro Von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02370v4",
                "updated": "2024-10-31T11:37:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    37,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-04T06:59:21Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    6,
                    59,
                    21,
                    6,
                    35,
                    0
                ],
                "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTimes: Autoregressive Time Series Forecasters via Large Language\n  Models"
                },
                "summary": "Foundation models of time series have not been fully developed due to the\nlimited availability of time series corpora and the underexploration of\nscalable pre-training. Based on the similar sequential formulation of time\nseries and natural language, increasing research demonstrates the feasibility\nof leveraging large language models (LLM) for time series. Nevertheless, the\ninherent autoregressive property and decoder-only architecture of LLMs have not\nbeen fully considered, resulting in insufficient utilization of LLM abilities.\nTo fully revitalize the general-purpose token transition and multi-step\ngeneration capability of large language models, we propose AutoTimes to\nrepurpose LLMs as autoregressive time series forecasters, which projects time\nseries into the embedding space of language tokens and autoregressively\ngenerates future predictions with arbitrary lengths. Compatible with any\ndecoder-only LLMs, the consequent forecaster exhibits the flexibility of the\nlookback length and scalability with larger LLMs. Further, we formulate time\nseries as prompts, extending the context for prediction beyond the lookback\nwindow, termed in-context forecasting. By introducing LLM-embedded textual\ntimestamps, AutoTimes can utilize chronological information to align\nmultivariate time series. Empirically, AutoTimes achieves state-of-the-art with\n0.1% trainable parameters and over $5\\times$ training/inference speedup\ncompared to advanced LLM-based forecasters. Code is available at this\nrepository: https://github.com/thuml/AutoTimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models of time series have not been fully developed due to the\nlimited availability of time series corpora and the underexploration of\nscalable pre-training. Based on the similar sequential formulation of time\nseries and natural language, increasing research demonstrates the feasibility\nof leveraging large language models (LLM) for time series. Nevertheless, the\ninherent autoregressive property and decoder-only architecture of LLMs have not\nbeen fully considered, resulting in insufficient utilization of LLM abilities.\nTo fully revitalize the general-purpose token transition and multi-step\ngeneration capability of large language models, we propose AutoTimes to\nrepurpose LLMs as autoregressive time series forecasters, which projects time\nseries into the embedding space of language tokens and autoregressively\ngenerates future predictions with arbitrary lengths. Compatible with any\ndecoder-only LLMs, the consequent forecaster exhibits the flexibility of the\nlookback length and scalability with larger LLMs. Further, we formulate time\nseries as prompts, extending the context for prediction beyond the lookback\nwindow, termed in-context forecasting. By introducing LLM-embedded textual\ntimestamps, AutoTimes can utilize chronological information to align\nmultivariate time series. Empirically, AutoTimes achieves state-of-the-art with\n0.1% trainable parameters and over $5\\times$ training/inference speedup\ncompared to advanced LLM-based forecasters. Code is available at this\nrepository: https://github.com/thuml/AutoTimes."
                },
                "authors": [
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Guo Qin"
                    },
                    {
                        "name": "Xiangdong Huang"
                    },
                    {
                        "name": "Jianmin Wang"
                    },
                    {
                        "name": "Mingsheng Long"
                    }
                ],
                "author_detail": {
                    "name": "Mingsheng Long"
                },
                "author": "Mingsheng Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23836v1",
                "updated": "2024-10-31T11:32:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    32,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:32:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    32,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts"
                },
                "summary": "This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes."
                },
                "authors": [
                    {
                        "name": "Xiang Deng"
                    },
                    {
                        "name": "Youxin Pang"
                    },
                    {
                        "name": "Xiaochen Zhao"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Lizhen Wang"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Shi Yan"
                    },
                    {
                        "name": "Hongwen Zhang"
                    },
                    {
                        "name": "Yebin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yebin Liu"
                },
                "author": "Yebin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v2",
                "updated": "2024-10-31T11:16:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    16,
                    46,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "arxiv_comment": "24 pages, 10 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.01762v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.01762v4",
                "updated": "2024-10-31T11:11:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    11,
                    24,
                    3,
                    305,
                    0
                ],
                "published": "2023-05-27T06:00:51Z",
                "published_parsed": [
                    2023,
                    5,
                    27,
                    6,
                    0,
                    51,
                    5,
                    147,
                    0
                ],
                "title": "Rapid Plug-in Defenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Plug-in Defenders"
                },
                "summary": "In the realm of daily services, the deployment of deep neural networks\nunderscores the paramount importance of their reliability. However, the\nvulnerability of these networks to adversarial attacks, primarily\nevasion-based, poses a concerning threat to their functionality. Common methods\nfor enhancing robustness involve heavy adversarial training or leveraging\nlearned knowledge from clean data, both necessitating substantial computational\nresources. This inherent time-intensive nature severely limits the agility of\nlarge foundational models to swiftly counter adversarial perturbations. To\naddress this challenge, this paper focuses on the Rapid Plug-in Defender\n(RaPiD) problem, aiming to rapidly counter adversarial perturbations without\naltering the deployed model. Drawing inspiration from the generalization and\nthe universal computation ability of pre-trained transformer models, we propose\na novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)\nfor RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes\nthe normalization layer parameters within the defender using a limited set of\nclean and adversarial examples. Our evaluation centers on assessing CeTaD's\neffectiveness, transferability, and the impact of different components in\nscenarios involving one-shot adversarial examples. The proposed method is\ncapable of rapidly adapting to various attacks and different application\nscenarios without altering the target model and clean training data. We also\nexplore the influence of varying training data conditions on CeTaD's\nperformance. Notably, CeTaD exhibits adaptability across differentiable service\nmodels and proves the potential of continuous learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of daily services, the deployment of deep neural networks\nunderscores the paramount importance of their reliability. However, the\nvulnerability of these networks to adversarial attacks, primarily\nevasion-based, poses a concerning threat to their functionality. Common methods\nfor enhancing robustness involve heavy adversarial training or leveraging\nlearned knowledge from clean data, both necessitating substantial computational\nresources. This inherent time-intensive nature severely limits the agility of\nlarge foundational models to swiftly counter adversarial perturbations. To\naddress this challenge, this paper focuses on the Rapid Plug-in Defender\n(RaPiD) problem, aiming to rapidly counter adversarial perturbations without\naltering the deployed model. Drawing inspiration from the generalization and\nthe universal computation ability of pre-trained transformer models, we propose\na novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)\nfor RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes\nthe normalization layer parameters within the defender using a limited set of\nclean and adversarial examples. Our evaluation centers on assessing CeTaD's\neffectiveness, transferability, and the impact of different components in\nscenarios involving one-shot adversarial examples. The proposed method is\ncapable of rapidly adapting to various attacks and different application\nscenarios without altering the target model and clean training data. We also\nexplore the influence of varying training data conditions on CeTaD's\nperformance. Notably, CeTaD exhibits adaptability across differentiable service\nmodels and proves the potential of continuous learning."
                },
                "authors": [
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Yujian Betterest Li"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Handing Wang"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.01762v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.01762v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08126v2",
                "updated": "2024-10-31T11:11:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    11,
                    18,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-10T17:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Mars: Situated Inductive Reasoning in an Open-World Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mars: Situated Inductive Reasoning in an Open-World Environment"
                },
                "summary": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Song-chun Zhu"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted by NeurIPS 2024 Track Datasets and Benchmarks. Project page:\n  https://marscrafter.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23822v1",
                "updated": "2024-10-31T11:07:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T11:07:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding"
                },
                "summary": "Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review."
                },
                "authors": [
                    {
                        "name": "Jinlong He"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Shenjun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Shenjun Zhong"
                },
                "author": "Shenjun Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11393v2",
                "updated": "2024-10-31T11:07:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    7,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents"
                },
                "summary": "In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents."
                },
                "authors": [
                    {
                        "name": "Amine Ben Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_comment": "36 pages, 19 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04181v2",
                "updated": "2024-10-31T11:01:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    1,
                    16,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-06T10:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    49,
                    46,
                    4,
                    250,
                    0
                ],
                "title": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering"
                },
                "summary": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui"
                },
                "authors": [
                    {
                        "name": "Larissa Pusch"
                    },
                    {
                        "name": "Tim O. F. Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Tim O. F. Conrad"
                },
                "author": "Tim O. F. Conrad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23815v1",
                "updated": "2024-10-31T10:58:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    58,
                    59,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:58:59Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    58,
                    59,
                    3,
                    305,
                    0
                ],
                "title": "The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing\n  Audio Generation Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing\n  Audio Generation Challenge"
                },
                "summary": "This paper presents the NPU-HWC system submitted to the ISCSLP 2024\nInspirational and Convincing Audio Generation Challenge 2024 (ICAGC). Our\nsystem consists of two modules: a speech generator for Track 1 and a background\naudio generator for Track 2. In Track 1, we employ Single-Codec to tokenize the\nspeech into discrete tokens and use a language-model-based approach to achieve\nzero-shot speaking style cloning. The Single-Codec effectively decouples timbre\nand speaking style at the token level, reducing the acoustic modeling burden on\nthe autoregressive language model. Additionally, we use DSPGAN to upsample 16\nkHz mel-spectrograms to high-fidelity 48 kHz waveforms. In Track 2, we propose\na background audio generator based on large language models (LLMs). This system\nproduces scene-appropriate accompaniment descriptions, synthesizes background\naudio with Tango 2, and integrates it with the speech generated by our Track 1\nsystem. Our submission achieves the second place and the first place in Track 1\nand Track 2 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the NPU-HWC system submitted to the ISCSLP 2024\nInspirational and Convincing Audio Generation Challenge 2024 (ICAGC). Our\nsystem consists of two modules: a speech generator for Track 1 and a background\naudio generator for Track 2. In Track 1, we employ Single-Codec to tokenize the\nspeech into discrete tokens and use a language-model-based approach to achieve\nzero-shot speaking style cloning. The Single-Codec effectively decouples timbre\nand speaking style at the token level, reducing the acoustic modeling burden on\nthe autoregressive language model. Additionally, we use DSPGAN to upsample 16\nkHz mel-spectrograms to high-fidelity 48 kHz waveforms. In Track 2, we propose\na background audio generator based on large language models (LLMs). This system\nproduces scene-appropriate accompaniment descriptions, synthesizes background\naudio with Tango 2, and integrates it with the speech generated by our Track 1\nsystem. Our submission achieves the second place and the first place in Track 1\nand Track 2 respectively."
                },
                "authors": [
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Jixun Yao"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Kangxiang Xia"
                    },
                    {
                        "name": "Zhao Guo"
                    },
                    {
                        "name": "Ziyu Zhang"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "accepted by ISCSLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11200v3",
                "updated": "2024-10-31T10:15:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    15,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-17T04:20:02Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    4,
                    20,
                    2,
                    0,
                    169,
                    0
                ],
                "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning"
                },
                "summary": "Large language model (LLM) agents have demonstrated impressive capabilities\nin utilizing external tools and knowledge to boost accuracy and reduce\nhallucinations. However, developing prompting techniques that enable LLM agents\nto effectively use these tools and knowledge remains a heuristic and\nlabor-intensive task. Here, we introduce AvaTaR, a novel and automated\nframework that optimizes an LLM agent to effectively leverage provided tools,\nimproving performance on a given task. During optimization, we design a\ncomparator module to iteratively deliver insightful and comprehensive prompts\nto the LLM agent by contrastively reasoning between positive and negative\nexamples sampled from training data. We demonstrate AvaTaR on four complex\nmultimodal retrieval datasets featuring textual, visual, and relational\ninformation, and three general question-answering (QA) datasets. We find AvaTaR\nconsistently outperforms state-of-the-art approaches across all seven tasks,\nexhibiting strong generalization ability when applied to novel cases and\nachieving an average relative improvement of 14% on the Hit@1 metric for the\nretrieval datasets and 13% for the QA datasets. Code and dataset are available\nat https://github.com/zou-group/avatar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have demonstrated impressive capabilities\nin utilizing external tools and knowledge to boost accuracy and reduce\nhallucinations. However, developing prompting techniques that enable LLM agents\nto effectively use these tools and knowledge remains a heuristic and\nlabor-intensive task. Here, we introduce AvaTaR, a novel and automated\nframework that optimizes an LLM agent to effectively leverage provided tools,\nimproving performance on a given task. During optimization, we design a\ncomparator module to iteratively deliver insightful and comprehensive prompts\nto the LLM agent by contrastively reasoning between positive and negative\nexamples sampled from training data. We demonstrate AvaTaR on four complex\nmultimodal retrieval datasets featuring textual, visual, and relational\ninformation, and three general question-answering (QA) datasets. We find AvaTaR\nconsistently outperforms state-of-the-art approaches across all seven tasks,\nexhibiting strong generalization ability when applied to novel cases and\nachieving an average relative improvement of 14% on the Hit@1 metric for the\nretrieval datasets and 13% for the QA datasets. Code and dataset are available\nat https://github.com/zou-group/avatar."
                },
                "authors": [
                    {
                        "name": "Shirley Wu"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Qian Huang"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Kaidi Cao"
                    },
                    {
                        "name": "Vassilis N. Ioannidis"
                    },
                    {
                        "name": "Karthik Subbian"
                    },
                    {
                        "name": "Jure Leskovec"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "NeurIPS 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06196v3",
                "updated": "2024-10-31T10:14:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    14,
                    49,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-10T11:50:29Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    11,
                    50,
                    29,
                    0,
                    162,
                    0
                ],
                "title": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages"
                },
                "summary": "In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 38.7% accuracy, a 24.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 38.7% accuracy, a 24.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels."
                },
                "authors": [
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Simi Hellsten"
                    },
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Jabez Magomere"
                    },
                    {
                        "name": "Ethan A. Chi"
                    },
                    {
                        "name": "Ryan Chi"
                    },
                    {
                        "name": "Scott A. Hale"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    }
                ],
                "author_detail": {
                    "name": "Hannah Rose Kirk"
                },
                "author": "Hannah Rose Kirk",
                "arxiv_comment": "Oral presentation at NeurIPS 2024 Datasets and Benchmarks Track. 10\n  pages, 5 figures, 22 pages supplemental materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15187v2",
                "updated": "2024-10-31T10:10:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    10,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-21T14:29:39Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    14,
                    29,
                    39,
                    4,
                    173,
                    0
                ],
                "title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis"
                },
                "summary": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark."
                },
                "authors": [
                    {
                        "name": "Yulong Hui"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05266v2",
                "updated": "2024-10-31T10:07:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    7,
                    54,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-08T12:42:36Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    12,
                    42,
                    36,
                    4,
                    68,
                    0
                ],
                "title": "ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords."
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Junseok Seo"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17619v2",
                "updated": "2024-10-31T09:45:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    45,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-23T07:17:31Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    17,
                    31,
                    2,
                    297,
                    0
                ],
                "title": "From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management"
                },
                "summary": "This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows."
                },
                "authors": [
                    {
                        "name": "Juhani Merilehto"
                    }
                ],
                "author_detail": {
                    "name": "Juhani Merilehto"
                },
                "author": "Juhani Merilehto",
                "arxiv_comment": "11 pages, 1 figure; corrected the corresponding authors e-mail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23771v1",
                "updated": "2024-10-31T09:39:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    39,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T09:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    39,
                    28,
                    3,
                    305,
                    0
                ],
                "title": "What is Wrong with Perplexity for Long-context Language Modeling?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is Wrong with Perplexity for Long-context Language Modeling?"
                },
                "summary": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL."
                },
                "authors": [
                    {
                        "name": "Lizhe Fang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Stefanie Jegelka"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23769v1",
                "updated": "2024-10-31T09:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    33,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T09:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    33,
                    37,
                    3,
                    305,
                    0
                ],
                "title": "The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams"
                },
                "summary": "Recent research on large language models (LLMs) has primarily focused on\ntheir adaptation and application in specialized domains. The application of\nLLMs in the medical field is mainly concentrated on tasks such as the\nautomation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The\nchallenge of becoming a good teacher is more formidable than that of becoming a\ngood student, and this study pioneers the application of LLMs in the field of\nmedical education. In this work, we investigate the extent to which LLMs can\ngenerate medical qualification exam questions and corresponding answers based\non few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic\ndiseases, we tasked the LLMs with generating open-ended questions and answers\nbased on a subset of sampled admission reports across eight widely used LLMs,\nincluding ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and\nMistral. Furthermore, we engaged medical experts to manually evaluate these\nopen-ended questions and answers across multiple dimensions. The study found\nthat LLMs, after using few-shot prompts, can effectively mimic real-world\nmedical qualification exam questions, whereas there is room for improvement in\nthe correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to\ncorrect and rectify reference answers. Given the immense potential of\nartificial intelligence in the medical field, the task of generating questions\nand answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on large language models (LLMs) has primarily focused on\ntheir adaptation and application in specialized domains. The application of\nLLMs in the medical field is mainly concentrated on tasks such as the\nautomation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The\nchallenge of becoming a good teacher is more formidable than that of becoming a\ngood student, and this study pioneers the application of LLMs in the field of\nmedical education. In this work, we investigate the extent to which LLMs can\ngenerate medical qualification exam questions and corresponding answers based\non few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic\ndiseases, we tasked the LLMs with generating open-ended questions and answers\nbased on a subset of sampled admission reports across eight widely used LLMs,\nincluding ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and\nMistral. Furthermore, we engaged medical experts to manually evaluate these\nopen-ended questions and answers across multiple dimensions. The study found\nthat LLMs, after using few-shot prompts, can effectively mimic real-world\nmedical qualification exam questions, whereas there is room for improvement in\nthe correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to\ncorrect and rectify reference answers. Given the immense potential of\nartificial intelligence in the medical field, the task of generating questions\nand answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research."
                },
                "authors": [
                    {
                        "name": "Yunqi Zhu"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Xuebing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Yang"
                },
                "author": "Xuebing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23764v1",
                "updated": "2024-10-31T09:27:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    27,
                    38,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T09:27:38Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    27,
                    38,
                    3,
                    305,
                    0
                ],
                "title": "Static Analysis Framework for Detecting Use-After-Free Bugs in C++",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Analysis Framework for Detecting Use-After-Free Bugs in C++"
                },
                "summary": "Pointers are a powerful, but dangerous feature provided by the C and C++\nprogramming languages, and incorrect use of pointers is a common source of bugs\nand security vulnerabilities. Making secure software is crucial, as\nvulnerabilities exploited by malicious actors not only lead to monetary losses,\nbut possibly loss of human lives. Fixing these vulnerabilities is costly if\nthey are found at the end of development, and the cost will be even higher if\nfound after deployment. That is why it is desirable to find the bugs as early\nin the development process as possible. We propose a framework that can\nstatically find use-after-free bugs at compile-time and report the errors to\nthe users. It works by tracking the lifetime of objects and memory locations\npointers might point to and, using this information, a possibly invalid\ndereferencing of a pointer can be detected. The framework was tested on over\n100 handwritten small tests, as well as 5 real-world projects, and has shown\ngood results detecting errors, while at the same time highlighting some\nscenarios where false positive reports may occur. Based on the results, it was\nconcluded that our framework achieved its goals, as it is able to detect\nmultiple patterns of use-after-free bugs, and correctly report the errors to\nthe programmer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointers are a powerful, but dangerous feature provided by the C and C++\nprogramming languages, and incorrect use of pointers is a common source of bugs\nand security vulnerabilities. Making secure software is crucial, as\nvulnerabilities exploited by malicious actors not only lead to monetary losses,\nbut possibly loss of human lives. Fixing these vulnerabilities is costly if\nthey are found at the end of development, and the cost will be even higher if\nfound after deployment. That is why it is desirable to find the bugs as early\nin the development process as possible. We propose a framework that can\nstatically find use-after-free bugs at compile-time and report the errors to\nthe users. It works by tracking the lifetime of objects and memory locations\npointers might point to and, using this information, a possibly invalid\ndereferencing of a pointer can be detected. The framework was tested on over\n100 handwritten small tests, as well as 5 real-world projects, and has shown\ngood results detecting errors, while at the same time highlighting some\nscenarios where false positive reports may occur. Based on the results, it was\nconcluded that our framework achieved its goals, as it is able to detect\nmultiple patterns of use-after-free bugs, and correctly report the errors to\nthe programmer."
                },
                "authors": [
                    {
                        "name": "Vlad-Alexandru Teodorescu"
                    },
                    {
                        "name": "Dorel Lucanu"
                    }
                ],
                "author_detail": {
                    "name": "Dorel Lucanu"
                },
                "author": "Dorel Lucanu",
                "arxiv_doi": "10.4204/EPTCS.410.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.410.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings FROM 2024, arXiv:2410.23020",
                "arxiv_journal_ref": "EPTCS 410, 2024, pp. 99-115",
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03621v2",
                "updated": "2024-10-31T09:11:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    11,
                    3,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-05T15:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers"
                },
                "summary": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally."
                },
                "authors": [
                    {
                        "name": "Amit Ben-Artzy"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23746v1",
                "updated": "2024-10-31T09:01:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    1,
                    25,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T09:01:25Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    1,
                    25,
                    3,
                    305,
                    0
                ],
                "title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios"
                },
                "summary": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating advanced prompt usages, human\nrevisions like word substitutions, and writing errors. Our development of\nDetectRL reveals the strengths and limitations of current SOTA detectors. More\nimportantly, we analyzed the potential impact of writing styles, model types,\nattack methods, the text lengths, and real-world human writing factors on\ndifferent types of detectors. We believe DetectRL could serve as an effective\nbenchmark for assessing detectors in real-world scenarios, evolving with\nadvanced attack methods, thus providing more stressful evaluation to drive the\ndevelopment of more efficient detectors. Data and code are publicly available\nat: https://github.com/NLP2CT/DetectRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating advanced prompt usages, human\nrevisions like word substitutions, and writing errors. Our development of\nDetectRL reveals the strengths and limitations of current SOTA detectors. More\nimportantly, we analyzed the potential impact of writing styles, model types,\nattack methods, the text lengths, and real-world human writing factors on\ndifferent types of detectors. We believe DetectRL could serve as an effective\nbenchmark for assessing detectors in real-world scenarios, evolving with\nadvanced attack methods, thus providing more stressful evaluation to drive the\ndevelopment of more efficient detectors. Data and code are publicly available\nat: https://github.com/NLP2CT/DetectRL."
                },
                "authors": [
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Yulin Yuan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "Accepted to NeurIPS 2024 Dataset & Benchmarking Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23743v1",
                "updated": "2024-10-31T08:58:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    58,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T08:58:06Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    58,
                    6,
                    3,
                    305,
                    0
                ],
                "title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A\n  Gradient Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A\n  Gradient Perspective"
                },
                "summary": "What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12833v2",
                "updated": "2024-10-31T08:54:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    54,
                    53,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-19T12:14:09Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    14,
                    9,
                    4,
                    110,
                    0
                ],
                "title": "How Far Can We Go with Practical Function-Level Program Repair?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Can We Go with Practical Function-Level Program Repair?"
                },
                "summary": "Recently, multiple Automated Program Repair (APR) techniques based on Large\nLanguage Models (LLMs) have been proposed to enhance the repair performance.\nWhile these techniques mainly focus on the single-line or hunk-level repair,\nthey face significant challenges in real-world application due to the limited\nrepair task scope and costly statement-level fault localization. However, the\nmore practical function-level APR, which broadens the scope of APR task to fix\nentire buggy functions and requires only cost-efficient function-level fault\nlocalization, remains underexplored. In this paper, we conduct the first\ncomprehensive study of LLM-based function-level APR including investigating the\neffect of the few-shot learning mechanism and the auxiliary repair-relevant\ninformation. Specifically, we adopt six widely-studied LLMs and construct a\nbenchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates\nthat LLMs with zero-shot learning are already powerful function-level APR\ntechniques, while applying the few-shot learning mechanism leads to disparate\nrepair performance. Moreover, we find that directly applying the auxiliary\nrepair-relevant information to LLMs significantly increases function-level\nrepair performance. Inspired by our findings, we propose an LLM-based\nfunction-level APR technique, namely SRepair, which adopts a dual-LLM framework\nto leverage the power of the auxiliary repair-relevant information for\nadvancing the repair performance. The evaluation results demonstrate that\nSRepair can correctly fix 300 single-function bugs in the Defects4J dataset,\nlargely surpassing all previous APR techniques by at least 85%, without the\nneed for the costly statement-level fault location information. Furthermore,\nSRepair successfully fixes 32 multi-function bugs in the Defects4J dataset,\nwhich is the first time achieved by any APR technique ever to our best\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multiple Automated Program Repair (APR) techniques based on Large\nLanguage Models (LLMs) have been proposed to enhance the repair performance.\nWhile these techniques mainly focus on the single-line or hunk-level repair,\nthey face significant challenges in real-world application due to the limited\nrepair task scope and costly statement-level fault localization. However, the\nmore practical function-level APR, which broadens the scope of APR task to fix\nentire buggy functions and requires only cost-efficient function-level fault\nlocalization, remains underexplored. In this paper, we conduct the first\ncomprehensive study of LLM-based function-level APR including investigating the\neffect of the few-shot learning mechanism and the auxiliary repair-relevant\ninformation. Specifically, we adopt six widely-studied LLMs and construct a\nbenchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates\nthat LLMs with zero-shot learning are already powerful function-level APR\ntechniques, while applying the few-shot learning mechanism leads to disparate\nrepair performance. Moreover, we find that directly applying the auxiliary\nrepair-relevant information to LLMs significantly increases function-level\nrepair performance. Inspired by our findings, we propose an LLM-based\nfunction-level APR technique, namely SRepair, which adopts a dual-LLM framework\nto leverage the power of the auxiliary repair-relevant information for\nadvancing the repair performance. The evaluation results demonstrate that\nSRepair can correctly fix 300 single-function bugs in the Defects4J dataset,\nlargely surpassing all previous APR techniques by at least 85%, without the\nneed for the costly statement-level fault location information. Furthermore,\nSRepair successfully fixes 32 multi-function bugs in the Defects4J dataset,\nwhich is the first time achieved by any APR technique ever to our best\nknowledge."
                },
                "authors": [
                    {
                        "name": "Jiahong Xiang"
                    },
                    {
                        "name": "Xiaoyang Xu"
                    },
                    {
                        "name": "Fanchu Kong"
                    },
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Zizheng Zhang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Yuqun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqun Zhang"
                },
                "author": "Yuqun Zhang",
                "arxiv_comment": "https://github.com/GhabiX/SRepair/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23736v1",
                "updated": "2024-10-31T08:49:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    49,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T08:49:05Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    49,
                    5,
                    3,
                    305,
                    0
                ],
                "title": "MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed\n  Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed\n  Image Retrieval"
                },
                "summary": "Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Fei Su"
                    },
                    {
                        "name": "Zhicheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Zhao"
                },
                "author": "Zhicheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00072v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00072v5",
                "updated": "2024-10-31T08:35:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    35,
                    42,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-21T08:52:11Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    8,
                    52,
                    11,
                    4,
                    173,
                    0
                ],
                "title": "Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback"
                },
                "summary": "RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Yukai Miao"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Dawei Wang"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Yanyu Ren"
                    },
                    {
                        "name": "Hongtao Xie"
                    },
                    {
                        "name": "Ce Yang"
                    },
                    {
                        "name": "Xuhui Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xuhui Cai"
                },
                "author": "Xuhui Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00072v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00072v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23728v1",
                "updated": "2024-10-31T08:30:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T08:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "title": "GigaCheck: Detecting LLM-generated Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GigaCheck: Detecting LLM-generated Content"
                },
                "summary": "With the increasing quality and spread of LLM-based assistants, the amount of\nartificially generated content is growing rapidly. In many cases and tasks,\nsuch texts are already indistinguishable from those written by humans, and the\nquality of generation tends to only increase. At the same time, detection\nmethods are developing more slowly, making it challenging to prevent misuse of\nthese technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize artificially generated intervals\nwithin text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing quality and spread of LLM-based assistants, the amount of\nartificially generated content is growing rapidly. In many cases and tasks,\nsuch texts are already indistinguishable from those written by humans, and the\nquality of generation tends to only increase. At the same time, detection\nmethods are developing more slowly, making it challenging to prevent misuse of\nthese technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize artificially generated intervals\nwithin text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets."
                },
                "authors": [
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Aleksandra Tsybina"
                    },
                    {
                        "name": "Sergey Yakubson"
                    },
                    {
                        "name": "Aleksandr Gordeev"
                    },
                    {
                        "name": "Vladimir Dokholyan"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23722v1",
                "updated": "2024-10-31T08:19:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    19,
                    8,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T08:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    19,
                    8,
                    3,
                    305,
                    0
                ],
                "title": "Features characterizing safe aerial-aquatic robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Features characterizing safe aerial-aquatic robots"
                },
                "summary": "This paper underscores the importance of environmental monitoring, and\nspecifically of freshwater ecosystems, which play a critical role in sustaining\nlife and global economy. Despite their importance, insufficient data\navailability prevents a comprehensive understanding of these ecosystems,\nthereby impeding informed decision-making concerning their preservation.\nAerial-aquatic robots are identified as effective tools for freshwater sensing,\noffering rapid deployment and avoiding the need of using ships and manned\nteams.\n  To advance the field of aerial aquatic robots, this paper conducts a\ncomprehensive review of air-water transitions focusing on the water entry\nstrategy of existing prototypes. This analysis also highlights the safety risks\nassociated with each transition and proposes a set of design requirements\nrelating to robots' tasks, mission objectives, and safety measures. To further\nexplore the proposed design requirements, we present a novel robot with VTOL\ncapability, enabling seamless air water transitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper underscores the importance of environmental monitoring, and\nspecifically of freshwater ecosystems, which play a critical role in sustaining\nlife and global economy. Despite their importance, insufficient data\navailability prevents a comprehensive understanding of these ecosystems,\nthereby impeding informed decision-making concerning their preservation.\nAerial-aquatic robots are identified as effective tools for freshwater sensing,\noffering rapid deployment and avoiding the need of using ships and manned\nteams.\n  To advance the field of aerial aquatic robots, this paper conducts a\ncomprehensive review of air-water transitions focusing on the water entry\nstrategy of existing prototypes. This analysis also highlights the safety risks\nassociated with each transition and proposes a set of design requirements\nrelating to robots' tasks, mission objectives, and safety measures. To further\nexplore the proposed design requirements, we present a novel robot with VTOL\ncapability, enabling seamless air water transitions."
                },
                "authors": [
                    {
                        "name": "Andrea Giordano"
                    },
                    {
                        "name": "Luca Romanello"
                    },
                    {
                        "name": "Diego Perez Gonzalez"
                    },
                    {
                        "name": "Mirko Kovac"
                    },
                    {
                        "name": "Sophie F. Armanini"
                    }
                ],
                "author_detail": {
                    "name": "Sophie F. Armanini"
                },
                "author": "Sophie F. Armanini",
                "arxiv_comment": "Peer-reviewed and accepted in IEEE Ubiquitous Robots 2024, New York\n  City",
                "arxiv_journal_ref": "IEEE Ubiquitous Robots 2024, New York City",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03978v3",
                "updated": "2024-10-31T08:11:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    8,
                    11,
                    4,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-04T14:50:45Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    14,
                    50,
                    45,
                    3,
                    186,
                    0
                ],
                "title": "Benchmarking Complex Instruction-Following with Multiple Constraints\n  Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Complex Instruction-Following with Multiple Constraints\n  Composition"
                },
                "summary": "Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition."
                },
                "authors": [
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Lindong Wu"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Wenchuang Li"
                    },
                    {
                        "name": "Binxin Hu"
                    },
                    {
                        "name": "Wendy Gao"
                    },
                    {
                        "name": "Jiaxin Xu"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23704v1",
                "updated": "2024-10-31T07:48:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T07:48:58Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    48,
                    58,
                    3,
                    305,
                    0
                ],
                "title": "Using Scenario-Writing for Identifying and Mitigating Impacts of\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Scenario-Writing for Identifying and Mitigating Impacts of\n  Generative AI"
                },
                "summary": "Impact assessments have emerged as a common way to identify the negative and\npositive implications of AI deployment, with the goal of avoiding the downsides\nof its use. It is undeniable that impact assessments are important - especially\nin the case of rapidly proliferating technologies such as generative AI. But it\nis also essential to critically interrogate the current literature and practice\non impact assessment, to identify its shortcomings, and to develop new\napproaches that are responsive to these limitations. In this provocation, we do\njust that by first critiquing the current impact assessment literature and then\nproposing a novel approach that addresses our concerns: Scenario-Based\nSociotechnical Envisioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact assessments have emerged as a common way to identify the negative and\npositive implications of AI deployment, with the goal of avoiding the downsides\nof its use. It is undeniable that impact assessments are important - especially\nin the case of rapidly proliferating technologies such as generative AI. But it\nis also essential to critically interrogate the current literature and practice\non impact assessment, to identify its shortcomings, and to develop new\napproaches that are responsive to these limitations. In this provocation, we do\njust that by first critiquing the current impact assessment literature and then\nproposing a novel approach that addresses our concerns: Scenario-Based\nSociotechnical Envisioning."
                },
                "authors": [
                    {
                        "name": "Kimon Kieslich"
                    },
                    {
                        "name": "Nicholas Diakopoulos"
                    },
                    {
                        "name": "Natali Helberger"
                    }
                ],
                "author_detail": {
                    "name": "Natali Helberger"
                },
                "author": "Natali Helberger",
                "arxiv_comment": "Publication at NeurIPS'24 workshop Evaluating Evaluations: Examining\n  Best Practices for Measuring Broader Impacts of Generative AI (EvalEval)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23703v1",
                "updated": "2024-10-31T07:48:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    48,
                    44,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T07:48:44Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    48,
                    44,
                    3,
                    305,
                    0
                ],
                "title": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large\n  Language Models"
                },
                "summary": "Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge."
                },
                "authors": [
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Yuxin Xiong"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Branislav Kveton"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23698v1",
                "updated": "2024-10-31T07:41:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    41,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T07:41:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    41,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Aggregate-and-Adapt Natural Language Prompts for Downstream\n  Generalization of CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggregate-and-Adapt Natural Language Prompts for Downstream\n  Generalization of CLIP"
                },
                "summary": "Large pretrained vision-language models like CLIP have shown promising\ngeneralization capability, but may struggle in specialized domains (e.g.,\nsatellite imagery) or fine-grained classification (e.g., car models) where the\nvisual concepts are unseen or under-represented during pretraining. Prompt\nlearning offers a parameter-efficient finetuning framework that can adapt CLIP\nto downstream tasks even when limited annotation data are available. In this\npaper, we improve prompt learning by distilling the textual knowledge from\nnatural language prompts (either human- or LLM-generated) to provide rich\npriors for those under-represented concepts. We first obtain a prompt\n``summary'' aligned to each input image via a learned prompt aggregator. Then\nwe jointly train a prompt generator, optimized to produce a prompt embedding\nthat stays close to the aggregated summary while minimizing task loss at the\nsame time. We dub such prompt embedding as Aggregate-and-Adapted Prompt\nEmbedding (AAPE). AAPE is shown to be able to generalize to different\ndownstream data distributions and tasks, including vision-language\nunderstanding tasks (e.g., few-shot classification, VQA) and generation tasks\n(image captioning) where AAPE achieves competitive performance. We also show\nAAPE is particularly helpful to handle non-canonical and OOD examples.\nFurthermore, AAPE learning eliminates LLM-based inference cost as required by\nbaselines, and scales better with data and LLM model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pretrained vision-language models like CLIP have shown promising\ngeneralization capability, but may struggle in specialized domains (e.g.,\nsatellite imagery) or fine-grained classification (e.g., car models) where the\nvisual concepts are unseen or under-represented during pretraining. Prompt\nlearning offers a parameter-efficient finetuning framework that can adapt CLIP\nto downstream tasks even when limited annotation data are available. In this\npaper, we improve prompt learning by distilling the textual knowledge from\nnatural language prompts (either human- or LLM-generated) to provide rich\npriors for those under-represented concepts. We first obtain a prompt\n``summary'' aligned to each input image via a learned prompt aggregator. Then\nwe jointly train a prompt generator, optimized to produce a prompt embedding\nthat stays close to the aggregated summary while minimizing task loss at the\nsame time. We dub such prompt embedding as Aggregate-and-Adapted Prompt\nEmbedding (AAPE). AAPE is shown to be able to generalize to different\ndownstream data distributions and tasks, including vision-language\nunderstanding tasks (e.g., few-shot classification, VQA) and generation tasks\n(image captioning) where AAPE achieves competitive performance. We also show\nAAPE is particularly helpful to handle non-canonical and OOD examples.\nFurthermore, AAPE learning eliminates LLM-based inference cost as required by\nbaselines, and scales better with data and LLM model size."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Samira Abnar"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Josh Susskind"
                    }
                ],
                "author_detail": {
                    "name": "Josh Susskind"
                },
                "author": "Josh Susskind",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09774v2",
                "updated": "2024-10-31T07:40:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    40,
                    40,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-15T15:46:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    46,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization"
                },
                "summary": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications."
                },
                "authors": [
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07933v2",
                "updated": "2024-10-31T07:36:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    36,
                    39,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-12T06:56:20Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    6,
                    56,
                    20,
                    2,
                    164,
                    0
                ],
                "title": "Large Language Model Unlearning via Embedding-Corrupted Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Unlearning via Embedding-Corrupted Prompts"
                },
                "summary": "Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\n\\textit{nearly zero side effects} in general domains and domains closely\nrelated to the unlearned ones. Additionally, we highlight the scalability of\nour method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no\nadditional cost as the number of parameters increases. We have made our code\npublicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\n\\textit{nearly zero side effects} in general domains and domains closely\nrelated to the unlearned ones. Additionally, we highlight the scalability of\nour method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no\nadditional cost as the number of parameters increases. We have made our code\npublicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}."
                },
                "authors": [
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Yaxuan Wang"
                    },
                    {
                        "name": "Jeffrey Flanigan"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "NeurIPS 2024 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21647v2",
                "updated": "2024-10-31T07:31:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    31,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T01:21:05Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    1,
                    21,
                    5,
                    1,
                    303,
                    0
                ],
                "title": "Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'"
                },
                "summary": "Large language models (LLMs) have achieved high accuracy, i.e., more than 90\npass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a\nnatural question is, whether LLMs achieve comparable code completion\nperformance compared to human developers? Unfortunately, one cannot answer this\nquestion using existing manual crafted or simple (e.g., single-line) code\ngeneration benchmarks, since such tasks fail to represent real-world software\ndevelopment tasks. In addition, existing benchmarks often use poor code\ncorrectness metrics, providing misleading conclusions.\n  To address these challenges, we create REPOCOD, a code generation benchmark\nwith 980 problems collected from 11 popular real-world projects, with more than\n58% of them requiring file-level or repository-level context information. In\naddition, REPOCOD has the longest average canonical solution length (331.6\ntokens) and the highest average cyclomatic complexity (9.00) compared to\nexisting benchmarks. Each task in REPOCOD includes 313.5 developerwritten test\ncases on average for better correctness evaluation. In our evaluations of ten\nLLMs, none of the models achieve more than 30 pass@1 on REPOCOD, indicating the\nnecessity of building stronger LLMs that can help developers in real-world\nsoftware development. REPOCOD is available at\nhttps://github.com/ltasset/REPOCOD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved high accuracy, i.e., more than 90\npass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a\nnatural question is, whether LLMs achieve comparable code completion\nperformance compared to human developers? Unfortunately, one cannot answer this\nquestion using existing manual crafted or simple (e.g., single-line) code\ngeneration benchmarks, since such tasks fail to represent real-world software\ndevelopment tasks. In addition, existing benchmarks often use poor code\ncorrectness metrics, providing misleading conclusions.\n  To address these challenges, we create REPOCOD, a code generation benchmark\nwith 980 problems collected from 11 popular real-world projects, with more than\n58% of them requiring file-level or repository-level context information. In\naddition, REPOCOD has the longest average canonical solution length (331.6\ntokens) and the highest average cyclomatic complexity (9.00) compared to\nexisting benchmarks. Each task in REPOCOD includes 313.5 developerwritten test\ncases on average for better correctness evaluation. In our evaluations of ten\nLLMs, none of the models achieve more than 30 pass@1 on REPOCOD, indicating the\nnecessity of building stronger LLMs that can help developers in real-world\nsoftware development. REPOCOD is available at\nhttps://github.com/ltasset/REPOCOD"
                },
                "authors": [
                    {
                        "name": "Shanchao Liang"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23691v1",
                "updated": "2024-10-31T07:28:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    28,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T07:28:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    28,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "Automatically Learning Hybrid Digital Twins of Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Learning Hybrid Digital Twins of Dynamical Systems"
                },
                "summary": "Digital Twins (DTs) are computational models that simulate the states and\ntemporal dynamics of real-world systems, playing a crucial role in prediction,\nunderstanding, and decision-making across diverse domains. However, existing\napproaches to DTs often struggle to generalize to unseen conditions in\ndata-scarce settings, a crucial requirement for such models. To address these\nlimitations, our work begins by establishing the essential desiderata for\neffective DTs. Hybrid Digital Twins ($\\textbf{HDTwins}$) represent a promising\napproach to address these requirements, modeling systems using a composition of\nboth mechanistic and neural components. This hybrid architecture simultaneously\nleverages (partial) domain knowledge and neural network expressiveness to\nenhance generalization, with its modular design facilitating improved\nevolvability. While existing hybrid models rely on expert-specified\narchitectures with only parameters optimized on data, $\\textit{automatically}$\nspecifying and optimizing HDTwins remains intractable due to the complex search\nspace and the need for flexible integration of domain priors. To overcome this\ncomplexity, we propose an evolutionary algorithm ($\\textbf{HDTwinGen}$) that\nemploys Large Language Models (LLMs) to autonomously propose, evaluate, and\noptimize HDTwins. Specifically, LLMs iteratively generate novel model\nspecifications, while offline tools are employed to optimize emitted\nparameters. Correspondingly, proposed models are evaluated and evolved based on\ntargeted feedback, enabling the discovery of increasingly effective hybrid\nmodels. Our empirical results reveal that HDTwinGen produces generalizable,\nsample-efficient, and evolvable models, significantly advancing DTs' efficacy\nin real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twins (DTs) are computational models that simulate the states and\ntemporal dynamics of real-world systems, playing a crucial role in prediction,\nunderstanding, and decision-making across diverse domains. However, existing\napproaches to DTs often struggle to generalize to unseen conditions in\ndata-scarce settings, a crucial requirement for such models. To address these\nlimitations, our work begins by establishing the essential desiderata for\neffective DTs. Hybrid Digital Twins ($\\textbf{HDTwins}$) represent a promising\napproach to address these requirements, modeling systems using a composition of\nboth mechanistic and neural components. This hybrid architecture simultaneously\nleverages (partial) domain knowledge and neural network expressiveness to\nenhance generalization, with its modular design facilitating improved\nevolvability. While existing hybrid models rely on expert-specified\narchitectures with only parameters optimized on data, $\\textit{automatically}$\nspecifying and optimizing HDTwins remains intractable due to the complex search\nspace and the need for flexible integration of domain priors. To overcome this\ncomplexity, we propose an evolutionary algorithm ($\\textbf{HDTwinGen}$) that\nemploys Large Language Models (LLMs) to autonomously propose, evaluate, and\noptimize HDTwins. Specifically, LLMs iteratively generate novel model\nspecifications, while offline tools are employed to optimize emitted\nparameters. Correspondingly, proposed models are evaluated and evolved based on\ntargeted feedback, enabling the discovery of increasingly effective hybrid\nmodels. Our empirical results reveal that HDTwinGen produces generalizable,\nsample-efficient, and evolvable models, significantly advancing DTs' efficacy\nin real-world applications."
                },
                "authors": [
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Tennison Liu"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Accepted as Spotlight at NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17617v3",
                "updated": "2024-10-31T07:23:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    23,
                    9,
                    3,
                    305,
                    0
                ],
                "published": "2023-12-29T14:25:22Z",
                "published_parsed": [
                    2023,
                    12,
                    29,
                    14,
                    25,
                    22,
                    4,
                    363,
                    0
                ],
                "title": "Large Language Models for Generative Information Extraction: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Generative Information Extraction: A Survey"
                },
                "summary": "Information extraction (IE) aims to extract structural knowledge from plain\nnatural language texts. Recently, generative Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in text understanding and generation. As a\nresult, numerous works have been proposed to integrate LLMs for IE tasks based\non a generative paradigm. To conduct a comprehensive systematic review and\nexploration of LLM efforts for IE tasks, in this study, we survey the most\nrecent advancements in this field. We first present an extensive overview by\ncategorizing these works in terms of various IE subtasks and techniques, and\nthen we empirically analyze the most advanced methods and discover the emerging\ntrend of IE tasks with LLMs. Based on a thorough review conducted, we identify\nseveral insights in technique and promising research directions that deserve\nfurther exploration in future studies. We maintain a public repository and\nconsistently update related works and resources on GitHub\n(\\href{https://github.com/quqxui/Awesome-LLM4IE-Papers}{LLM4IE repository})",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) aims to extract structural knowledge from plain\nnatural language texts. Recently, generative Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in text understanding and generation. As a\nresult, numerous works have been proposed to integrate LLMs for IE tasks based\non a generative paradigm. To conduct a comprehensive systematic review and\nexploration of LLM efforts for IE tasks, in this study, we survey the most\nrecent advancements in this field. We first present an extensive overview by\ncategorizing these works in terms of various IE subtasks and techniques, and\nthen we empirically analyze the most advanced methods and discover the emerging\ntrend of IE tasks with LLMs. Based on a thorough review conducted, we identify\nseveral insights in technique and promising research directions that deserve\nfurther exploration in future studies. We maintain a public repository and\nconsistently update related works and resources on GitHub\n(\\href{https://github.com/quqxui/Awesome-LLM4IE-Papers}{LLM4IE repository})"
                },
                "authors": [
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Wenjun Peng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40555-y}. You can cite the FCS version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22793v2",
                "updated": "2024-10-31T07:20:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    20,
                    35,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T08:17:10Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    17,
                    10,
                    2,
                    304,
                    0
                ],
                "title": "Less is More: DocString Compression in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: DocString Compression in Code Generation"
                },
                "summary": "The widespread use of Large Language Models (LLMs) in software engineering\nhas intensified the need for improved model and resource efficiency. In\nparticular, for neural code generation, LLMs are used to translate\nfunction/method signature and DocString to executable code. DocStrings which\ncapture user re quirements for the code and used as the prompt for LLMs, often\ncontains redundant information. Recent advancements in prompt compression have\nshown promising results in Natural Language Processing (NLP), but their\napplicability to code generation remains uncertain. Our empirical study show\nthat the state-of-the-art prompt compression methods achieve only about 10%\nreduction, as further reductions would cause significant performance\ndegradation. In our study, we propose a novel compression method, ShortenDoc,\ndedicated to DocString compression for code generation. Our extensive\nexperiments on six code generation datasets, five open-source LLMs (1B to 10B\nparameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves\n25-40% compression while preserving the quality of generated code,\noutperforming other baseline methods at similar compression levels. The benefit\nof this research is to improve efficiency and reduce the cost while maintaining\nthe quality of the generated code, especially when calling third-party APIs,\nand is able to reduce the token processing cost by 25-40%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of Large Language Models (LLMs) in software engineering\nhas intensified the need for improved model and resource efficiency. In\nparticular, for neural code generation, LLMs are used to translate\nfunction/method signature and DocString to executable code. DocStrings which\ncapture user re quirements for the code and used as the prompt for LLMs, often\ncontains redundant information. Recent advancements in prompt compression have\nshown promising results in Natural Language Processing (NLP), but their\napplicability to code generation remains uncertain. Our empirical study show\nthat the state-of-the-art prompt compression methods achieve only about 10%\nreduction, as further reductions would cause significant performance\ndegradation. In our study, we propose a novel compression method, ShortenDoc,\ndedicated to DocString compression for code generation. Our extensive\nexperiments on six code generation datasets, five open-source LLMs (1B to 10B\nparameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves\n25-40% compression while preserving the quality of generated code,\noutperforming other baseline methods at similar compression levels. The benefit\nof this research is to improve efficiency and reduce the cost while maintaining\nthe quality of the generated code, especially when calling third-party APIs,\nand is able to reduce the token processing cost by 25-40%."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "UNDER REVIEW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]