[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v1",
                "updated": "2025-03-20T16:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qinxiang Lin"
                    },
                    {
                        "name": "Jinwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v1",
                "updated": "2025-03-20T13:25:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v2",
                "updated": "2025-03-20T03:55:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    3,
                    55,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v2",
                "updated": "2025-03-19T04:49:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    49,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v1",
                "updated": "2025-03-17T23:38:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v1",
                "updated": "2025-03-17T11:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v4",
                "updated": "2025-03-16T16:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    16,
                    25,
                    31,
                    6,
                    75,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.16428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16428v1",
                "updated": "2025-03-20T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    58,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    58,
                    3,
                    79,
                    0
                ],
                "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAttention: Block Sparse Attention with Antidiagonal Scoring"
                },
                "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16419v1",
                "updated": "2025-03-20T17:59:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."
                },
                "authors": [
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Jiamu Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Andrew Wen"
                    },
                    {
                        "name": "Shaochen"
                    },
                    {
                        "name": "Zhong"
                    },
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "arxiv_affiliation": "Henry",
                "author": "Xia Hu",
                "arxiv_comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16416v1",
                "updated": "2025-03-20T17:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    23,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    23,
                    3,
                    79,
                    0
                ],
                "title": "Survey on Evaluation of LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Evaluation of LLM-based Agents"
                },
                "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Alan Li"
                    },
                    {
                        "name": "Guy Uziel"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    }
                ],
                "author_detail": {
                    "name": "Michal Shmueli-Scheuer"
                },
                "author": "Michal Shmueli-Scheuer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16413v1",
                "updated": "2025-03-20T17:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    12,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:12Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    12,
                    3,
                    79,
                    0
                ],
                "title": "M3: 3D-Spatial MultiModal Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3: 3D-Spatial MultiModal Memory"
                },
                "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."
                },
                "authors": [
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Yuchen Song"
                    },
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Xuanbin Peng"
                    },
                    {
                        "name": "Jianglong Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "arxiv_comment": "ICLR2025 homepage: https://m3-spatial-memory.github.io code:\n  https://github.com/MaureenZOU/m3-spatial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15242v2",
                "updated": "2025-03-20T17:58:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    58,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-19T14:19:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?"
                },
                "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."
                },
                "authors": [
                    {
                        "name": "Pierre Chambon"
                    },
                    {
                        "name": "Baptiste Roziere"
                    },
                    {
                        "name": "Benoit Sagot"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18908v2",
                "updated": "2025-03-20T17:56:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    56,
                    5,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-26T17:59:09Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    17,
                    59,
                    9,
                    4,
                    208,
                    0
                ],
                "title": "Wolf: Dense Video Captioning with a World Summarization Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wolf: Dense Video Captioning with a World Summarization Framework"
                },
                "summary": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/."
                },
                "authors": [
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ran Tian"
                    },
                    {
                        "name": "Shuhan Tan"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yin Cui"
                    },
                    {
                        "name": "Sushant Veer"
                    },
                    {
                        "name": "Max Ehrlich"
                    },
                    {
                        "name": "Jonah Philion"
                    },
                    {
                        "name": "Xinshuo Weng"
                    },
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Marco Pavone"
                    }
                ],
                "author_detail": {
                    "name": "Marco Pavone"
                },
                "author": "Marco Pavone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16402v1",
                "updated": "2025-03-20T17:55:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    55,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:55:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    55,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination"
                },
                "summary": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Dongbai Li"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16401v1",
                "updated": "2025-03-20T17:54:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:54:42Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    42,
                    3,
                    79,
                    0
                ],
                "title": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them"
                },
                "summary": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning."
                },
                "authors": [
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Tianren Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "author": "Feng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16400v1",
                "updated": "2025-03-20T17:54:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    37,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:54:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos"
                },
                "summary": "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Junjie Guo"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16397v1",
                "updated": "2025-03-20T17:54:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    2,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:54:02Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    2,
                    3,
                    79,
                    0
                ],
                "title": "Scale-wise Distillation of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-wise Distillation of Diffusion Models"
                },
                "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies."
                },
                "authors": [
                    {
                        "name": "Nikita Starodubcev"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Artem Babenko"
                    },
                    {
                        "name": "Dmitry Baranchuk"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Baranchuk"
                },
                "author": "Dmitry Baranchuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16385v1",
                "updated": "2025-03-20T17:46:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    46,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:46:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    46,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."
                },
                "authors": [
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Yulin Song"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "GengRu Chen"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03661v3",
                "updated": "2025-03-20T17:45:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    45,
                    12,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-04T06:02:52Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    6,
                    2,
                    52,
                    3,
                    186,
                    0
                ],
                "title": "Where's That Voice Coming? Continual Learning for Sound Source\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where's That Voice Coming? Continual Learning for Sound Source\n  Localization"
                },
                "summary": "Sound source localization (SSL) is essential for many speech-processing\napplications. Deep learning models have achieved high performance, but often\nfail when the training and inference environments differ. Adapting SSL models\nto dynamic acoustic conditions faces a major challenge: catastrophic\nforgetting. In this work, we propose an exemplar-free continual learning\nstrategy for SSL (CL-SSL) to address such a forgetting phenomenon. CL-SSL\napplies task-specific sub-networks to adapt across diverse acoustic\nenvironments while retaining previously learned knowledge. It also uses a\nscaling mechanism to limit parameter growth, ensuring consistent performance\nacross incremental tasks. We evaluated CL-SSL on simulated data with varying\nmicrophone distances and real-world data with different noise levels. The\nresults demonstrate CL-SSL's ability to maintain high accuracy with minimal\nparameter increase, offering an efficient solution for SSL applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound source localization (SSL) is essential for many speech-processing\napplications. Deep learning models have achieved high performance, but often\nfail when the training and inference environments differ. Adapting SSL models\nto dynamic acoustic conditions faces a major challenge: catastrophic\nforgetting. In this work, we propose an exemplar-free continual learning\nstrategy for SSL (CL-SSL) to address such a forgetting phenomenon. CL-SSL\napplies task-specific sub-networks to adapt across diverse acoustic\nenvironments while retaining previously learned knowledge. It also uses a\nscaling mechanism to limit parameter growth, ensuring consistent performance\nacross incremental tasks. We evaluated CL-SSL on simulated data with varying\nmicrophone distances and real-world data with different noise levels. The\nresults demonstrate CL-SSL's ability to maintain high accuracy with minimal\nparameter increase, offering an efficient solution for SSL applications."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "Accepted to ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03035v2",
                "updated": "2025-03-20T17:43:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    43,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-03T22:41:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    41,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments"
                },
                "summary": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. while many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. recently, large language models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. however, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. we\naddress these limitations via spine, an online planner for missions with\nincomplete mission specifications provided in natural language. the planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. tasks are\nautomatically validated for safety and refined online with new map\nobservations. we evaluate spine in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at:\nhttps://zacravichandran.github.io/SPINE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. while many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. recently, large language models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. however, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. we\naddress these limitations via spine, an online planner for missions with\nincomplete mission specifications provided in natural language. the planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. tasks are\nautomatically validated for safety and refined online with new map\nobservations. we evaluate spine in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at:\nhttps://zacravichandran.github.io/SPINE."
                },
                "authors": [
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Mariliza Tzes"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16379v1",
                "updated": "2025-03-20T17:42:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    42,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:42:11Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    42,
                    11,
                    3,
                    79,
                    0
                ],
                "title": "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project"
                },
                "summary": "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses."
                },
                "authors": [
                    {
                        "name": "P. S. Corasaniti"
                    },
                    {
                        "name": "T. R. G. Richardson"
                    },
                    {
                        "name": "S. Ettori"
                    },
                    {
                        "name": "M. De Petris"
                    },
                    {
                        "name": "E. Rasia"
                    },
                    {
                        "name": "W. Cui"
                    },
                    {
                        "name": "G. Yepes"
                    },
                    {
                        "name": "G. Gianfagna"
                    },
                    {
                        "name": "A. M. C. Le Bun"
                    },
                    {
                        "name": "Y. Rasera"
                    }
                ],
                "author_detail": {
                    "name": "Y. Rasera"
                },
                "author": "Y. Rasera",
                "arxiv_comment": "23 pages, 29 figures, accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v5",
                "updated": "2025-03-20T17:39:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    13,
                    3,
                    79,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_comment": "13 pages, 6 figures, VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12372v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16376v1",
                "updated": "2025-03-20T17:39:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    6,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:39:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images"
                },
                "summary": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."
                },
                "authors": [
                    {
                        "name": "Leyang Wang"
                    },
                    {
                        "name": "Joice Lin"
                    }
                ],
                "author_detail": {
                    "name": "Joice Lin"
                },
                "author": "Joice Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16356v1",
                "updated": "2025-03-20T17:14:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners"
                },
                "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."
                },
                "authors": [
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16355v1",
                "updated": "2025-03-20T17:13:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    13,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:13:57Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    13,
                    57,
                    3,
                    79,
                    0
                ],
                "title": "DEMNUni: the Sunyaev-Zel'dovich effect in the presence of massive\n  neutrinos and dynamical dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMNUni: the Sunyaev-Zel'dovich effect in the presence of massive\n  neutrinos and dynamical dark energy"
                },
                "summary": "In recent years, the study of secondary anisotropies in the Cosmic Microwave\nBackground has become a fundamental instrument to test our understanding of\nCosmology and Astrophysics. Using a set of lightcones produced with the ``Dark\nEnergy and Massive Neutrino Universe'' $N$-body simulations we study how\ndifferent dark energy models and neutrino masses impact the properties of the\nSunyaev-Zel'dovich (SZ) effects, focusing on the signal arising from galaxy\nclusters and groups. We analyse the distribution of values, Compton-$y$\nparameter for the thermal SZ effect and $\\Delta T/T$ for the kinematic SZ\neffect, and study their angular power spectra. We find that the distribution of\nlogarithmic Compton parameter can be fitted with a skewed Gaussian, with a mean\nthat, at fixed dark energy model, decreases linearly with an approximate slope\nof $10 f_\\nu$. Regarding the power spectrum of the thermal SZ effect, we find\nthat an increase in $\\sum {m_\\nu}$ is observed as a power-law scaling with\nrespect to $\\sigma_8^{\\mathrm{cb}}$, with exponents ranging from 7.2 to 8.2. We\nalso find that four cosmological models, one with $\\sum {m_\\nu} = 0.16$ eV and\nthree with $\\sum {m_\\nu} = 0.32$ eV, fit equally well the Planck data for the\nCompton-$y$. For all the \\texttt{DEMNUni} models we forecast the cumulative\nsignal-to-noise for thermal SZ observations with the LAT instrument of Simons\nObservatory; furthermore, we compute a tailored $\\chi_\\mathrm{SNR}^2$ estimator\nto infer if they can be distinguished from the reference $\\Lambda$CDM. We also\nprovide estimates for the power spectrum of the cluster component of the\nkinematic SZ effect, in all the different cosmological scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the study of secondary anisotropies in the Cosmic Microwave\nBackground has become a fundamental instrument to test our understanding of\nCosmology and Astrophysics. Using a set of lightcones produced with the ``Dark\nEnergy and Massive Neutrino Universe'' $N$-body simulations we study how\ndifferent dark energy models and neutrino masses impact the properties of the\nSunyaev-Zel'dovich (SZ) effects, focusing on the signal arising from galaxy\nclusters and groups. We analyse the distribution of values, Compton-$y$\nparameter for the thermal SZ effect and $\\Delta T/T$ for the kinematic SZ\neffect, and study their angular power spectra. We find that the distribution of\nlogarithmic Compton parameter can be fitted with a skewed Gaussian, with a mean\nthat, at fixed dark energy model, decreases linearly with an approximate slope\nof $10 f_\\nu$. Regarding the power spectrum of the thermal SZ effect, we find\nthat an increase in $\\sum {m_\\nu}$ is observed as a power-law scaling with\nrespect to $\\sigma_8^{\\mathrm{cb}}$, with exponents ranging from 7.2 to 8.2. We\nalso find that four cosmological models, one with $\\sum {m_\\nu} = 0.16$ eV and\nthree with $\\sum {m_\\nu} = 0.32$ eV, fit equally well the Planck data for the\nCompton-$y$. For all the \\texttt{DEMNUni} models we forecast the cumulative\nsignal-to-noise for thermal SZ observations with the LAT instrument of Simons\nObservatory; furthermore, we compute a tailored $\\chi_\\mathrm{SNR}^2$ estimator\nto infer if they can be distinguished from the reference $\\Lambda$CDM. We also\nprovide estimates for the power spectrum of the cluster component of the\nkinematic SZ effect, in all the different cosmological scenarios."
                },
                "authors": [
                    {
                        "name": "Davide Luchina"
                    },
                    {
                        "name": "Mauro Roncarelli"
                    },
                    {
                        "name": "Matteo Calabrese"
                    },
                    {
                        "name": "Giulio Fabbian"
                    },
                    {
                        "name": "Carmelita Carbone"
                    }
                ],
                "author_detail": {
                    "name": "Carmelita Carbone"
                },
                "author": "Carmelita Carbone",
                "arxiv_comment": "25 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16354v1",
                "updated": "2025-03-20T17:13:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    13,
                    51,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:13:51Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    13,
                    51,
                    3,
                    79,
                    0
                ],
                "title": "Hypercyclicity of Weighted shifts on weighted Bergman and Dirichlet\n  spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypercyclicity of Weighted shifts on weighted Bergman and Dirichlet\n  spaces"
                },
                "summary": "Let $B_w$ and $F_w$ denote, respectively, the weighted backward and forward\nshift operators defined on the weighted Bergman space $A^p_{\\phi}$, or the\nweighted Dirichlet space ${D}^p_{\\phi}$ of the unit disc, where the weight\nfunction $\\phi(z)$ is mostly radial. We first obtain sufficient conditions for\n$B_w$ and $F_w$ to be continuous on these spaces. For radial weights, we derive\nnorm estimates for coefficient functionals on $A^p_{\\phi}$ and $D^p_{\\phi}$,\nand using those estimates we infer when the weighted shifts or their adjoints\nare hypercyclic. We also deal with a non-radial case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $B_w$ and $F_w$ denote, respectively, the weighted backward and forward\nshift operators defined on the weighted Bergman space $A^p_{\\phi}$, or the\nweighted Dirichlet space ${D}^p_{\\phi}$ of the unit disc, where the weight\nfunction $\\phi(z)$ is mostly radial. We first obtain sufficient conditions for\n$B_w$ and $F_w$ to be continuous on these spaces. For radial weights, we derive\nnorm estimates for coefficient functionals on $A^p_{\\phi}$ and $D^p_{\\phi}$,\nand using those estimates we infer when the weighted shifts or their adjoints\nare hypercyclic. We also deal with a non-radial case."
                },
                "authors": [
                    {
                        "name": "Bibhash Kumar Das"
                    },
                    {
                        "name": "Aneesh Mundayadan"
                    }
                ],
                "author_detail": {
                    "name": "Aneesh Mundayadan"
                },
                "author": "Aneesh Mundayadan",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.FA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "47A16, 46E22, 32K05, 47B32, 47B37, 37A99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16351v1",
                "updated": "2025-03-20T17:09:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:09:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling\n  Biological Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling\n  Biological Sequences"
                },
                "summary": "Deep learning architectures such as convolutional neural networks and\nTransformers have revolutionized biological sequence modeling, with recent\nadvances driven by scaling up foundation and task-specific models. The\ncomputational resources and large datasets required, however, limit their\napplicability in biological contexts. We introduce Lyra, a subquadratic\narchitecture for sequence modeling, grounded in the biological framework of\nepistasis for understanding sequence-to-function relationships. Mathematically,\nwe demonstrate that state space models efficiently capture global epistatic\ninteractions and combine them with projected gated convolutions for modeling\nlocal relationships. We demonstrate that Lyra is performant across over 100\nwide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in\nmany key areas, including protein fitness landscape prediction, biophysical\nproperty prediction (e.g. disordered protein region functions) peptide\nengineering applications (e.g. antibody binding, cell-penetrating peptide\nprediction), RNA structure analysis, RNA function prediction, and CRISPR guide\ndesign. It achieves this with orders-of-magnitude improvements in inference\nspeed and reduction in parameters (up to 120,000-fold in our tests) compared to\nrecent biology foundation models. Using Lyra, we were able to train and run\nevery task in this study on two or fewer GPUs in under two hours, democratizing\naccess to biological sequence modeling at SOTA performance, with potential\napplications to many fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning architectures such as convolutional neural networks and\nTransformers have revolutionized biological sequence modeling, with recent\nadvances driven by scaling up foundation and task-specific models. The\ncomputational resources and large datasets required, however, limit their\napplicability in biological contexts. We introduce Lyra, a subquadratic\narchitecture for sequence modeling, grounded in the biological framework of\nepistasis for understanding sequence-to-function relationships. Mathematically,\nwe demonstrate that state space models efficiently capture global epistatic\ninteractions and combine them with projected gated convolutions for modeling\nlocal relationships. We demonstrate that Lyra is performant across over 100\nwide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in\nmany key areas, including protein fitness landscape prediction, biophysical\nproperty prediction (e.g. disordered protein region functions) peptide\nengineering applications (e.g. antibody binding, cell-penetrating peptide\nprediction), RNA structure analysis, RNA function prediction, and CRISPR guide\ndesign. It achieves this with orders-of-magnitude improvements in inference\nspeed and reduction in parameters (up to 120,000-fold in our tests) compared to\nrecent biology foundation models. Using Lyra, we were able to train and run\nevery task in this study on two or fewer GPUs in under two hours, democratizing\naccess to biological sequence modeling at SOTA performance, with potential\napplications to many fields."
                },
                "authors": [
                    {
                        "name": "Krithik Ramesh"
                    },
                    {
                        "name": "Sameed M. Siddiqui"
                    },
                    {
                        "name": "Albert Gu"
                    },
                    {
                        "name": "Michael D. Mitzenmacher"
                    },
                    {
                        "name": "Pardis C. Sabeti"
                    }
                ],
                "author_detail": {
                    "name": "Pardis C. Sabeti"
                },
                "arxiv_affiliation": "Howard Hughes Medical Institute",
                "author": "Pardis C. Sabeti",
                "arxiv_comment": "53 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16334v1",
                "updated": "2025-03-20T16:55:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    55,
                    26,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:55:26Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    55,
                    26,
                    3,
                    79,
                    0
                ],
                "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates"
                },
                "summary": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."
                },
                "authors": [
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Lifu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lifu Huang"
                },
                "author": "Lifu Huang",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12509v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12509v4",
                "updated": "2025-03-20T16:45:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-18T03:47:53Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    47,
                    53,
                    1,
                    49,
                    0
                ],
                "title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents"
                },
                "summary": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code."
                },
                "authors": [
                    {
                        "name": "Kangda Wei"
                    },
                    {
                        "name": "Xi Shi"
                    },
                    {
                        "name": "Jonathan Tong"
                    },
                    {
                        "name": "Sai Ramana Reddy"
                    },
                    {
                        "name": "Anandhavelu Natarajan"
                    },
                    {
                        "name": "Rajiv Jain"
                    },
                    {
                        "name": "Aparna Garimella"
                    },
                    {
                        "name": "Ruihong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruihong Huang"
                },
                "author": "Ruihong Huang",
                "arxiv_comment": "Need company internal approval before public release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12509v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12509v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16326v1",
                "updated": "2025-03-20T16:45:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:45:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence"
                },
                "summary": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication."
                },
                "authors": [
                    {
                        "name": "Long Yuan"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Wangyuxuan Zhai"
                    },
                    {
                        "name": "Xiaoyu Zhu"
                    },
                    {
                        "name": "You Li"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Yun Nie"
                },
                "author": "Jian-Yun Nie",
                "arxiv_comment": "15 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16320v1",
                "updated": "2025-03-20T16:44:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    44,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:44:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    44,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Issue2Test: Generating Reproducing Test Cases from Issue Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue2Test: Generating Reproducing Test Cases from Issue Reports"
                },
                "summary": "Automated tools for solving GitHub issues are receiving significant attention\nby both researchers and practitioners, e.g., in the form of foundation models\nand LLM-based agents prompted with issues. A crucial step toward successfully\nsolving an issue is creating a test case that accurately reproduces the issue.\nSuch a test case can guide the search for an appropriate patch and help\nvalidate whether the patch matches the issue's intent. However, existing\ntechniques for issue reproduction show only moderate success. This paper\npresents Issue2Test, an LLM-based technique for automatically generating a\nreproducing test case for a given issue report. Unlike automated regression\ntest generators, which aim at creating passing tests, our approach aims at a\ntest that fails, and that fails specifically for the reason described in the\nissue. To this end, Issue2Test performs three steps: (1) understand the issue\nand gather context (e.g., related files and project-specific guidelines)\nrelevant for reproducing it; (2) generate a candidate test case; and (3)\niteratively refine the test case based on compilation and runtime feedback\nuntil it fails and the failure aligns with the problem described in the issue.\nWe evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully\nreproduces 30.4 of the issues, achieving a 40.1% relative improvement over the\nbest existing technique. Our evaluation also shows that Issue2test reproduces\n28 issues that seven prior techniques fail to address, contributing a total of\n68.3% of all issues reproduced by any tool. We envision our approach to\ncontribute to enhancing the overall progress in the important task of\nautomatically solving GitHub issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated tools for solving GitHub issues are receiving significant attention\nby both researchers and practitioners, e.g., in the form of foundation models\nand LLM-based agents prompted with issues. A crucial step toward successfully\nsolving an issue is creating a test case that accurately reproduces the issue.\nSuch a test case can guide the search for an appropriate patch and help\nvalidate whether the patch matches the issue's intent. However, existing\ntechniques for issue reproduction show only moderate success. This paper\npresents Issue2Test, an LLM-based technique for automatically generating a\nreproducing test case for a given issue report. Unlike automated regression\ntest generators, which aim at creating passing tests, our approach aims at a\ntest that fails, and that fails specifically for the reason described in the\nissue. To this end, Issue2Test performs three steps: (1) understand the issue\nand gather context (e.g., related files and project-specific guidelines)\nrelevant for reproducing it; (2) generate a candidate test case; and (3)\niteratively refine the test case based on compilation and runtime feedback\nuntil it fails and the failure aligns with the problem described in the issue.\nWe evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully\nreproduces 30.4 of the issues, achieving a 40.1% relative improvement over the\nbest existing technique. Our evaluation also shows that Issue2test reproduces\n28 issues that seven prior techniques fail to address, contributing a total of\n68.3% of all issues reproduced by any tool. We envision our approach to\ncontribute to enhancing the overall progress in the important task of\nautomatically solving GitHub issues."
                },
                "authors": [
                    {
                        "name": "Noor Nashid"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Ali Mesbah"
                    }
                ],
                "author_detail": {
                    "name": "Ali Mesbah"
                },
                "author": "Ali Mesbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09091v2",
                "updated": "2025-03-20T16:43:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    43,
                    54,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-12T06:03:33Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    3,
                    33,
                    2,
                    71,
                    0
                ],
                "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Foundation Models for Computational Pathology: A Survey"
                },
                "summary": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Guihong Wan"
                    },
                    {
                        "name": "Xintao Wu"
                    },
                    {
                        "name": "Xinyu Wu"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Christine G. Lian"
                    },
                    {
                        "name": "Peter K. Sorger"
                    },
                    {
                        "name": "Yevgeniy R. Semenov"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16315v1",
                "updated": "2025-03-20T16:38:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    38,
                    16,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:38:16Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    38,
                    16,
                    3,
                    79,
                    0
                ],
                "title": "Active Learning For Repairable Hardware Systems With Partial Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning For Repairable Hardware Systems With Partial Coverage"
                },
                "summary": "Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test."
                },
                "authors": [
                    {
                        "name": "Michael Potter"
                    },
                    {
                        "name": "Beyza Kalkanlı"
                    },
                    {
                        "name": "Deniz Erdoğmuş"
                    },
                    {
                        "name": "Michael Everett"
                    }
                ],
                "author_detail": {
                    "name": "Michael Everett"
                },
                "author": "Michael Everett",
                "arxiv_comment": "Submitted to IEEE Reliability and Maintainability Symposium - Europe\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18400v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18400v3",
                "updated": "2025-03-20T16:37:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    37,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2024-04-29T03:30:06Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    3,
                    30,
                    6,
                    0,
                    120,
                    0
                ],
                "title": "LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models"
                },
                "summary": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR"
                },
                "authors": [
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Kazem Meidani"
                    },
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    },
                    {
                        "name": "Chandan K Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K Reddy"
                },
                "author": "Chandan K Reddy",
                "arxiv_comment": "ICLR 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18400v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18400v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13017v2",
                "updated": "2025-03-20T16:31:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    31,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-17T10:21:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    21,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction"
                },
                "summary": "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction."
                },
                "authors": [
                    {
                        "name": "Philip D. Michel"
                    },
                    {
                        "name": "Paolo A. Mazzali"
                    },
                    {
                        "name": "Daniel A. Perley"
                    },
                    {
                        "name": "K-Ryan Hinds"
                    },
                    {
                        "name": "Jacob L. Wise"
                    }
                ],
                "author_detail": {
                    "name": "Jacob L. Wise"
                },
                "author": "Jacob L. Wise",
                "arxiv_doi": "10.1093/mnras/staf443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.13017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 Pages, 12 Figures, 5 Tables, Accepted for publication in MNRAS\n  2025 March 07. Received in original form 2024 December 19",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v1",
                "updated": "2025-03-20T16:25:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2409.18486",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v1",
                "updated": "2025-03-20T16:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qinxiang Lin"
                    },
                    {
                        "name": "Jinwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16301v1",
                "updated": "2025-03-20T16:23:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "The Impact Of Industrial Production On Economic Growth: New Empirical\n  Evidence For Türkiye In A Material Development Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact Of Industrial Production On Economic Growth: New Empirical\n  Evidence For Türkiye In A Material Development Framework"
                },
                "summary": "There are different views in the literature regarding economic growth and\ndevelopment. Growth and development hypotheses have been addressed from many\ndifferent perspectives. It is aimed to examine the view of production with the\nHeavy Industry Initiative proposed by Prof. Dr. Necmettin Erbakan a crucial\npart of Material Development in T\\\"urkiye. In this context, an examination has\nbeen conducted on GDP, Mining, Manufacturing, Energy, and Chemical Industry\nProductions in T\\\"urkiye between 1995-2023, focusing on percentage changes. The\nstationarity of our data set has been checked, and a VAR estimation has been\nperformed by determining the lag length. Long-term relationships have been\nidentified through the Johansen Cointegration test, and after confirming that\nthere are no issues of autocorrelation and changing variance, variance\ndecomposition has been carried out. Thus, inferences have been made regarding\nthe effects of the identified sectors on GDP. Based on the analysis results, it\nhas been observed that there is a stronger relationship between GDP and the\nManufacturing Sector compared to the Mining, Energy, and Chemical Sectors, and\nit has been concluded that sectoral shocks diminish over time. Additionally,\nVAR Impulse-Response Analysis has been conducted among the variables. In this\ncontext, it has been observed that the variables in the dataset have an impact\non each other in both the short and long term. It has been concluded that the\nshocks were more pronounced in the initial periods, while the economy reached a\nbalance over time. In this regard, it has been confirmed that the heavy\nindustry initiative exhibited different responses sectorally, while also being\nsignificant in terms of economic growth and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are different views in the literature regarding economic growth and\ndevelopment. Growth and development hypotheses have been addressed from many\ndifferent perspectives. It is aimed to examine the view of production with the\nHeavy Industry Initiative proposed by Prof. Dr. Necmettin Erbakan a crucial\npart of Material Development in T\\\"urkiye. In this context, an examination has\nbeen conducted on GDP, Mining, Manufacturing, Energy, and Chemical Industry\nProductions in T\\\"urkiye between 1995-2023, focusing on percentage changes. The\nstationarity of our data set has been checked, and a VAR estimation has been\nperformed by determining the lag length. Long-term relationships have been\nidentified through the Johansen Cointegration test, and after confirming that\nthere are no issues of autocorrelation and changing variance, variance\ndecomposition has been carried out. Thus, inferences have been made regarding\nthe effects of the identified sectors on GDP. Based on the analysis results, it\nhas been observed that there is a stronger relationship between GDP and the\nManufacturing Sector compared to the Mining, Energy, and Chemical Sectors, and\nit has been concluded that sectoral shocks diminish over time. Additionally,\nVAR Impulse-Response Analysis has been conducted among the variables. In this\ncontext, it has been observed that the variables in the dataset have an impact\non each other in both the short and long term. It has been concluded that the\nshocks were more pronounced in the initial periods, while the economy reached a\nbalance over time. In this regard, it has been confirmed that the heavy\nindustry initiative exhibited different responses sectorally, while also being\nsignificant in terms of economic growth and development."
                },
                "authors": [
                    {
                        "name": "Ali Doğdu"
                    },
                    {
                        "name": "Murad Kayacan"
                    }
                ],
                "author_detail": {
                    "name": "Murad Kayacan"
                },
                "author": "Murad Kayacan",
                "arxiv_comment": "in Turkish language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v3",
                "updated": "2025-03-20T16:15:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    15,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16284v1",
                "updated": "2025-03-20T16:12:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    12,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:12:42Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    12,
                    42,
                    3,
                    79,
                    0
                ],
                "title": "PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance\n  Learning for Whole Slide Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance\n  Learning for Whole Slide Image Classification"
                },
                "summary": "Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs."
                },
                "authors": [
                    {
                        "name": "Sharon Peled"
                    },
                    {
                        "name": "Yosef E. Maruvka"
                    },
                    {
                        "name": "Moti Freiman"
                    }
                ],
                "author_detail": {
                    "name": "Moti Freiman"
                },
                "author": "Moti Freiman",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18688v3",
                "updated": "2025-03-20T16:07:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    7,
                    9,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-27T19:00:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    19,
                    0,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment"
                },
                "summary": "With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively."
                },
                "authors": [
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Vaibhav Singh"
                    },
                    {
                        "name": "Tianrui Guan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16278v1",
                "updated": "2025-03-20T16:07:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    7,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:07:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    7,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens"
                },
                "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR."
                },
                "authors": [
                    {
                        "name": "Shuqi Lu"
                    },
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Xiaohong Ji"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16262v1",
                "updated": "2025-03-20T15:56:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    54,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:56:54Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    54,
                    3,
                    79,
                    0
                ],
                "title": "Neurosymbolic Architectural Reasoning: Towards Formal Analysis through\n  Neural Software Architecture Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic Architectural Reasoning: Towards Formal Analysis through\n  Neural Software Architecture Inference"
                },
                "summary": "Formal analysis to ensure adherence of software to defined architectural\nconstraints is not yet broadly used within software development, due to the\neffort involved in defining formal architecture models. Within this paper, we\noutline neural architecture inference to solve the problem of having a formal\narchitecture definition for subsequent symbolic reasoning over these\narchitectures, enabling neurosymbolic architectural reasoning. We discuss how\nthis approach works in general and outline a research agenda based on six\ngeneral research question that need to be addressed, to achieve this vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal analysis to ensure adherence of software to defined architectural\nconstraints is not yet broadly used within software development, due to the\neffort involved in defining formal architecture models. Within this paper, we\noutline neural architecture inference to solve the problem of having a formal\narchitecture definition for subsequent symbolic reasoning over these\narchitectures, enabling neurosymbolic architectural reasoning. We discuss how\nthis approach works in general and outline a research agenda based on six\ngeneral research question that need to be addressed, to achieve this vision."
                },
                "authors": [
                    {
                        "name": "Steffen Herbold"
                    },
                    {
                        "name": "Christoph Knieke"
                    },
                    {
                        "name": "Andreas Rausch"
                    },
                    {
                        "name": "Christian Schindler"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schindler"
                },
                "author": "Christian Schindler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16260v1",
                "updated": "2025-03-20T15:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:56:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data"
                },
                "summary": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts."
                },
                "authors": [
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15195v2",
                "updated": "2025-03-20T15:49:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    49,
                    10,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-19T13:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Handwritten Text Recognition"
                },
                "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."
                },
                "authors": [
                    {
                        "name": "Giorgia Crosilla"
                    },
                    {
                        "name": "Lukas Klic"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16252v1",
                "updated": "2025-03-20T15:46:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    46,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:46:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    46,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning"
                },
                "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1."
                },
                "authors": [
                    {
                        "name": "Zhaowei Liu"
                    },
                    {
                        "name": "Xin Guo"
                    },
                    {
                        "name": "Fangqi Lou"
                    },
                    {
                        "name": "Lingfeng Zeng"
                    },
                    {
                        "name": "Jinyi Niu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Weige Cai"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Xueqian Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Dezhi Chen"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Zuo Bai"
                    },
                    {
                        "name": "Liwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liwen Zhang"
                },
                "author": "Liwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00906v3",
                "updated": "2025-03-20T15:42:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    42,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-01T17:51:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    51,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board."
                },
                "authors": [
                    {
                        "name": "Kowe Kadoma"
                    },
                    {
                        "name": "Danaé Metaxa"
                    },
                    {
                        "name": "Mor Naaman"
                    }
                ],
                "author_detail": {
                    "name": "Mor Naaman"
                },
                "author": "Mor Naaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06061v2",
                "updated": "2025-03-20T15:37:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    37,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2024-05-09T19:10:11Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    19,
                    10,
                    11,
                    3,
                    130,
                    0
                ],
                "title": "GPTCoach: Towards LLM-Based Physical Activity Coaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTCoach: Towards LLM-Based Physical Activity Coaching"
                },
                "summary": "Mobile health applications show promise for scalable physical activity\npromotion but are often insufficiently personalized. In contrast, health\ncoaching offers highly personalized support but can be prohibitively expensive\nand inaccessible. This study draws inspiration from health coaching to explore\nhow large language models (LLMs) might address personalization challenges in\nmobile health. We conduct formative interviews with 12 health professionals and\n10 potential coaching recipients to develop design principles for an LLM-based\nhealth coach. We then built GPTCoach, a chatbot that implements the onboarding\nconversation from an evidence-based coaching program, uses conversational\nstrategies from motivational interviewing, and incorporates wearable data to\ncreate personalized physical activity plans. In a lab study with 16\nparticipants using three months of historical data, we find promising evidence\nthat GPTCoach gathers rich qualitative information to offer personalized\nsupport, with users feeling comfortable sharing concerns. We conclude with\nimplications for future research on LLM-based physical activity support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile health applications show promise for scalable physical activity\npromotion but are often insufficiently personalized. In contrast, health\ncoaching offers highly personalized support but can be prohibitively expensive\nand inaccessible. This study draws inspiration from health coaching to explore\nhow large language models (LLMs) might address personalization challenges in\nmobile health. We conduct formative interviews with 12 health professionals and\n10 potential coaching recipients to develop design principles for an LLM-based\nhealth coach. We then built GPTCoach, a chatbot that implements the onboarding\nconversation from an evidence-based coaching program, uses conversational\nstrategies from motivational interviewing, and incorporates wearable data to\ncreate personalized physical activity plans. In a lab study with 16\nparticipants using three months of historical data, we find promising evidence\nthat GPTCoach gathers rich qualitative information to offer personalized\nsupport, with users feeling comfortable sharing concerns. We conclude with\nimplications for future research on LLM-based physical activity support."
                },
                "authors": [
                    {
                        "name": "Matthew Jörke"
                    },
                    {
                        "name": "Shardul Sapkota"
                    },
                    {
                        "name": "Lyndsea Warkenthien"
                    },
                    {
                        "name": "Niklas Vainio"
                    },
                    {
                        "name": "Paul Schmiedmayer"
                    },
                    {
                        "name": "Emma Brunskill"
                    },
                    {
                        "name": "James A. Landay"
                    }
                ],
                "author_detail": {
                    "name": "James A. Landay"
                },
                "author": "James A. Landay",
                "arxiv_doi": "10.1145/3706598.3713819",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713819",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.06061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please note that the title has been updated from a previous pre-print\n  (previously: \"Supporting Physical Activity Behavior Change with LLM-Based\n  Conversational Agents\")",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v3",
                "updated": "2025-03-20T15:35:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    35,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization in language models via probabilistic extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization in language models via probabilistic extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "A. Feder Cooper"
                    }
                ],
                "author_detail": {
                    "name": "A. Feder Cooper"
                },
                "author": "A. Feder Cooper",
                "arxiv_comment": "NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10491v2",
                "updated": "2025-03-20T15:32:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    32,
                    47,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-14T13:35:47Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    35,
                    47,
                    0,
                    288,
                    0
                ],
                "title": "TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning"
                },
                "summary": "Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Aritra Bhowmik"
                    },
                    {
                        "name": "Mohammad Mahdi Derakhshani"
                    },
                    {
                        "name": "Dennis Koelma"
                    },
                    {
                        "name": "Yuki M. Asano"
                    },
                    {
                        "name": "Martin R. Oswald"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20089v2",
                "updated": "2025-03-20T15:28:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    28,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2024-09-30T08:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM safeguarding via refusal feature adversarial training"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Virginie Do"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16222v1",
                "updated": "2025-03-20T15:17:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    17,
                    5,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:17:05Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    17,
                    5,
                    3,
                    79,
                    0
                ],
                "title": "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems"
                },
                "summary": "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Teresa Klatzer"
                    },
                    {
                        "name": "Savvas Melidonis"
                    },
                    {
                        "name": "Marcelo Pereyra"
                    },
                    {
                        "name": "Konstantinos C. Zygalakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos C. Zygalakis"
                },
                "author": "Konstantinos C. Zygalakis",
                "arxiv_comment": "31 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53B21, 60H35, 62F15, 65C40, 65C60, 65J22, 68U10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19109v2",
                "updated": "2025-03-20T15:14:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    14,
                    30,
                    3,
                    79,
                    0
                ],
                "published": "2024-06-27T11:40:13Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    11,
                    40,
                    13,
                    3,
                    179,
                    0
                ],
                "title": "Efficient approximations of transcriptional bursting effects on the\n  dynamics of a gene regulatory network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient approximations of transcriptional bursting effects on the\n  dynamics of a gene regulatory network"
                },
                "summary": "Mathematical models of gene regulatory networks are widely used to study cell\nfate changes and transcriptional regulation. When designing such models, it is\nimportant to accurately account for sources of stochasticity. However, doing so\ncan be computationally expensive and analytically untractable, posing limits on\nthe extent of our explorations and on parameter inference. Here, we explore\nthis challenge using the example of a simple auto-negative feedback motif, in\nwhich we incorporate stochastic variation due to transcriptional bursting and\nnoise from finite copy numbers. We find that transcriptional bursting may\nchange the qualitative dynamics of the system by inducing oscillations when\nthey would not otherwise be present, or by magnifying existing oscillations. We\ndescribe multiple levels of approximation for the model in the form of\ndifferential equations, piecewise deterministic processes, and stochastic\ndifferential equations. Importantly, we derive how the classical chemical\nLangevin equation can be extended to include a noise term representing\ntranscriptional bursting. This approximation drastically decreases computation\ntimes and allows us to analytically calculate properties of the dynamics, such\nas their power spectrum. We explore when these approximations break down and\nprovide recommendations for their use. Our analysis illustrates the importance\nof accounting for transcriptional bursting when simulating gene regulatory\nnetwork dynamics and provides recommendations to do so with computationally\nefficient methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical models of gene regulatory networks are widely used to study cell\nfate changes and transcriptional regulation. When designing such models, it is\nimportant to accurately account for sources of stochasticity. However, doing so\ncan be computationally expensive and analytically untractable, posing limits on\nthe extent of our explorations and on parameter inference. Here, we explore\nthis challenge using the example of a simple auto-negative feedback motif, in\nwhich we incorporate stochastic variation due to transcriptional bursting and\nnoise from finite copy numbers. We find that transcriptional bursting may\nchange the qualitative dynamics of the system by inducing oscillations when\nthey would not otherwise be present, or by magnifying existing oscillations. We\ndescribe multiple levels of approximation for the model in the form of\ndifferential equations, piecewise deterministic processes, and stochastic\ndifferential equations. Importantly, we derive how the classical chemical\nLangevin equation can be extended to include a noise term representing\ntranscriptional bursting. This approximation drastically decreases computation\ntimes and allows us to analytically calculate properties of the dynamics, such\nas their power spectrum. We explore when these approximations break down and\nprovide recommendations for their use. Our analysis illustrates the importance\nof accounting for transcriptional bursting when simulating gene regulatory\nnetwork dynamics and provides recommendations to do so with computationally\nefficient methods."
                },
                "authors": [
                    {
                        "name": "Jochen Kursawe"
                    },
                    {
                        "name": "Antoine Moneyron"
                    },
                    {
                        "name": "Tobias Galla"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Galla"
                },
                "author": "Tobias Galla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16219v1",
                "updated": "2025-03-20T15:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    13,
                    23,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    13,
                    23,
                    3,
                    79,
                    0
                ],
                "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't"
                },
                "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs."
                },
                "authors": [
                    {
                        "name": "Quy-Anh Dang"
                    },
                    {
                        "name": "Chris Ngo"
                    }
                ],
                "author_detail": {
                    "name": "Chris Ngo"
                },
                "author": "Chris Ngo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v2",
                "updated": "2025-03-20T15:09:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    9,
                    51,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07058v3",
                "updated": "2025-03-20T15:01:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    1,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-10T21:49:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    49,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties"
                },
                "summary": "A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin."
                },
                "authors": [
                    {
                        "name": "Zixin Tang"
                    },
                    {
                        "name": "Chieh-Yang Huang"
                    },
                    {
                        "name": "Tsung-Che Li"
                    },
                    {
                        "name": "Ho Yin Sam Ng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                "author": "Ting-Hao 'Kenneth' Huang",
                "arxiv_comment": "Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16212v1",
                "updated": "2025-03-20T15:00:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:00:41Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion"
                },
                "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion."
                },
                "authors": [
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Chenlin Ming"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09487v2",
                "updated": "2025-03-20T14:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    58,
                    40,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-12T15:46:12Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    46,
                    12,
                    2,
                    71,
                    0
                ],
                "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness"
                },
                "summary": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
                },
                "authors": [
                    {
                        "name": "Beier Zhu"
                    },
                    {
                        "name": "Jiequan Cui"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15419v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15419v5",
                "updated": "2025-03-20T14:54:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    54,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2024-08-27T21:50:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    50,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "Bayesian Inference General Procedures for A Single-subject Test Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference General Procedures for A Single-subject Test Study"
                },
                "summary": "Abnormality detection in identifying a single-subject which deviates from the\nmajority of a control group dataset is a fundamental problem. Typically, the\ncontrol group is characterised using standard Normal statistics, and the\ndetection of a single abnormal subject is in that context. However, in many\nsituations, the control group cannot be described by Normal statistics, making\nstandard statistical methods inappropriate. This paper presents a Bayesian\nInference General Procedures for A Single-subject Test (BIGPAST) designed to\nmitigate the effects of skewness under the assumption that the dataset of the\ncontrol group comes from the skewed Student \\( t \\) distribution. BIGPAST\noperates under the null hypothesis that the single-subject follows the same\ndistribution as the control group. We assess BIGPAST's performance against\nother methods through simulation studies. The results demonstrate that BIGPAST\nis robust against deviations from normality and outperforms the existing\napproaches in accuracy nearest to the nominal accuracy 0.95.\n  BIGPAST can reduce model misspecification errors under the skewed Student\n  $t$ assumption by up to 12 times, as demonstrated in Section 3.3. We\n  apply BIGPAST to a Magnetoencephalography (MEG) dataset consisting of an\n  individual with mild traumatic brain injury and an age and gender-matched\n  control group. For example, the previous method failed to detect\nabnormalities\n  in 8 brain areas, whereas BIGPAST successfully identified them, demonstrating\n  its effectiveness in detecting abnormalities in a single-subject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abnormality detection in identifying a single-subject which deviates from the\nmajority of a control group dataset is a fundamental problem. Typically, the\ncontrol group is characterised using standard Normal statistics, and the\ndetection of a single abnormal subject is in that context. However, in many\nsituations, the control group cannot be described by Normal statistics, making\nstandard statistical methods inappropriate. This paper presents a Bayesian\nInference General Procedures for A Single-subject Test (BIGPAST) designed to\nmitigate the effects of skewness under the assumption that the dataset of the\ncontrol group comes from the skewed Student \\( t \\) distribution. BIGPAST\noperates under the null hypothesis that the single-subject follows the same\ndistribution as the control group. We assess BIGPAST's performance against\nother methods through simulation studies. The results demonstrate that BIGPAST\nis robust against deviations from normality and outperforms the existing\napproaches in accuracy nearest to the nominal accuracy 0.95.\n  BIGPAST can reduce model misspecification errors under the skewed Student\n  $t$ assumption by up to 12 times, as demonstrated in Section 3.3. We\n  apply BIGPAST to a Magnetoencephalography (MEG) dataset consisting of an\n  individual with mild traumatic brain injury and an age and gender-matched\n  control group. For example, the previous method failed to detect\nabnormalities\n  in 8 brain areas, whereas BIGPAST successfully identified them, demonstrating\n  its effectiveness in detecting abnormalities in a single-subject."
                },
                "authors": [
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Gary Green"
                    },
                    {
                        "name": "Sarah J. A. Carr"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_doi": "10.1016/j.neuri.2025.100195",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neuri.2025.100195",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15419v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15419v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "37 pages, 13 figures and 11 tables",
                "arxiv_journal_ref": "Neuroscience Informatics Volume 5, Issue 2, June 2025, 100195\n  Neuroscience Informatics, Volume 5, Issue 2",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19146v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19146v4",
                "updated": "2025-03-20T14:50:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    50,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-28T13:45:42Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    45,
                    42,
                    3,
                    333,
                    0
                ],
                "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"
                },
                "summary": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ran Rubin"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19146v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19146v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16730v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16730v4",
                "updated": "2025-03-20T14:48:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    48,
                    10,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-23T09:32:44Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    9,
                    32,
                    44,
                    5,
                    328,
                    0
                ],
                "title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks"
                },
                "summary": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16730v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16730v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.14203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.14203v2",
                "updated": "2025-03-20T14:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    41,
                    40,
                    3,
                    79,
                    0
                ],
                "published": "2023-07-26T14:02:24Z",
                "published_parsed": [
                    2023,
                    7,
                    26,
                    14,
                    2,
                    24,
                    2,
                    207,
                    0
                ],
                "title": "Dynamic Regression Discontinuity: An Event-Study Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Regression Discontinuity: An Event-Study Approach"
                },
                "summary": "I propose a novel argument to identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nreformulate two assumptions from the difference-in-differences literature, no\nanticipation and common trends, to attain point identification of\ncutoff-specific impulse responses. The estimand of each target parameter can be\nexpressed as the sum of two static RDD contrasts, thereby allowing for\nnonparametric estimation and inference with standard local polynomial methods.\nI also propose a nonparametric approach to aggregate treatment effects across\ncalendar time and treatment paths, leveraging a limited path independence\nrestriction to reduce the dimensionality of the parameter space. I apply this\napproach to estimate the dynamic effects of school district expenditure\nauthorizations on housing prices in Wisconsin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I propose a novel argument to identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nreformulate two assumptions from the difference-in-differences literature, no\nanticipation and common trends, to attain point identification of\ncutoff-specific impulse responses. The estimand of each target parameter can be\nexpressed as the sum of two static RDD contrasts, thereby allowing for\nnonparametric estimation and inference with standard local polynomial methods.\nI also propose a nonparametric approach to aggregate treatment effects across\ncalendar time and treatment paths, leveraging a limited path independence\nrestriction to reduce the dimensionality of the parameter space. I apply this\napproach to estimate the dynamic effects of school district expenditure\nauthorizations on housing prices in Wisconsin."
                },
                "authors": [
                    {
                        "name": "Francesco Ruggieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Ruggieri"
                },
                "author": "Francesco Ruggieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.14203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.14203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16194v1",
                "updated": "2025-03-20T14:41:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    41,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:41:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    41,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction"
                },
                "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds."
                },
                "authors": [
                    {
                        "name": "Ziyao Guo"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16193v1",
                "updated": "2025-03-20T14:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "Affective Polarization Amongst Swedish Politicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Polarization Amongst Swedish Politicians"
                },
                "summary": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research."
                },
                "authors": [
                    {
                        "name": "François t'Serstevens"
                    },
                    {
                        "name": "Roberto Cerina"
                    },
                    {
                        "name": "Gustav Pepper"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Pepper"
                },
                "author": "Gustav Pepper",
                "arxiv_comment": "5 figures, 4 tables, 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16191v1",
                "updated": "2025-03-20T14:39:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    39,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:39:11Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    39,
                    11,
                    3,
                    79,
                    0
                ],
                "title": "Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making"
                },
                "summary": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management."
                },
                "authors": [
                    {
                        "name": "Yinon Goldshtein"
                    },
                    {
                        "name": "Gal Perelman"
                    },
                    {
                        "name": "Assaf Schuster"
                    },
                    {
                        "name": "Avi Ostfeld"
                    }
                ],
                "author_detail": {
                    "name": "Avi Ostfeld"
                },
                "author": "Avi Ostfeld",
                "arxiv_comment": "Accepted to EWRI Congress 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16188v1",
                "updated": "2025-03-20T14:37:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    37,
                    45,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:37:45Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    37,
                    45,
                    3,
                    79,
                    0
                ],
                "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning"
                },
                "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Jike Zhong"
                    },
                    {
                        "name": "Yuxiang Lai"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16183v1",
                "updated": "2025-03-20T14:34:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    34,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:34:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    34,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog\n  Computations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog\n  Computations"
                },
                "summary": "The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 72.3\\% with conventional\nNoisy Training to 97.3\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 38.5\\% to 89.9\\% on Tiny ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 72.3\\% with conventional\nNoisy Training to 97.3\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 38.5\\% to 89.9\\% on Tiny ImageNet."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Hendrik Borras"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    }
                ],
                "author_detail": {
                    "name": "Holger Fröning"
                },
                "author": "Holger Fröning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10361v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10361v5",
                "updated": "2025-03-20T14:27:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    27,
                    22,
                    3,
                    79,
                    0
                ],
                "published": "2023-05-17T16:38:11Z",
                "published_parsed": [
                    2023,
                    5,
                    17,
                    16,
                    38,
                    11,
                    2,
                    137,
                    0
                ],
                "title": "Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps://github.com/eilamshapira/HumanChoicePrediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps://github.com/eilamshapira/HumanChoicePrediction"
                },
                "authors": [
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Omer Madmon"
                    },
                    {
                        "name": "Reut Apel"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.10361v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10361v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00267v3",
                "updated": "2025-03-20T14:23:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    23,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2023-12-01T00:54:02Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    0,
                    54,
                    2,
                    4,
                    335,
                    0
                ],
                "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Efficient Preference Alignment in LLMs via Active Exploration"
                },
                "summary": "Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature."
                },
                "authors": [
                    {
                        "name": "Viraj Mehta"
                    },
                    {
                        "name": "Syrine Belakaria"
                    },
                    {
                        "name": "Vikramjeet Das"
                    },
                    {
                        "name": "Ojash Neopane"
                    },
                    {
                        "name": "Yijia Dai"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Barbara Engelhardt"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Willie Neiswanger"
                    }
                ],
                "author_detail": {
                    "name": "Willie Neiswanger"
                },
                "author": "Willie Neiswanger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11819v2",
                "updated": "2025-03-20T14:19:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    19,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-15T17:46:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "Mind the memory: Consistent time reversal removes artefactual scaling of\n  energy dissipation rate and provides more accurate and reliable thermodynamic\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the memory: Consistent time reversal removes artefactual scaling of\n  energy dissipation rate and provides more accurate and reliable thermodynamic\n  inference"
                },
                "summary": "It has been proposed that an observed inverse power-law dependence of the\nMarkovian estimate for the steady-state dissipation rate on the coarse-graining\nscale in self-similar networks reflects a scale-dependent energy dissipation.\nBy explicit examples, it is demonstrated here that there are in general no\nrelations between such an apparent power-law dependence and the actual\ndissipation on different length scales. We construct fractal networks with a\nsingle dissipative scale and networks with a true inverse energy-dissipation\ncascade, and show that they display the same scaling behavior. Moreover, we\nshow that a self-similar network structure does not imply an inverse power-law\nscaling but may be mistaken for one in practice. When no dissipative cycles\nbecome hidden by the coarse graining, any scale dependence of the dissipation\nestimate vanishes if the memory is correctly accounted for in the time-reversal\noperation. A $k$-th order estimator is derived and necessary and sufficient\nconditions are proved for a guaranteed lower bound on dissipation. These\nhigher-order estimators saturated in the order are proved to provide sharper\nlower bounds on dissipation and their scale dependence signifies hidden\ndissipative cycles. It is shown that estimators not saturated in the order may\nerroneously overestimate the microscopic dissipation. Our results underscore\nthe still underappreciated importance of correctly accounting for memory in\nanalyzing coarse observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been proposed that an observed inverse power-law dependence of the\nMarkovian estimate for the steady-state dissipation rate on the coarse-graining\nscale in self-similar networks reflects a scale-dependent energy dissipation.\nBy explicit examples, it is demonstrated here that there are in general no\nrelations between such an apparent power-law dependence and the actual\ndissipation on different length scales. We construct fractal networks with a\nsingle dissipative scale and networks with a true inverse energy-dissipation\ncascade, and show that they display the same scaling behavior. Moreover, we\nshow that a self-similar network structure does not imply an inverse power-law\nscaling but may be mistaken for one in practice. When no dissipative cycles\nbecome hidden by the coarse graining, any scale dependence of the dissipation\nestimate vanishes if the memory is correctly accounted for in the time-reversal\noperation. A $k$-th order estimator is derived and necessary and sufficient\nconditions are proved for a guaranteed lower bound on dissipation. These\nhigher-order estimators saturated in the order are proved to provide sharper\nlower bounds on dissipation and their scale dependence signifies hidden\ndissipative cycles. It is shown that estimators not saturated in the order may\nerroneously overestimate the microscopic dissipation. Our results underscore\nthe still underappreciated importance of correctly accounting for memory in\nanalyzing coarse observations."
                },
                "authors": [
                    {
                        "name": "Tassilo Schwarz"
                    },
                    {
                        "name": "Anatoly B. Kolomeisky"
                    },
                    {
                        "name": "Aljaž Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaž Godec"
                },
                "author": "Aljaž Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16167v1",
                "updated": "2025-03-20T14:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models"
                },
                "summary": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results."
                },
                "authors": [
                    {
                        "name": "Hong Yi Lin"
                    },
                    {
                        "name": "Chunhua Liu"
                    },
                    {
                        "name": "Haoyu Gao"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    },
                    {
                        "name": "Christoph Treude"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Treude"
                },
                "author": "Christoph Treude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16161v1",
                "updated": "2025-03-20T13:58:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    58,
                    32,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:58:32Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    58,
                    32,
                    3,
                    79,
                    0
                ],
                "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation"
                },
                "summary": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
                },
                "authors": [
                    {
                        "name": "Alex-Razvan Ispas"
                    },
                    {
                        "name": "Charles-Elie Simon"
                    },
                    {
                        "name": "Fabien Caspani"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "arxiv_comment": "17 pages, 5 figures, published at 1st workshop of Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI at ICLR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16158v1",
                "updated": "2025-03-20T13:56:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    56,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:56:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    56,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems"
                },
                "summary": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research."
                },
                "authors": [
                    {
                        "name": "Shenbin Qian"
                    },
                    {
                        "name": "Constantin Orăsan"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Félix do Carmo"
                    }
                ],
                "author_detail": {
                    "name": "Félix do Carmo"
                },
                "author": "Félix do Carmo",
                "arxiv_comment": "Accepted to the 10th Workshop on Noisy and User-generated Text at\n  NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16146v1",
                "updated": "2025-03-20T13:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:49:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "Distributed Split Computing Using Diffusive Metrics for UAV Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Split Computing Using Diffusive Metrics for UAV Swarms"
                },
                "summary": "In large-scale UAV swarms, dynamically executing machine learning tasks can\npose significant challenges due to network volatility and the heterogeneous\nresource constraints of each UAV. Traditional approaches often rely on\ncentralized orchestration to partition tasks among nodes. However, these\nmethods struggle with communication bottlenecks, latency, and reliability when\nthe swarm grows or the topology shifts rapidly. To overcome these limitations,\nwe propose a fully distributed, diffusive metric-based approach for split\ncomputing in UAV swarms. Our solution introduces a new iterative measure,\ntermed the aggregated gigaflops, capturing each node's own computing capacity\nalong with that of its neighbors without requiring global network knowledge. By\nforwarding partial inferences intelligently to underutilized nodes, we achieve\nimproved task throughput, lower latency, and enhanced energy efficiency.\nFurther, to handle sudden workload surges and rapidly changing node conditions,\nwe incorporate an early-exit mechanism that can adapt the inference pathway\non-the-fly. Extensive simulations demonstrate that our approach significantly\noutperforms baseline strategies across multiple performance indices, including\nlatency, fairness, and energy consumption. These results highlight the\nfeasibility of large-scale distributed intelligence in UAV swarms and provide a\nblueprint for deploying robust, scalable ML services in diverse aerial\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale UAV swarms, dynamically executing machine learning tasks can\npose significant challenges due to network volatility and the heterogeneous\nresource constraints of each UAV. Traditional approaches often rely on\ncentralized orchestration to partition tasks among nodes. However, these\nmethods struggle with communication bottlenecks, latency, and reliability when\nthe swarm grows or the topology shifts rapidly. To overcome these limitations,\nwe propose a fully distributed, diffusive metric-based approach for split\ncomputing in UAV swarms. Our solution introduces a new iterative measure,\ntermed the aggregated gigaflops, capturing each node's own computing capacity\nalong with that of its neighbors without requiring global network knowledge. By\nforwarding partial inferences intelligently to underutilized nodes, we achieve\nimproved task throughput, lower latency, and enhanced energy efficiency.\nFurther, to handle sudden workload surges and rapidly changing node conditions,\nwe incorporate an early-exit mechanism that can adapt the inference pathway\non-the-fly. Extensive simulations demonstrate that our approach significantly\noutperforms baseline strategies across multiple performance indices, including\nlatency, fairness, and energy consumption. These results highlight the\nfeasibility of large-scale distributed intelligence in UAV swarms and provide a\nblueprint for deploying robust, scalable ML services in diverse aerial\nnetworks."
                },
                "authors": [
                    {
                        "name": "Talip Tolga Sarı"
                    },
                    {
                        "name": "Gökhan Seçinti"
                    },
                    {
                        "name": "Angelo Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Trotta"
                },
                "author": "Angelo Trotta",
                "arxiv_comment": "This work has been submitted to a IEEE journal for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16144v1",
                "updated": "2025-03-20T13:47:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    47,
                    6,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:47:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    47,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of\n  Unit Tests with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of\n  Unit Tests with LLMs"
                },
                "summary": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score."
                },
                "authors": [
                    {
                        "name": "Djamel Eddine Khelladi"
                    },
                    {
                        "name": "Charly Reux"
                    },
                    {
                        "name": "Mathieu Acher"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Acher"
                },
                "author": "Mathieu Acher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06759v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06759v4",
                "updated": "2025-03-20T13:46:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    46,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-10T18:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    57,
                    0,
                    41,
                    0
                ],
                "title": "Rationalization Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationalization Models for Text-to-SQL"
                },
                "summary": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability."
                },
                "authors": [
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Nhan Pham"
                    },
                    {
                        "name": "Michael Glass"
                    },
                    {
                        "name": "Junkyu Lee"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    }
                ],
                "author_detail": {
                    "name": "Dharmashankar Subramanian"
                },
                "author": "Dharmashankar Subramanian",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06759v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06759v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03029v2",
                "updated": "2025-03-20T13:43:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    43,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2024-03-05T15:05:06Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    15,
                    5,
                    6,
                    1,
                    65,
                    0
                ],
                "title": "Socratic Reasoning Improves Positive Text Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic Reasoning Improves Positive Text Rewriting"
                },
                "summary": "Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts."
                },
                "authors": [
                    {
                        "name": "Anmol Goel"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Christian Montag"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v1",
                "updated": "2025-03-20T13:25:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08144v2",
                "updated": "2025-03-20T13:21:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    21,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-11T08:02:54Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    2,
                    54,
                    1,
                    70,
                    0
                ],
                "title": "Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method"
                },
                "summary": "Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Chengcheng Chen"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Yugang Chang"
                    },
                    {
                        "name": "Weiming Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zeng"
                },
                "author": "Weiming Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00371v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00371v3",
                "updated": "2025-03-20T13:20:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    20,
                    9,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-01T06:56:58Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    6,
                    56,
                    58,
                    5,
                    60,
                    0
                ],
                "title": "Jointly Understand Your Command and Intention:Reciprocal Co-Evolution\n  between Scene-Aware 3D Human Motion Synthesis and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jointly Understand Your Command and Intention:Reciprocal Co-Evolution\n  between Scene-Aware 3D Human Motion Synthesis and Analysis"
                },
                "summary": "As two intimate reciprocal tasks, scene-aware human motion synthesis and\nanalysis require a joint understanding between multiple modalities, including\n3D body motions, 3D scenes, and textual descriptions. In this paper, we\nintegrate these two paired processes into a Co-Evolving Synthesis-Analysis\n(CESA) pipeline and mutually benefit their learning. Specifically, scene-aware\ntext-to-human synthesis generates diverse indoor motion samples from the same\ntextual description to enrich human-scene interaction intra-class diversity,\nthus significantly benefiting training a robust human motion analysis system.\nReciprocally, human motion analysis would enforce semantic scrutiny on each\nsynthesized motion sample to ensure its semantic consistency with the given\ntextual description, thus improving realistic motion synthesis. Considering\nthat real-world indoor human motions are goal-oriented and path-guided, we\npropose a cascaded generation strategy that factorizes text-driven\nscene-specific human motion generation into three stages: goal inferring, path\nplanning, and pose synthesizing. Coupling CESA with this powerful cascaded\nmotion synthesis model, we jointly improve realistic human motion synthesis and\nrobust human motion analysis in 3D scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As two intimate reciprocal tasks, scene-aware human motion synthesis and\nanalysis require a joint understanding between multiple modalities, including\n3D body motions, 3D scenes, and textual descriptions. In this paper, we\nintegrate these two paired processes into a Co-Evolving Synthesis-Analysis\n(CESA) pipeline and mutually benefit their learning. Specifically, scene-aware\ntext-to-human synthesis generates diverse indoor motion samples from the same\ntextual description to enrich human-scene interaction intra-class diversity,\nthus significantly benefiting training a robust human motion analysis system.\nReciprocally, human motion analysis would enforce semantic scrutiny on each\nsynthesized motion sample to ensure its semantic consistency with the given\ntextual description, thus improving realistic motion synthesis. Considering\nthat real-world indoor human motions are goal-oriented and path-guided, we\npropose a cascaded generation strategy that factorizes text-driven\nscene-specific human motion generation into three stages: goal inferring, path\nplanning, and pose synthesizing. Coupling CESA with this powerful cascaded\nmotion synthesis model, we jointly improve realistic human motion synthesis and\nrobust human motion analysis in 3D scenes."
                },
                "authors": [
                    {
                        "name": "Xuehao Gao"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Shaoyi Du"
                    },
                    {
                        "name": "Guo-Jun Qi"
                    },
                    {
                        "name": "Junwei Han"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Han"
                },
                "author": "Junwei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00371v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00371v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04174v3",
                "updated": "2025-03-20T13:08:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    8,
                    9,
                    3,
                    79,
                    0
                ],
                "published": "2024-01-08T19:00:05Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    19,
                    0,
                    5,
                    0,
                    8,
                    0
                ],
                "title": "Optimal, fast, and robust inference of reionization-era cosmology with\n  the 21cmPIE-INN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal, fast, and robust inference of reionization-era cosmology with\n  the 21cmPIE-INN"
                },
                "summary": "Modern machine learning will allow for simulation-based inference from\nreionization-era 21cm observations at the Square Kilometre Array. Our framework\ncombines a convolutional summary network and a conditional invertible network\nthrough a physics-inspired latent representation. It allows for an efficient\nand extremely fast determination of the posteriors of astrophysical and\ncosmological parameters, jointly with well-calibrated and on average unbiased\nsummaries. The sensitivity to non-Gaussian information makes our method a\npromising alternative to the established power spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning will allow for simulation-based inference from\nreionization-era 21cm observations at the Square Kilometre Array. Our framework\ncombines a convolutional summary network and a conditional invertible network\nthrough a physics-inspired latent representation. It allows for an efficient\nand extremely fast determination of the posteriors of astrophysical and\ncosmological parameters, jointly with well-calibrated and on average unbiased\nsummaries. The sensitivity to non-Gaussian information makes our method a\npromising alternative to the established power spectra."
                },
                "authors": [
                    {
                        "name": "Benedikt Schosser"
                    },
                    {
                        "name": "Caroline Heneka"
                    },
                    {
                        "name": "Tilman Plehn"
                    }
                ],
                "author_detail": {
                    "name": "Tilman Plehn"
                },
                "author": "Tilman Plehn",
                "arxiv_comment": "15+10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16118v1",
                "updated": "2025-03-20T13:05:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    5,
                    12,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:05:12Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    5,
                    12,
                    3,
                    79,
                    0
                ],
                "title": "Forecasting Extreme Temperatures in Siberia Using Supervised Learning\n  and Conformal Prediction Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Extreme Temperatures in Siberia Using Supervised Learning\n  and Conformal Prediction Regions"
                },
                "summary": "In this paper, we step back from a variety of competing heat wave definitions\nand forecast directly unusually high temperatures. Our testbed is the Russian\nFar East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua\nspacecraft are organized into a within-subject design that can reduce nuisance\nvariation in forecasted temperatures. Spatial grid cells are the study units.\nEach is exposed to precursors of a faux heat wave in 2022 and to precursors of\na reported heat wave in 2023. The precursors are used to forecast temperatures\ntwo weeks in the future for each of 31 consecutive days. Algorithmic fitting\nprocedures produce forecasts with promise and relatively small conformal\nprediction regions having a coverage probability of at least .75. Spatial and\ntemporal dependence are manageable. At worst, there is weak dependence such\nthat conformal prediction inference is only asymptotically valid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we step back from a variety of competing heat wave definitions\nand forecast directly unusually high temperatures. Our testbed is the Russian\nFar East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua\nspacecraft are organized into a within-subject design that can reduce nuisance\nvariation in forecasted temperatures. Spatial grid cells are the study units.\nEach is exposed to precursors of a faux heat wave in 2022 and to precursors of\na reported heat wave in 2023. The precursors are used to forecast temperatures\ntwo weeks in the future for each of 31 consecutive days. Algorithmic fitting\nprocedures produce forecasts with promise and relatively small conformal\nprediction regions having a coverage probability of at least .75. Spatial and\ntemporal dependence are manageable. At worst, there is weak dependence such\nthat conformal prediction inference is only asymptotically valid."
                },
                "authors": [
                    {
                        "name": "Richard A. Berk"
                    },
                    {
                        "name": "Amy Braverman"
                    }
                ],
                "author_detail": {
                    "name": "Amy Braverman"
                },
                "author": "Amy Braverman",
                "arxiv_comment": "8 figures, 28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00895v2",
                "updated": "2025-03-20T13:03:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    3,
                    26,
                    3,
                    79,
                    0
                ],
                "published": "2025-01-01T16:56:43Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    56,
                    43,
                    2,
                    1,
                    0
                ],
                "title": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a\n  Global-Scale Dataset and a Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a\n  Global-Scale Dataset and a Foundation Model"
                },
                "summary": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10.5 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is https://chen-yang-liu.github.io/Text2Earth",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10.5 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is https://chen-yang-liu.github.io/Text2Earth"
                },
                "authors": [
                    {
                        "name": "Chenyang Liu"
                    },
                    {
                        "name": "Keyan Chen"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Zhenwei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zhenwei Shi"
                },
                "author": "Zhenwei Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16114v1",
                "updated": "2025-03-20T13:00:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "The Impact of Revealing Large Language Model Stochasticity on Trust,\n  Reliability, and Anthropomorphization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Revealing Large Language Model Stochasticity on Trust,\n  Reliability, and Anthropomorphization"
                },
                "summary": "Interfaces for interacting with large language models (LLMs) are often\ndesigned to mimic human conversations, typically presenting a single response\nto user queries. This design choice can obscure the probabilistic and\npredictive nature of these models, potentially fostering undue trust and\nover-anthropomorphization of the underlying model. In this paper, we\ninvestigate (i) the effect of displaying multiple responses simultaneously as a\ncountermeasure to these issues, and (ii) how a cognitive support\nmechanism-highlighting structural and semantic similarities across\nresponses-helps users deal with the increased cognitive load of that\nintervention. We conducted a within-subjects study in which participants\ninspected responses generated by an LLM under three conditions: one response,\nten responses with cognitive support, and ten responses without cognitive\nsupport. Participants then answered questions about workload, trust and\nreliance, and anthropomorphization. We conclude by reporting the results of\nthese studies and discussing future work and design opportunities for future\nLLM interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interfaces for interacting with large language models (LLMs) are often\ndesigned to mimic human conversations, typically presenting a single response\nto user queries. This design choice can obscure the probabilistic and\npredictive nature of these models, potentially fostering undue trust and\nover-anthropomorphization of the underlying model. In this paper, we\ninvestigate (i) the effect of displaying multiple responses simultaneously as a\ncountermeasure to these issues, and (ii) how a cognitive support\nmechanism-highlighting structural and semantic similarities across\nresponses-helps users deal with the increased cognitive load of that\nintervention. We conducted a within-subjects study in which participants\ninspected responses generated by an LLM under three conditions: one response,\nten responses with cognitive support, and ten responses without cognitive\nsupport. Participants then answered questions about workload, trust and\nreliance, and anthropomorphization. We conclude by reporting the results of\nthese studies and discussing future work and design opportunities for future\nLLM interfaces."
                },
                "authors": [
                    {
                        "name": "Chelse Swoopes"
                    },
                    {
                        "name": "Tyler Holloway"
                    },
                    {
                        "name": "Elena L. Glassman"
                    }
                ],
                "author_detail": {
                    "name": "Elena L. Glassman"
                },
                "author": "Elena L. Glassman",
                "arxiv_comment": "Accepted and presented at Trust and Reliance in Evolving Human-AI\n  Workflows (TREW) Workshop, CHI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19761v2",
                "updated": "2025-03-20T12:53:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    53,
                    47,
                    3,
                    79,
                    0
                ],
                "published": "2024-06-28T09:04:01Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    9,
                    4,
                    1,
                    4,
                    180,
                    0
                ],
                "title": "A non-orthogonal representation of the chemical space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A non-orthogonal representation of the chemical space"
                },
                "summary": "We present a novel approach to generate a fingerprint for crystalline\nmaterials that balances efficiency for machine processing and human\ninterpretability, allowing its application in both machine learning inference\nand understanding of structure-property relationships. Our proposed material\nencoding has two components: one representing the crystal structure and the\nother characterizing the chemical composition, that we call Pettifor embedding.\nFor the latter we construct a non-orthogonal space where each axis represents a\nchemical element and where the angle between the axes quantifies a measure of\nthe similarity between them. The chemical composition is then defined by the\npoint on the unit sphere in this non-orthogonal space. We show that the\nPettifor embeddings systematically outperform other commonly used elemental\nembeddings in compositional machine learning models. Using the Pettifor\nembeddings to define a distance metric and applying dimension reduction\ntechniques, we construct a two-dimensional global map of the space of\nthermodynamically stable crystalline compounds. Despite their simplicity, such\nmaps succeed in providing a physical separation of material classes according\nto basic physical properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to generate a fingerprint for crystalline\nmaterials that balances efficiency for machine processing and human\ninterpretability, allowing its application in both machine learning inference\nand understanding of structure-property relationships. Our proposed material\nencoding has two components: one representing the crystal structure and the\nother characterizing the chemical composition, that we call Pettifor embedding.\nFor the latter we construct a non-orthogonal space where each axis represents a\nchemical element and where the angle between the axes quantifies a measure of\nthe similarity between them. The chemical composition is then defined by the\npoint on the unit sphere in this non-orthogonal space. We show that the\nPettifor embeddings systematically outperform other commonly used elemental\nembeddings in compositional machine learning models. Using the Pettifor\nembeddings to define a distance metric and applying dimension reduction\ntechniques, we construct a two-dimensional global map of the space of\nthermodynamically stable crystalline compounds. Despite their simplicity, such\nmaps succeed in providing a physical separation of material classes according\nto basic physical properties."
                },
                "authors": [
                    {
                        "name": "Tiago F. T. Cerqueira"
                    },
                    {
                        "name": "Haichen Wang"
                    },
                    {
                        "name": "Silvana Botti"
                    },
                    {
                        "name": "Miguel A. L. Marques"
                    }
                ],
                "author_detail": {
                    "name": "Miguel A. L. Marques"
                },
                "author": "Miguel A. L. Marques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16106v1",
                "updated": "2025-03-20T12:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    51,
                    19,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    51,
                    19,
                    3,
                    79,
                    0
                ],
                "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain\n  Generalization in CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain\n  Generalization in CLIP"
                },
                "summary": "We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Mohamad Hassan N C"
                    },
                    {
                        "name": "Divyam Gupta"
                    },
                    {
                        "name": "Mainak Singha"
                    },
                    {
                        "name": "Sai Bhargav Rongali"
                    },
                    {
                        "name": "Ankit Jha"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Biplab Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Banerjee"
                },
                "author": "Biplab Banerjee",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16094v1",
                "updated": "2025-03-20T12:34:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    34,
                    1,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:34:01Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    34,
                    1,
                    3,
                    79,
                    0
                ],
                "title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning"
                },
                "summary": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances."
                },
                "authors": [
                    {
                        "name": "Reem I. Masoud"
                    },
                    {
                        "name": "Martin Ferianc"
                    },
                    {
                        "name": "Philip Treleaven"
                    },
                    {
                        "name": "Miguel Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rodrigues"
                },
                "author": "Miguel Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16075v1",
                "updated": "2025-03-20T12:12:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    12,
                    1,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:12:01Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    12,
                    1,
                    3,
                    79,
                    0
                ],
                "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step\n  Adversarial Network: Contribution to the FuseMyCells Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step\n  Adversarial Network: Contribution to the FuseMyCells Challenge"
                },
                "summary": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
                },
                "authors": [
                    {
                        "name": "Marek Wodzinski"
                    },
                    {
                        "name": "Henning Müller"
                    }
                ],
                "author_detail": {
                    "name": "Henning Müller"
                },
                "author": "Henning Müller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15112v2",
                "updated": "2025-03-20T12:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    11,
                    30,
                    3,
                    79,
                    0
                ],
                "published": "2024-09-23T15:20:07Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    20,
                    7,
                    0,
                    267,
                    0
                ],
                "title": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish"
                },
                "summary": "Evaluating the capabilities of Large Language Models (LLMs) to assist\nteachers and students in educational tasks is receiving increasing attention.\nIn this paper, we assess ChatGPT's capacities to solve and grade real\nprogramming exams, from an accredited BSc degree in Computer Science, written\nin Spanish. Our findings suggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in tackling complex problems or\nevaluating solutions authored by others are far from effective. As part of this\nresearch, we also release a new corpus of programming tasks and the\ncorresponding prompts for solving the problems or grading the solutions. This\nresource can be further exploited by other research teams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the capabilities of Large Language Models (LLMs) to assist\nteachers and students in educational tasks is receiving increasing attention.\nIn this paper, we assess ChatGPT's capacities to solve and grade real\nprogramming exams, from an accredited BSc degree in Computer Science, written\nin Spanish. Our findings suggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in tackling complex problems or\nevaluating solutions authored by others are far from effective. As part of this\nresearch, we also release a new corpus of programming tasks and the\ncorresponding prompts for solving the problems or grading the solutions. This\nresource can be further exploited by other research teams."
                },
                "authors": [
                    {
                        "name": "Pablo Saborido-Fernández"
                    },
                    {
                        "name": "Marcos Fernández-Pichel"
                    },
                    {
                        "name": "David E. Losada"
                    }
                ],
                "author_detail": {
                    "name": "David E. Losada"
                },
                "author": "David E. Losada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16073v1",
                "updated": "2025-03-20T12:09:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    9,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:09:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    9,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Quantum Chebyshev Probabilistic Models for Fragmentation Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Chebyshev Probabilistic Models for Fragmentation Functions"
                },
                "summary": "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics."
                },
                "authors": [
                    {
                        "name": "Jorge J. Martínez de Lejarza"
                    },
                    {
                        "name": "Hsin-Yu Wu"
                    },
                    {
                        "name": "Oleksandr Kyriienko"
                    },
                    {
                        "name": "Germán Rodrigo"
                    },
                    {
                        "name": "Michele Grossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Grossi"
                },
                "author": "Michele Grossi",
                "arxiv_comment": "5+11 pages, 4+7 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11624v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11624v4",
                "updated": "2025-03-20T12:06:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    6,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2024-06-17T15:07:55Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    7,
                    55,
                    0,
                    169,
                    0
                ],
                "title": "Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers"
                },
                "summary": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead."
                },
                "authors": [
                    {
                        "name": "Omer Sahin Tas"
                    },
                    {
                        "name": "Royden Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Royden Wagner"
                },
                "author": "Royden Wagner",
                "arxiv_comment": "ICLR 2025 camera-ready. Our implementation is available at\n  github.com/kit-mrt/future-motion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11624v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11624v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03146v2",
                "updated": "2025-03-20T12:04:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    4,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2024-06-05T11:01:42Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    11,
                    1,
                    42,
                    2,
                    157,
                    0
                ],
                "title": "Tiny models from tiny data: Textual and null-text inversion for few-shot\n  distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny models from tiny data: Textual and null-text inversion for few-shot\n  distillation"
                },
                "summary": "Few-shot learning deals with problems such as image classification using very\nfew training examples. Recent vision foundation models show excellent few-shot\ntransfer abilities, but are large and slow at inference. Using knowledge\ndistillation, the capabilities of high-performing but slow models can be\ntransferred to tiny, efficient models. However, common distillation methods\nrequire a large set of unlabeled data, which is not available in the few-shot\nsetting. To overcome this lack of data, there has been a recent interest in\nusing synthetic data. We expand on this line of research by presenting a novel\ndiffusion model inversion technique (TINT) combining the diversity of textual\ninversion with the specificity of null-text inversion. Using this method in a\nfew-shot distillation pipeline leads to state-of-the-art accuracy among small\nstudent models on popular benchmarks, while being significantly faster than\nprior work. Popular few-shot benchmarks involve evaluation over a large number\nof episodes, which is computationally cumbersome for methods involving\nsynthetic data generation. We also present a theoretical analysis on how the\naccuracy estimator variance depends on the number of episodes and query\nexamples, and use these results to lower the computational effort required for\nmethod evaluation. Finally, to further motivate the use of generative models in\nfew-shot distillation, we demonstrate that our method outperforms training on\nreal data mined from the dataset used in the original diffusion model training.\nSource code is available at https://github.com/pixwse/tiny2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot learning deals with problems such as image classification using very\nfew training examples. Recent vision foundation models show excellent few-shot\ntransfer abilities, but are large and slow at inference. Using knowledge\ndistillation, the capabilities of high-performing but slow models can be\ntransferred to tiny, efficient models. However, common distillation methods\nrequire a large set of unlabeled data, which is not available in the few-shot\nsetting. To overcome this lack of data, there has been a recent interest in\nusing synthetic data. We expand on this line of research by presenting a novel\ndiffusion model inversion technique (TINT) combining the diversity of textual\ninversion with the specificity of null-text inversion. Using this method in a\nfew-shot distillation pipeline leads to state-of-the-art accuracy among small\nstudent models on popular benchmarks, while being significantly faster than\nprior work. Popular few-shot benchmarks involve evaluation over a large number\nof episodes, which is computationally cumbersome for methods involving\nsynthetic data generation. We also present a theoretical analysis on how the\naccuracy estimator variance depends on the number of episodes and query\nexamples, and use these results to lower the computational effort required for\nmethod evaluation. Finally, to further motivate the use of generative models in\nfew-shot distillation, we demonstrate that our method outperforms training on\nreal data mined from the dataset used in the original diffusion model training.\nSource code is available at https://github.com/pixwse/tiny2."
                },
                "authors": [
                    {
                        "name": "Erik Landolsi"
                    },
                    {
                        "name": "Fredrik Kahl"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kahl"
                },
                "author": "Fredrik Kahl",
                "arxiv_comment": "24 pages (13 main pages + references and appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0; I.2.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16071v1",
                "updated": "2025-03-20T12:04:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    4,
                    40,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    4,
                    40,
                    3,
                    79,
                    0
                ],
                "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLMs by RAG Principles: Towards LLM-native Memory"
                },
                "summary": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types."
                },
                "authors": [
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Shuchi Wu"
                    },
                    {
                        "name": "Ruochen Liu"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Fangbo Tao"
                    }
                ],
                "author_detail": {
                    "name": "Fangbo Tao"
                },
                "author": "Fangbo Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16063v1",
                "updated": "2025-03-20T11:56:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    56,
                    14,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:56:14Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    56,
                    14,
                    3,
                    79,
                    0
                ],
                "title": "Two-stage Incomplete Utterance Rewriting on Editing Operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage Incomplete Utterance Rewriting on Editing Operation"
                },
                "summary": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly."
                },
                "authors": [
                    {
                        "name": "Zhiyu Cao"
                    },
                    {
                        "name": "Peifeng Li"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    },
                    {
                        "name": "Yaxin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yaxin Fan"
                },
                "author": "Yaxin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v3",
                "updated": "2025-03-20T11:55:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    55,
                    30,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16049v1",
                "updated": "2025-03-20T11:34:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    34,
                    13,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:34:13Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    34,
                    13,
                    3,
                    79,
                    0
                ],
                "title": "Federated Quantum-Train Long Short-Term Memory for Gravitational Wave\n  Signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Quantum-Train Long Short-Term Memory for Gravitational Wave\n  Signal"
                },
                "summary": "We present Federated QT-LSTM, a novel framework that combines the\nQuantum-Train (QT) methodology with Long Short-Term Memory (LSTM) networks in a\nfederated learning setup. By leveraging quantum neural networks (QNNs) to\ngenerate classical LSTM model parameters during training, the framework\neffectively addresses challenges in model compression, scalability, and\ncomputational efficiency. Importantly, Federated QT-LSTM eliminates the\nreliance on quantum devices during inference, making it practical for\nreal-world applications. Experiments on simulated gravitational wave (GW)\nsignal datasets demonstrate the framework's superior performance compared to\nbaseline models, including LSTM and QLSTM, achieving lower training and testing\nlosses while significantly reducing the number of trainable parameters. The\nresults also reveal that deeper QT layers enhance model expressiveness for\ncomplex tasks, highlighting the adaptability of the framework. Federated\nQT-LSTM provides a scalable and efficient solution for privacy-preserving\ndistributed learning, showcasing the potential of quantum-inspired techniques\nin advancing time-series prediction and signal reconstruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Federated QT-LSTM, a novel framework that combines the\nQuantum-Train (QT) methodology with Long Short-Term Memory (LSTM) networks in a\nfederated learning setup. By leveraging quantum neural networks (QNNs) to\ngenerate classical LSTM model parameters during training, the framework\neffectively addresses challenges in model compression, scalability, and\ncomputational efficiency. Importantly, Federated QT-LSTM eliminates the\nreliance on quantum devices during inference, making it practical for\nreal-world applications. Experiments on simulated gravitational wave (GW)\nsignal datasets demonstrate the framework's superior performance compared to\nbaseline models, including LSTM and QLSTM, achieving lower training and testing\nlosses while significantly reducing the number of trainable parameters. The\nresults also reveal that deeper QT layers enhance model expressiveness for\ncomplex tasks, highlighting the adaptability of the framework. Federated\nQT-LSTM provides a scalable and efficient solution for privacy-preserving\ndistributed learning, showcasing the potential of quantum-inspired techniques\nin advancing time-series prediction and signal reconstruction tasks."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Wei-Jia Huang"
                    },
                    {
                        "name": "Yen-Jui Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Jui Chang"
                },
                "author": "Yen-Jui Chang",
                "arxiv_comment": "Accepted at IEEE INFOCOM 2025 QuNAP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01478v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01478v5",
                "updated": "2025-03-20T11:28:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    28,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-03T12:37:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    37,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Lu Dai"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Jinhui Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01478v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01478v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16043v1",
                "updated": "2025-03-20T11:26:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    26,
                    46,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:26:46Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    26,
                    46,
                    3,
                    79,
                    0
                ],
                "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and\n  Utterance Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete Utterance Rewriting with Editing Operation Guidance and\n  Utterance Augmentation"
                },
                "summary": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR."
                },
                "authors": [
                    {
                        "name": "Zhiyu Cao"
                    },
                    {
                        "name": "Peifeng Li"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoming Zhu"
                },
                "author": "Qiaoming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01414v2",
                "updated": "2025-03-20T11:21:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    21,
                    7,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-03T02:47:03Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    2,
                    47,
                    3,
                    6,
                    308,
                    0
                ],
                "title": "A Deep Dive Into Large Language Model Code Generation Mistakes: What and\n  Why?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Dive Into Large Language Model Code Generation Mistakes: What and\n  Why?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nwidespread application in automated code generation. However, these models can\nstill generate defective code that deviates from the specification. Previous\nresearch has mainly focused on the mistakes in LLM-generated standalone\nfunctions, overlooking real-world software development situations where the\nsuccessful generation of the code requires software contexts such as external\ndependencies. In this paper, we considered both of these code generation\nsituations and identified a range of \\textit{non-syntactic mistakes} arising\nfrom LLMs' misunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through extensive manual\nanalyses, four of which were missed by previous works. To better understand\nthese mistakes, we proposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of LLMs in detecting\nmistakes and their reasons. Our evaluation demonstrated that GPT-4 with the\nReAct prompting technique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading function signatures.\nWe believe that these findings offer valuable insights into enhancing the\nquality of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nwidespread application in automated code generation. However, these models can\nstill generate defective code that deviates from the specification. Previous\nresearch has mainly focused on the mistakes in LLM-generated standalone\nfunctions, overlooking real-world software development situations where the\nsuccessful generation of the code requires software contexts such as external\ndependencies. In this paper, we considered both of these code generation\nsituations and identified a range of \\textit{non-syntactic mistakes} arising\nfrom LLMs' misunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through extensive manual\nanalyses, four of which were missed by previous works. To better understand\nthese mistakes, we proposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of LLMs in detecting\nmistakes and their reasons. Our evaluation demonstrated that GPT-4 with the\nReAct prompting technique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading function signatures.\nWe believe that these findings offer valuable insights into enhancing the\nquality of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "QiHong Chen"
                    },
                    {
                        "name": "Jiachen Yu"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Jiecheng Deng"
                    },
                    {
                        "name": "Justin Tian Jin Chen"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16041v1",
                "updated": "2025-03-20T11:19:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:19:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation"
                },
                "summary": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes"
                },
                "authors": [
                    {
                        "name": "Bisola Faith Kayode"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Egonna Anaesiuba-Bristol"
                    },
                    {
                        "name": "Onyekachukwu Ojumah"
                    },
                    {
                        "name": "Oluwagbade Odimayo"
                    },
                    {
                        "name": "Toyese Oloyede"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Teslim Kazeem"
                    },
                    {
                        "name": "Habeeb Alli"
                    },
                    {
                        "name": "Udodirim Ibem Offia"
                    },
                    {
                        "name": "Prisca Chinazor Amajuoyi"
                    }
                ],
                "author_detail": {
                    "name": "Prisca Chinazor Amajuoyi"
                },
                "author": "Prisca Chinazor Amajuoyi",
                "arxiv_comment": "12 Pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16040v1",
                "updated": "2025-03-20T11:14:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    14,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:14:39Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    14,
                    39,
                    3,
                    79,
                    0
                ],
                "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond"
                },
                "summary": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped."
                },
                "authors": [
                    {
                        "name": "Yaoyao Yu"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yinghao Hu"
                    },
                    {
                        "name": "Bin Wei"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16036v1",
                "updated": "2025-03-20T11:09:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:09:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models"
                },
                "summary": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom."
                },
                "authors": [
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Longxiang Tang"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Chuanbin Liu"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16034v1",
                "updated": "2025-03-20T11:00:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:00:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "Verification and External Parameter Inference for Stochastic World\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and External Parameter Inference for Stochastic World\n  Models"
                },
                "summary": "Given its ability to analyse stochastic models ranging from discrete and\ncontinuous-time Markov chains to Markov decision processes and stochastic\ngames, probabilistic model checking (PMC) is widely used to verify system\ndependability and performance properties. However, modelling the behaviour of,\nand verifying these properties for many software-intensive systems requires the\njoint analysis of multiple interdependent stochastic models of different types,\nwhich existing PMC techniques and tools cannot handle. To address this\nlimitation, we introduce a tool-supported UniversaL stochasTIc Modelling,\nverificAtion and synThEsis (ULTIMATE) framework that supports the\nrepresentation, verification and synthesis of heterogeneous multi-model\nstochastic systems with complex model interdependencies. Through its unique\nintegration of multiple PMC paradigms, and underpinned by a novel verification\nmethod for handling model interdependencies, ULTIMATE unifies-for the first\ntime-the modelling of probabilistic and nondeterministic uncertainty, discrete\nand continuous time, partial observability, and the use of both Bayesian and\nfrequentist inference to exploit domain knowledge and data about the modelled\nsystem and its context. A comprehensive suite of case studies and experiments\nconfirm the generality and effectiveness of our novel verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given its ability to analyse stochastic models ranging from discrete and\ncontinuous-time Markov chains to Markov decision processes and stochastic\ngames, probabilistic model checking (PMC) is widely used to verify system\ndependability and performance properties. However, modelling the behaviour of,\nand verifying these properties for many software-intensive systems requires the\njoint analysis of multiple interdependent stochastic models of different types,\nwhich existing PMC techniques and tools cannot handle. To address this\nlimitation, we introduce a tool-supported UniversaL stochasTIc Modelling,\nverificAtion and synThEsis (ULTIMATE) framework that supports the\nrepresentation, verification and synthesis of heterogeneous multi-model\nstochastic systems with complex model interdependencies. Through its unique\nintegration of multiple PMC paradigms, and underpinned by a novel verification\nmethod for handling model interdependencies, ULTIMATE unifies-for the first\ntime-the modelling of probabilistic and nondeterministic uncertainty, discrete\nand continuous time, partial observability, and the use of both Bayesian and\nfrequentist inference to exploit domain knowledge and data about the modelled\nsystem and its context. A comprehensive suite of case studies and experiments\nconfirm the generality and effectiveness of our novel verification framework."
                },
                "authors": [
                    {
                        "name": "Radu Calinescu"
                    },
                    {
                        "name": "Sinem Getir Yaman"
                    },
                    {
                        "name": "Simos Gerasimou"
                    },
                    {
                        "name": "Gricel Vázquez"
                    },
                    {
                        "name": "Micah Bassett"
                    }
                ],
                "author_detail": {
                    "name": "Micah Bassett"
                },
                "author": "Micah Bassett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16657v2",
                "updated": "2025-03-20T10:54:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    54,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2023-09-28T17:55:49Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    17,
                    55,
                    49,
                    3,
                    271,
                    0
                ],
                "title": "Asymptotically Optimal Sequential Multiple Testing Procedures for\n  Correlated Normal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically Optimal Sequential Multiple Testing Procedures for\n  Correlated Normal"
                },
                "summary": "Simultaneous statistical inference has been a cornerstone in the statistics\nmethodology literature because of its fundamental theory and paramount\napplications. The mainstream multiple testing literature has traditionally\nconsidered two frameworks: the sample size is deterministic, and the test\nstatistics corresponding to different tests are independent. However, in many\nmodern scientific avenues, these assumptions are often violated. There is\nlittle study that explores the multiple testing problem in a sequential\nframework where the test statistics corresponding to the various streams are\ndependent. This work fills this gap in a unified way by considering the\nclassical means-testing problem in an equicorrelated Gaussian and sequential\nframework. We focus on sequential test procedures that control the type I and\ntype II familywise error probabilities at pre-specified levels. We establish\nthat our proposed test procedures achieve the optimal expected sample sizes\nunder every possible signal configuration asymptotically, as the two error\nprobabilities vanish at arbitrary rates. Towards this, we elucidate that the\nratio of the expected sample size of our proposed rule and that of the\nclassical SPRT goes to one asymptotically, thus illustrating their connection.\nGeneralizing this, we show that our proposed procedures, with appropriately\nadjusted critical values, are asymptotically optimal for controlling any\nmultiple testing error metric lying between multiples of FWER in a certain\nsense. This class of metrics includes FDR/FNR, pFDR/pFNR, the per-comparison\nand per-family error rates, and the false positive rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous statistical inference has been a cornerstone in the statistics\nmethodology literature because of its fundamental theory and paramount\napplications. The mainstream multiple testing literature has traditionally\nconsidered two frameworks: the sample size is deterministic, and the test\nstatistics corresponding to different tests are independent. However, in many\nmodern scientific avenues, these assumptions are often violated. There is\nlittle study that explores the multiple testing problem in a sequential\nframework where the test statistics corresponding to the various streams are\ndependent. This work fills this gap in a unified way by considering the\nclassical means-testing problem in an equicorrelated Gaussian and sequential\nframework. We focus on sequential test procedures that control the type I and\ntype II familywise error probabilities at pre-specified levels. We establish\nthat our proposed test procedures achieve the optimal expected sample sizes\nunder every possible signal configuration asymptotically, as the two error\nprobabilities vanish at arbitrary rates. Towards this, we elucidate that the\nratio of the expected sample size of our proposed rule and that of the\nclassical SPRT goes to one asymptotically, thus illustrating their connection.\nGeneralizing this, we show that our proposed procedures, with appropriately\nadjusted critical values, are asymptotically optimal for controlling any\nmultiple testing error metric lying between multiples of FWER in a certain\nsense. This class of metrics includes FDR/FNR, pFDR/pFNR, the per-comparison\nand per-family error rates, and the false positive rate."
                },
                "authors": [
                    {
                        "name": "Monitirtha Dey"
                    },
                    {
                        "name": "Subir Kumar Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Subir Kumar Bhandari"
                },
                "author": "Subir Kumar Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16182v2",
                "updated": "2025-03-20T10:52:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    45,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-22T10:59:11Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    59,
                    11,
                    5,
                    53,
                    0
                ],
                "title": "IPO: Your Language Model is Secretly a Preference Classifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPO: Your Language Model is Secretly a Preference Classifier"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences."
                },
                "authors": [
                    {
                        "name": "Shivank Garg"
                    },
                    {
                        "name": "Ayush Singh"
                    },
                    {
                        "name": "Shweta Singh"
                    },
                    {
                        "name": "Paras Chopra"
                    }
                ],
                "author_detail": {
                    "name": "Paras Chopra"
                },
                "author": "Paras Chopra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.16428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16428v1",
                "updated": "2025-03-20T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    58,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    58,
                    3,
                    79,
                    0
                ],
                "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAttention: Block Sparse Attention with Antidiagonal Scoring"
                },
                "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16419v1",
                "updated": "2025-03-20T17:59:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."
                },
                "authors": [
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Jiamu Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Andrew Wen"
                    },
                    {
                        "name": "Shaochen"
                    },
                    {
                        "name": "Zhong"
                    },
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "arxiv_affiliation": "Henry",
                "author": "Xia Hu",
                "arxiv_comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16416v1",
                "updated": "2025-03-20T17:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    23,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    23,
                    3,
                    79,
                    0
                ],
                "title": "Survey on Evaluation of LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Evaluation of LLM-based Agents"
                },
                "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Alan Li"
                    },
                    {
                        "name": "Guy Uziel"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    }
                ],
                "author_detail": {
                    "name": "Michal Shmueli-Scheuer"
                },
                "author": "Michal Shmueli-Scheuer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16413v1",
                "updated": "2025-03-20T17:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    12,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:59:12Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    12,
                    3,
                    79,
                    0
                ],
                "title": "M3: 3D-Spatial MultiModal Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3: 3D-Spatial MultiModal Memory"
                },
                "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."
                },
                "authors": [
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Yuchen Song"
                    },
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Xuanbin Peng"
                    },
                    {
                        "name": "Jianglong Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "arxiv_comment": "ICLR2025 homepage: https://m3-spatial-memory.github.io code:\n  https://github.com/MaureenZOU/m3-spatial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15242v2",
                "updated": "2025-03-20T17:58:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    58,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-19T14:19:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?"
                },
                "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."
                },
                "authors": [
                    {
                        "name": "Pierre Chambon"
                    },
                    {
                        "name": "Baptiste Roziere"
                    },
                    {
                        "name": "Benoit Sagot"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18908v2",
                "updated": "2025-03-20T17:56:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    56,
                    5,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-26T17:59:09Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    17,
                    59,
                    9,
                    4,
                    208,
                    0
                ],
                "title": "Wolf: Dense Video Captioning with a World Summarization Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wolf: Dense Video Captioning with a World Summarization Framework"
                },
                "summary": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/."
                },
                "authors": [
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ran Tian"
                    },
                    {
                        "name": "Shuhan Tan"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yin Cui"
                    },
                    {
                        "name": "Sushant Veer"
                    },
                    {
                        "name": "Max Ehrlich"
                    },
                    {
                        "name": "Jonah Philion"
                    },
                    {
                        "name": "Xinshuo Weng"
                    },
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Marco Pavone"
                    }
                ],
                "author_detail": {
                    "name": "Marco Pavone"
                },
                "author": "Marco Pavone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16402v1",
                "updated": "2025-03-20T17:55:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    55,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:55:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    55,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination"
                },
                "summary": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Dongbai Li"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16401v1",
                "updated": "2025-03-20T17:54:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:54:42Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    54,
                    42,
                    3,
                    79,
                    0
                ],
                "title": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them"
                },
                "summary": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning."
                },
                "authors": [
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Tianren Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "author": "Feng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16385v1",
                "updated": "2025-03-20T17:46:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    46,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:46:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    46,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."
                },
                "authors": [
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Yulin Song"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "GengRu Chen"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03035v2",
                "updated": "2025-03-20T17:43:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    43,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-03T22:41:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    41,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments"
                },
                "summary": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. while many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. recently, large language models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. however, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. we\naddress these limitations via spine, an online planner for missions with\nincomplete mission specifications provided in natural language. the planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. tasks are\nautomatically validated for safety and refined online with new map\nobservations. we evaluate spine in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at:\nhttps://zacravichandran.github.io/SPINE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. while many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. recently, large language models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. however, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. we\naddress these limitations via spine, an online planner for missions with\nincomplete mission specifications provided in natural language. the planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. tasks are\nautomatically validated for safety and refined online with new map\nobservations. we evaluate spine in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at:\nhttps://zacravichandran.github.io/SPINE."
                },
                "authors": [
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Mariliza Tzes"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v5",
                "updated": "2025-03-20T17:39:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    13,
                    3,
                    79,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_comment": "13 pages, 6 figures, VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12372v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16376v1",
                "updated": "2025-03-20T17:39:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    6,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:39:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    39,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images"
                },
                "summary": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."
                },
                "authors": [
                    {
                        "name": "Leyang Wang"
                    },
                    {
                        "name": "Joice Lin"
                    }
                ],
                "author_detail": {
                    "name": "Joice Lin"
                },
                "author": "Joice Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18637v2",
                "updated": "2025-03-20T17:15:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    15,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-25T20:54:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    20,
                    54,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "From Stars to Molecules: AI Guided Device-Agnostic Super-Resolution\n  Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Stars to Molecules: AI Guided Device-Agnostic Super-Resolution\n  Imaging"
                },
                "summary": "Super-resolution imaging has revolutionized the study of systems ranging from\nmolecular structures to distant galaxies. However, existing super-resolution\nmethods require extensive calibration and retraining for each imaging setup,\nlimiting their practical deployment. We introduce a device-agnostic\ndeep-learning framework for super-resolution imaging of point-like emitters\nthat eliminates the need for calibration data or explicit knowledge of optical\nsystem parameters. Our model is trained on a diverse, numerically simulated\ndataset encompassing a broad range of imaging conditions, enabling\ngeneralization across different optical setups. Once trained, it reconstructs\nsuper-resolved images directly from a single resolution-limited camera frame\nwith superior accuracy and computational efficiency compared to\nstate-of-the-art methods. We experimentally validate our approach using a\ncustom microscopy setup with ground-truth emitter positions. We also\ndemonstrate its versatility on astronomical and single-molecule localization\nmicroscopy datasets, achieving unprecedented resolution without prior\ninformation. Our findings establish a pathway toward universal,\ncalibration-free super-resolution imaging, expanding its applicability across\nscientific disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-resolution imaging has revolutionized the study of systems ranging from\nmolecular structures to distant galaxies. However, existing super-resolution\nmethods require extensive calibration and retraining for each imaging setup,\nlimiting their practical deployment. We introduce a device-agnostic\ndeep-learning framework for super-resolution imaging of point-like emitters\nthat eliminates the need for calibration data or explicit knowledge of optical\nsystem parameters. Our model is trained on a diverse, numerically simulated\ndataset encompassing a broad range of imaging conditions, enabling\ngeneralization across different optical setups. Once trained, it reconstructs\nsuper-resolved images directly from a single resolution-limited camera frame\nwith superior accuracy and computational efficiency compared to\nstate-of-the-art methods. We experimentally validate our approach using a\ncustom microscopy setup with ground-truth emitter positions. We also\ndemonstrate its versatility on astronomical and single-molecule localization\nmicroscopy datasets, achieving unprecedented resolution without prior\ninformation. Our findings establish a pathway toward universal,\ncalibration-free super-resolution imaging, expanding its applicability across\nscientific disciplines."
                },
                "authors": [
                    {
                        "name": "Dominik Vašinka"
                    },
                    {
                        "name": "Filip Juráň"
                    },
                    {
                        "name": "Jaromír Běhal"
                    },
                    {
                        "name": "Miroslav Ježek"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Ježek"
                },
                "author": "Miroslav Ježek",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16356v1",
                "updated": "2025-03-20T17:14:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners"
                },
                "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."
                },
                "authors": [
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16334v1",
                "updated": "2025-03-20T16:55:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    55,
                    26,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:55:26Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    55,
                    26,
                    3,
                    79,
                    0
                ],
                "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates"
                },
                "summary": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."
                },
                "authors": [
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Lifu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lifu Huang"
                },
                "author": "Lifu Huang",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12509v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12509v4",
                "updated": "2025-03-20T16:45:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-18T03:47:53Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    47,
                    53,
                    1,
                    49,
                    0
                ],
                "title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents"
                },
                "summary": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code."
                },
                "authors": [
                    {
                        "name": "Kangda Wei"
                    },
                    {
                        "name": "Xi Shi"
                    },
                    {
                        "name": "Jonathan Tong"
                    },
                    {
                        "name": "Sai Ramana Reddy"
                    },
                    {
                        "name": "Anandhavelu Natarajan"
                    },
                    {
                        "name": "Rajiv Jain"
                    },
                    {
                        "name": "Aparna Garimella"
                    },
                    {
                        "name": "Ruihong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruihong Huang"
                },
                "author": "Ruihong Huang",
                "arxiv_comment": "Need company internal approval before public release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12509v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12509v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16326v1",
                "updated": "2025-03-20T16:45:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:45:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    45,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence"
                },
                "summary": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication."
                },
                "authors": [
                    {
                        "name": "Long Yuan"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Wangyuxuan Zhai"
                    },
                    {
                        "name": "Xiaoyu Zhu"
                    },
                    {
                        "name": "You Li"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Yun Nie"
                },
                "author": "Jian-Yun Nie",
                "arxiv_comment": "15 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16320v1",
                "updated": "2025-03-20T16:44:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    44,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:44:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    44,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Issue2Test: Generating Reproducing Test Cases from Issue Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue2Test: Generating Reproducing Test Cases from Issue Reports"
                },
                "summary": "Automated tools for solving GitHub issues are receiving significant attention\nby both researchers and practitioners, e.g., in the form of foundation models\nand LLM-based agents prompted with issues. A crucial step toward successfully\nsolving an issue is creating a test case that accurately reproduces the issue.\nSuch a test case can guide the search for an appropriate patch and help\nvalidate whether the patch matches the issue's intent. However, existing\ntechniques for issue reproduction show only moderate success. This paper\npresents Issue2Test, an LLM-based technique for automatically generating a\nreproducing test case for a given issue report. Unlike automated regression\ntest generators, which aim at creating passing tests, our approach aims at a\ntest that fails, and that fails specifically for the reason described in the\nissue. To this end, Issue2Test performs three steps: (1) understand the issue\nand gather context (e.g., related files and project-specific guidelines)\nrelevant for reproducing it; (2) generate a candidate test case; and (3)\niteratively refine the test case based on compilation and runtime feedback\nuntil it fails and the failure aligns with the problem described in the issue.\nWe evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully\nreproduces 30.4 of the issues, achieving a 40.1% relative improvement over the\nbest existing technique. Our evaluation also shows that Issue2test reproduces\n28 issues that seven prior techniques fail to address, contributing a total of\n68.3% of all issues reproduced by any tool. We envision our approach to\ncontribute to enhancing the overall progress in the important task of\nautomatically solving GitHub issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated tools for solving GitHub issues are receiving significant attention\nby both researchers and practitioners, e.g., in the form of foundation models\nand LLM-based agents prompted with issues. A crucial step toward successfully\nsolving an issue is creating a test case that accurately reproduces the issue.\nSuch a test case can guide the search for an appropriate patch and help\nvalidate whether the patch matches the issue's intent. However, existing\ntechniques for issue reproduction show only moderate success. This paper\npresents Issue2Test, an LLM-based technique for automatically generating a\nreproducing test case for a given issue report. Unlike automated regression\ntest generators, which aim at creating passing tests, our approach aims at a\ntest that fails, and that fails specifically for the reason described in the\nissue. To this end, Issue2Test performs three steps: (1) understand the issue\nand gather context (e.g., related files and project-specific guidelines)\nrelevant for reproducing it; (2) generate a candidate test case; and (3)\niteratively refine the test case based on compilation and runtime feedback\nuntil it fails and the failure aligns with the problem described in the issue.\nWe evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully\nreproduces 30.4 of the issues, achieving a 40.1% relative improvement over the\nbest existing technique. Our evaluation also shows that Issue2test reproduces\n28 issues that seven prior techniques fail to address, contributing a total of\n68.3% of all issues reproduced by any tool. We envision our approach to\ncontribute to enhancing the overall progress in the important task of\nautomatically solving GitHub issues."
                },
                "authors": [
                    {
                        "name": "Noor Nashid"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Ali Mesbah"
                    }
                ],
                "author_detail": {
                    "name": "Ali Mesbah"
                },
                "author": "Ali Mesbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09091v2",
                "updated": "2025-03-20T16:43:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    43,
                    54,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-12T06:03:33Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    3,
                    33,
                    2,
                    71,
                    0
                ],
                "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Foundation Models for Computational Pathology: A Survey"
                },
                "summary": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Guihong Wan"
                    },
                    {
                        "name": "Xintao Wu"
                    },
                    {
                        "name": "Xinyu Wu"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Christine G. Lian"
                    },
                    {
                        "name": "Peter K. Sorger"
                    },
                    {
                        "name": "Yevgeniy R. Semenov"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18400v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18400v3",
                "updated": "2025-03-20T16:37:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    37,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2024-04-29T03:30:06Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    3,
                    30,
                    6,
                    0,
                    120,
                    0
                ],
                "title": "LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models"
                },
                "summary": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR"
                },
                "authors": [
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Kazem Meidani"
                    },
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    },
                    {
                        "name": "Chandan K Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K Reddy"
                },
                "author": "Chandan K Reddy",
                "arxiv_comment": "ICLR 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18400v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18400v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v1",
                "updated": "2025-03-20T16:25:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2409.18486",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v3",
                "updated": "2025-03-20T16:15:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    15,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16280v1",
                "updated": "2025-03-20T16:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    8,
                    47,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:08:47Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    8,
                    47,
                    3,
                    79,
                    0
                ],
                "title": "Binary-Report Peer Prediction for Real-Valued Signal Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary-Report Peer Prediction for Real-Valued Signal Spaces"
                },
                "summary": "Theoretical guarantees about peer prediction mechanisms typically rely on the\ndiscreteness of the signal and report space. However, we posit that a discrete\nsignal model is not realistic: in practice, agents observe richer information\nand map their signals to a discrete report. In this paper, we formalize a model\nwith real-valued signals and binary reports. We study a natural class of\nsymmetric strategies where agents map their information to a binary value\naccording to a single real-valued threshold. We characterize equilibria for\nseveral well-known peer prediction mechanisms which are known to be truthful\nunder the binary report model. In general, even when every threshold would\ncorrespond to a truthful equilibrium in the binary signal model, only certain\nthresholds remain equilibria in our model. Furthermore, by studying the\ndynamics of this threshold, we find that some of these equilibria are unstable.\nThese results suggest important limitations for the deployment of existing peer\nprediction mechanisms in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical guarantees about peer prediction mechanisms typically rely on the\ndiscreteness of the signal and report space. However, we posit that a discrete\nsignal model is not realistic: in practice, agents observe richer information\nand map their signals to a discrete report. In this paper, we formalize a model\nwith real-valued signals and binary reports. We study a natural class of\nsymmetric strategies where agents map their information to a binary value\naccording to a single real-valued threshold. We characterize equilibria for\nseveral well-known peer prediction mechanisms which are known to be truthful\nunder the binary report model. In general, even when every threshold would\ncorrespond to a truthful equilibrium in the binary signal model, only certain\nthresholds remain equilibria in our model. Furthermore, by studying the\ndynamics of this threshold, we find that some of these equilibria are unstable.\nThese results suggest important limitations for the deployment of existing peer\nprediction mechanisms in practice."
                },
                "authors": [
                    {
                        "name": "Rafael Frongillo"
                    },
                    {
                        "name": "Ian Kash"
                    },
                    {
                        "name": "Mary Monroe"
                    }
                ],
                "author_detail": {
                    "name": "Mary Monroe"
                },
                "author": "Mary Monroe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18688v3",
                "updated": "2025-03-20T16:07:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    7,
                    9,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-27T19:00:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    19,
                    0,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment"
                },
                "summary": "With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively."
                },
                "authors": [
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Vaibhav Singh"
                    },
                    {
                        "name": "Tianrui Guan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16275v1",
                "updated": "2025-03-20T16:05:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    5,
                    35,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:05:35Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    5,
                    35,
                    3,
                    79,
                    0
                ],
                "title": "Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory\n  Estimation through Monocular Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory\n  Estimation through Monocular Priors"
                },
                "summary": "(Visual) Simultaneous Localization and Mapping (SLAM) remains a fundamental\nchallenge in enabling autonomous systems to navigate and understand large-scale\nenvironments. Traditional SLAM approaches struggle to balance efficiency and\naccuracy, particularly in large-scale settings where extensive computational\nresources are required for scene reconstruction and Bundle Adjustment (BA).\nHowever, this scene reconstruction, in the form of sparse pointclouds of visual\nlandmarks, is often only used within the SLAM system because navigation and\nplanning methods require different map representations. In this work, we\ntherefore investigate a more scalable Visual SLAM (VSLAM) approach without\nreconstruction, mainly based on approaches for two-view loop closures. By\nrestricting the map to a sparse keyframed pose graph without dense geometry\nrepresentations, our '2GO' system achieves efficient optimization with\ncompetitive absolute trajectory accuracy. In particular, we find that recent\nadvancements in image matching and monocular depth priors enable very accurate\ntrajectory optimization from two-view edges. We conduct extensive experiments\non diverse datasets, including large-scale scenarios, and provide a detailed\nanalysis of the trade-offs between runtime, accuracy, and map size. Our results\ndemonstrate that this streamlined approach supports real-time performance,\nscales well in map size and trajectory duration, and effectively broadens the\ncapabilities of VSLAM for long-duration deployments to large environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Visual) Simultaneous Localization and Mapping (SLAM) remains a fundamental\nchallenge in enabling autonomous systems to navigate and understand large-scale\nenvironments. Traditional SLAM approaches struggle to balance efficiency and\naccuracy, particularly in large-scale settings where extensive computational\nresources are required for scene reconstruction and Bundle Adjustment (BA).\nHowever, this scene reconstruction, in the form of sparse pointclouds of visual\nlandmarks, is often only used within the SLAM system because navigation and\nplanning methods require different map representations. In this work, we\ntherefore investigate a more scalable Visual SLAM (VSLAM) approach without\nreconstruction, mainly based on approaches for two-view loop closures. By\nrestricting the map to a sparse keyframed pose graph without dense geometry\nrepresentations, our '2GO' system achieves efficient optimization with\ncompetitive absolute trajectory accuracy. In particular, we find that recent\nadvancements in image matching and monocular depth priors enable very accurate\ntrajectory optimization from two-view edges. We conduct extensive experiments\non diverse datasets, including large-scale scenarios, and provide a detailed\nanalysis of the trade-offs between runtime, accuracy, and map size. Our results\ndemonstrate that this streamlined approach supports real-time performance,\nscales well in map size and trajectory duration, and effectively broadens the\ncapabilities of VSLAM for long-duration deployments to large environments."
                },
                "authors": [
                    {
                        "name": "Tian Yi Lim"
                    },
                    {
                        "name": "Boyang Sun"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Hermann Blum"
                    }
                ],
                "author_detail": {
                    "name": "Hermann Blum"
                },
                "author": "Hermann Blum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16271v1",
                "updated": "2025-03-20T16:03:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    3,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:03:39Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    3,
                    39,
                    3,
                    79,
                    0
                ],
                "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Robustness in Machine Learning: A Posterior Agreement\n  Approach"
                },
                "summary": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
                },
                "authors": [
                    {
                        "name": "João Borges S. Carvalho"
                    },
                    {
                        "name": "Alessandro Torcinovich"
                    },
                    {
                        "name": "Victor Jimenez Rodriguez"
                    },
                    {
                        "name": "Antonio E. Cinà"
                    },
                    {
                        "name": "Carlos Cotrini"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Joachim M. Buhmann"
                    }
                ],
                "author_detail": {
                    "name": "Joachim M. Buhmann"
                },
                "author": "Joachim M. Buhmann",
                "arxiv_comment": "Preprint submitted to TMLR. 29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16260v1",
                "updated": "2025-03-20T15:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:56:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    56,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data"
                },
                "summary": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts."
                },
                "authors": [
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15195v2",
                "updated": "2025-03-20T15:49:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    49,
                    10,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-19T13:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Handwritten Text Recognition"
                },
                "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."
                },
                "authors": [
                    {
                        "name": "Giorgia Crosilla"
                    },
                    {
                        "name": "Lukas Klic"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16252v1",
                "updated": "2025-03-20T15:46:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    46,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:46:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    46,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning"
                },
                "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1."
                },
                "authors": [
                    {
                        "name": "Zhaowei Liu"
                    },
                    {
                        "name": "Xin Guo"
                    },
                    {
                        "name": "Fangqi Lou"
                    },
                    {
                        "name": "Lingfeng Zeng"
                    },
                    {
                        "name": "Jinyi Niu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Weige Cai"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Xueqian Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Dezhi Chen"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Zuo Bai"
                    },
                    {
                        "name": "Liwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liwen Zhang"
                },
                "author": "Liwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00906v3",
                "updated": "2025-03-20T15:42:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    42,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-01T17:51:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    51,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into a variety of\nwriting tasks. While these tools can help people by generating ideas or\nproducing higher quality work, like many other AI tools they may risk causing a\nvariety of harms, disproportionately burdening historically marginalized\ngroups. In this work, we introduce and evaluate perceptual harm, a term for the\nharm caused to users when others perceive or suspect them of using AI. We\nexamined perceptual harms in three online experiments, each of which entailed\nhuman participants evaluating the profiles for fictional freelance writers. We\nasked participants whether they suspected the freelancers of using AI, the\nquality of their writing, and whether they should be hired. We found some\nsupport for perceptual harms against for certain demographic groups, but that\nperceptions of AI use negatively impacted writing evaluations and hiring\noutcomes across the board."
                },
                "authors": [
                    {
                        "name": "Kowe Kadoma"
                    },
                    {
                        "name": "Danaé Metaxa"
                    },
                    {
                        "name": "Mor Naaman"
                    }
                ],
                "author_detail": {
                    "name": "Mor Naaman"
                },
                "author": "Mor Naaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06061v2",
                "updated": "2025-03-20T15:37:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    37,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2024-05-09T19:10:11Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    19,
                    10,
                    11,
                    3,
                    130,
                    0
                ],
                "title": "GPTCoach: Towards LLM-Based Physical Activity Coaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTCoach: Towards LLM-Based Physical Activity Coaching"
                },
                "summary": "Mobile health applications show promise for scalable physical activity\npromotion but are often insufficiently personalized. In contrast, health\ncoaching offers highly personalized support but can be prohibitively expensive\nand inaccessible. This study draws inspiration from health coaching to explore\nhow large language models (LLMs) might address personalization challenges in\nmobile health. We conduct formative interviews with 12 health professionals and\n10 potential coaching recipients to develop design principles for an LLM-based\nhealth coach. We then built GPTCoach, a chatbot that implements the onboarding\nconversation from an evidence-based coaching program, uses conversational\nstrategies from motivational interviewing, and incorporates wearable data to\ncreate personalized physical activity plans. In a lab study with 16\nparticipants using three months of historical data, we find promising evidence\nthat GPTCoach gathers rich qualitative information to offer personalized\nsupport, with users feeling comfortable sharing concerns. We conclude with\nimplications for future research on LLM-based physical activity support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile health applications show promise for scalable physical activity\npromotion but are often insufficiently personalized. In contrast, health\ncoaching offers highly personalized support but can be prohibitively expensive\nand inaccessible. This study draws inspiration from health coaching to explore\nhow large language models (LLMs) might address personalization challenges in\nmobile health. We conduct formative interviews with 12 health professionals and\n10 potential coaching recipients to develop design principles for an LLM-based\nhealth coach. We then built GPTCoach, a chatbot that implements the onboarding\nconversation from an evidence-based coaching program, uses conversational\nstrategies from motivational interviewing, and incorporates wearable data to\ncreate personalized physical activity plans. In a lab study with 16\nparticipants using three months of historical data, we find promising evidence\nthat GPTCoach gathers rich qualitative information to offer personalized\nsupport, with users feeling comfortable sharing concerns. We conclude with\nimplications for future research on LLM-based physical activity support."
                },
                "authors": [
                    {
                        "name": "Matthew Jörke"
                    },
                    {
                        "name": "Shardul Sapkota"
                    },
                    {
                        "name": "Lyndsea Warkenthien"
                    },
                    {
                        "name": "Niklas Vainio"
                    },
                    {
                        "name": "Paul Schmiedmayer"
                    },
                    {
                        "name": "Emma Brunskill"
                    },
                    {
                        "name": "James A. Landay"
                    }
                ],
                "author_detail": {
                    "name": "James A. Landay"
                },
                "author": "James A. Landay",
                "arxiv_doi": "10.1145/3706598.3713819",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713819",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.06061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please note that the title has been updated from a previous pre-print\n  (previously: \"Supporting Physical Activity Behavior Change with LLM-Based\n  Conversational Agents\")",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v3",
                "updated": "2025-03-20T15:35:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    35,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization in language models via probabilistic extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization in language models via probabilistic extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "A. Feder Cooper"
                    }
                ],
                "author_detail": {
                    "name": "A. Feder Cooper"
                },
                "author": "A. Feder Cooper",
                "arxiv_comment": "NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10491v2",
                "updated": "2025-03-20T15:32:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    32,
                    47,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-14T13:35:47Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    35,
                    47,
                    0,
                    288,
                    0
                ],
                "title": "TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning"
                },
                "summary": "Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Aritra Bhowmik"
                    },
                    {
                        "name": "Mohammad Mahdi Derakhshani"
                    },
                    {
                        "name": "Dennis Koelma"
                    },
                    {
                        "name": "Yuki M. Asano"
                    },
                    {
                        "name": "Martin R. Oswald"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20089v2",
                "updated": "2025-03-20T15:28:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    28,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2024-09-30T08:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    8,
                    41,
                    39,
                    0,
                    274,
                    0
                ],
                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM safeguarding via refusal feature adversarial training"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Virginie Do"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05421v2",
                "updated": "2025-03-20T15:21:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    21,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2024-08-10T03:15:24Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    3,
                    15,
                    24,
                    5,
                    223,
                    0
                ],
                "title": "EPAM-Net: An Efficient Pose-driven Attention-guided Multimodal Network\n  for Video Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPAM-Net: An Efficient Pose-driven Attention-guided Multimodal Network\n  for Video Action Recognition"
                },
                "summary": "Existing multimodal-based human action recognition approaches are\ncomputationally intensive, limiting their deployment in real-time applications.\nIn this work, we present a novel and efficient pose-driven attention-guided\nmultimodal network (EPAM-Net) for action recognition in videos. Specifically,\nwe propose eXpand temporal Shift (X-ShiftNet) convolutional architectures for\nRGB and pose streams to capture spatio-temporal features from RGB videos and\ntheir skeleton sequences. The X-ShiftNet tackles the high computational cost of\nthe 3D CNNs by integrating the Temporal Shift Module (TSM) into an efficient 2D\nCNN, enabling efficient spatiotemporal learning. Then skeleton features are\nutilized to guide the visual network stream, focusing on keyframes and their\nsalient spatial regions using the proposed spatial-temporal attention block.\nFinally, the predictions of the two streams are fused for final classification.\nThe experimental results show that our method, with a significant reduction in\nfloating-point operations (FLOPs), outperforms and competes with the\nstate-of-the-art methods on NTU RGB-D 60, NTU RGB-D 120, PKU-MMD, and Toyota\nSmartHome datasets. The proposed EPAM-Net provides up to a 72.8x reduction in\nFLOPs and up to a 48.6x reduction in the number of network parameters. The code\nwill be available at\nhttps://github.com/ahmed-nady/Multimodal-Action-Recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multimodal-based human action recognition approaches are\ncomputationally intensive, limiting their deployment in real-time applications.\nIn this work, we present a novel and efficient pose-driven attention-guided\nmultimodal network (EPAM-Net) for action recognition in videos. Specifically,\nwe propose eXpand temporal Shift (X-ShiftNet) convolutional architectures for\nRGB and pose streams to capture spatio-temporal features from RGB videos and\ntheir skeleton sequences. The X-ShiftNet tackles the high computational cost of\nthe 3D CNNs by integrating the Temporal Shift Module (TSM) into an efficient 2D\nCNN, enabling efficient spatiotemporal learning. Then skeleton features are\nutilized to guide the visual network stream, focusing on keyframes and their\nsalient spatial regions using the proposed spatial-temporal attention block.\nFinally, the predictions of the two streams are fused for final classification.\nThe experimental results show that our method, with a significant reduction in\nfloating-point operations (FLOPs), outperforms and competes with the\nstate-of-the-art methods on NTU RGB-D 60, NTU RGB-D 120, PKU-MMD, and Toyota\nSmartHome datasets. The proposed EPAM-Net provides up to a 72.8x reduction in\nFLOPs and up to a 48.6x reduction in the number of network parameters. The code\nwill be available at\nhttps://github.com/ahmed-nady/Multimodal-Action-Recognition."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdelkawy"
                    },
                    {
                        "name": "Asem Ali"
                    },
                    {
                        "name": "Aly Farag"
                    }
                ],
                "author_detail": {
                    "name": "Aly Farag"
                },
                "author": "Aly Farag",
                "arxiv_doi": "10.1016/j.neucom.2025.129781",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2025.129781",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.05421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Neurocomputing, Volume 633, 7 June 2025, 129781",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16219v1",
                "updated": "2025-03-20T15:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    13,
                    23,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    13,
                    23,
                    3,
                    79,
                    0
                ],
                "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't"
                },
                "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs."
                },
                "authors": [
                    {
                        "name": "Quy-Anh Dang"
                    },
                    {
                        "name": "Chris Ngo"
                    }
                ],
                "author_detail": {
                    "name": "Chris Ngo"
                },
                "author": "Chris Ngo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v2",
                "updated": "2025-03-20T15:09:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    9,
                    51,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07058v3",
                "updated": "2025-03-20T15:01:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    1,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-10T21:49:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    49,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties"
                },
                "summary": "A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin."
                },
                "authors": [
                    {
                        "name": "Zixin Tang"
                    },
                    {
                        "name": "Chieh-Yang Huang"
                    },
                    {
                        "name": "Tsung-Che Li"
                    },
                    {
                        "name": "Ho Yin Sam Ng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                "author": "Ting-Hao 'Kenneth' Huang",
                "arxiv_comment": "Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16212v1",
                "updated": "2025-03-20T15:00:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:00:41Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion"
                },
                "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion."
                },
                "authors": [
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Chenlin Ming"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19146v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19146v4",
                "updated": "2025-03-20T14:50:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    50,
                    4,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-28T13:45:42Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    45,
                    42,
                    3,
                    333,
                    0
                ],
                "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"
                },
                "summary": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ran Rubin"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19146v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19146v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16730v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16730v4",
                "updated": "2025-03-20T14:48:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    48,
                    10,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-23T09:32:44Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    9,
                    32,
                    44,
                    5,
                    328,
                    0
                ],
                "title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks"
                },
                "summary": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16730v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16730v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16202v1",
                "updated": "2025-03-20T14:47:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    47,
                    27,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:47:27Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    47,
                    27,
                    3,
                    79,
                    0
                ],
                "title": "3D Stochastic Geometry Model for Aerial Vehicle-Relayed\n  Ground-Air-Satellite Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Stochastic Geometry Model for Aerial Vehicle-Relayed\n  Ground-Air-Satellite Connectivity"
                },
                "summary": "Due to their flexibility, aerial vehicles (AVs), such as unmanned aerial\nvehicles and airships, are widely employed as relays to assist communications\nbetween massive ground users (GUs) and satellites, forming an AV-relayed\nground-air-satellite solution (GASS). In GASS, the deployment of AVs is crucial\nto ensure overall performance from GUs to satellites. This paper develops a\nstochastic geometry-based analytical model for GASS under Matern hard-core\npoint process (MHCPP) distributed AVs. The 3D distributions of AVs and GUs are\nmodeled by considering their locations on spherical surfaces in the presence of\nhigh-altitude satellites. Accordingly, we derive an overall connectivity\nanalytical model for GASS, which includes the average performance of AV-relayed\ntwo-hop transmissions. Extensive numerical results validate the accuracy of the\nconnectivity model and provide essential insights for configuring AV\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their flexibility, aerial vehicles (AVs), such as unmanned aerial\nvehicles and airships, are widely employed as relays to assist communications\nbetween massive ground users (GUs) and satellites, forming an AV-relayed\nground-air-satellite solution (GASS). In GASS, the deployment of AVs is crucial\nto ensure overall performance from GUs to satellites. This paper develops a\nstochastic geometry-based analytical model for GASS under Matern hard-core\npoint process (MHCPP) distributed AVs. The 3D distributions of AVs and GUs are\nmodeled by considering their locations on spherical surfaces in the presence of\nhigh-altitude satellites. Accordingly, we derive an overall connectivity\nanalytical model for GASS, which includes the average performance of AV-relayed\ntwo-hop transmissions. Extensive numerical results validate the accuracy of the\nconnectivity model and provide essential insights for configuring AV\ndeployments."
                },
                "authors": [
                    {
                        "name": "Yulei Wang"
                    },
                    {
                        "name": "Yalin Liu"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yujie Qin"
                    },
                    {
                        "name": "Zhongjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhongjie Li"
                },
                "author": "Zhongjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16193v1",
                "updated": "2025-03-20T14:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "Affective Polarization Amongst Swedish Politicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Polarization Amongst Swedish Politicians"
                },
                "summary": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research."
                },
                "authors": [
                    {
                        "name": "François t'Serstevens"
                    },
                    {
                        "name": "Roberto Cerina"
                    },
                    {
                        "name": "Gustav Pepper"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Pepper"
                },
                "author": "Gustav Pepper",
                "arxiv_comment": "5 figures, 4 tables, 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16191v1",
                "updated": "2025-03-20T14:39:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    39,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:39:11Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    39,
                    11,
                    3,
                    79,
                    0
                ],
                "title": "Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making"
                },
                "summary": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management."
                },
                "authors": [
                    {
                        "name": "Yinon Goldshtein"
                    },
                    {
                        "name": "Gal Perelman"
                    },
                    {
                        "name": "Assaf Schuster"
                    },
                    {
                        "name": "Avi Ostfeld"
                    }
                ],
                "author_detail": {
                    "name": "Avi Ostfeld"
                },
                "author": "Avi Ostfeld",
                "arxiv_comment": "Accepted to EWRI Congress 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10361v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10361v5",
                "updated": "2025-03-20T14:27:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    27,
                    22,
                    3,
                    79,
                    0
                ],
                "published": "2023-05-17T16:38:11Z",
                "published_parsed": [
                    2023,
                    5,
                    17,
                    16,
                    38,
                    11,
                    2,
                    137,
                    0
                ],
                "title": "Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps://github.com/eilamshapira/HumanChoicePrediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps://github.com/eilamshapira/HumanChoicePrediction"
                },
                "authors": [
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Omer Madmon"
                    },
                    {
                        "name": "Reut Apel"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.10361v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10361v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00267v3",
                "updated": "2025-03-20T14:23:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    23,
                    17,
                    3,
                    79,
                    0
                ],
                "published": "2023-12-01T00:54:02Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    0,
                    54,
                    2,
                    4,
                    335,
                    0
                ],
                "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Efficient Preference Alignment in LLMs via Active Exploration"
                },
                "summary": "Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature."
                },
                "authors": [
                    {
                        "name": "Viraj Mehta"
                    },
                    {
                        "name": "Syrine Belakaria"
                    },
                    {
                        "name": "Vikramjeet Das"
                    },
                    {
                        "name": "Ojash Neopane"
                    },
                    {
                        "name": "Yijia Dai"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Barbara Engelhardt"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Willie Neiswanger"
                    }
                ],
                "author_detail": {
                    "name": "Willie Neiswanger"
                },
                "author": "Willie Neiswanger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16167v1",
                "updated": "2025-03-20T14:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models"
                },
                "summary": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results."
                },
                "authors": [
                    {
                        "name": "Hong Yi Lin"
                    },
                    {
                        "name": "Chunhua Liu"
                    },
                    {
                        "name": "Haoyu Gao"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    },
                    {
                        "name": "Christoph Treude"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Treude"
                },
                "author": "Christoph Treude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16165v1",
                "updated": "2025-03-20T14:06:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    6,
                    53,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:06:53Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    6,
                    53,
                    3,
                    79,
                    0
                ],
                "title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak\n  Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Optimal Attention and Local Model for Single Image Rain Streak\n  Removal"
                },
                "summary": "High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks."
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Wanshu Fan"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Dongsheng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Zhou"
                },
                "author": "Dongsheng Zhou",
                "arxiv_comment": "14 pages, 14 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16161v1",
                "updated": "2025-03-20T13:58:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    58,
                    32,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:58:32Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    58,
                    32,
                    3,
                    79,
                    0
                ],
                "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation"
                },
                "summary": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
                },
                "authors": [
                    {
                        "name": "Alex-Razvan Ispas"
                    },
                    {
                        "name": "Charles-Elie Simon"
                    },
                    {
                        "name": "Fabien Caspani"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "arxiv_comment": "17 pages, 5 figures, published at 1st workshop of Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI at ICLR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16159v1",
                "updated": "2025-03-20T13:57:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    57,
                    33,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:57:33Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    57,
                    33,
                    3,
                    79,
                    0
                ],
                "title": "Neural Combinatorial Optimization for Real-World Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Combinatorial Optimization for Real-World Routing"
                },
                "summary": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco."
                },
                "authors": [
                    {
                        "name": "Jiwoo Son"
                    },
                    {
                        "name": "Zhikai Zhao"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Changhyun Kwon"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16158v1",
                "updated": "2025-03-20T13:56:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    56,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:56:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    56,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems"
                },
                "summary": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research."
                },
                "authors": [
                    {
                        "name": "Shenbin Qian"
                    },
                    {
                        "name": "Constantin Orăsan"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Félix do Carmo"
                    }
                ],
                "author_detail": {
                    "name": "Félix do Carmo"
                },
                "author": "Félix do Carmo",
                "arxiv_comment": "Accepted to the 10th Workshop on Noisy and User-generated Text at\n  NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16144v1",
                "updated": "2025-03-20T13:47:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    47,
                    6,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:47:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    47,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of\n  Unit Tests with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of\n  Unit Tests with LLMs"
                },
                "summary": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score."
                },
                "authors": [
                    {
                        "name": "Djamel Eddine Khelladi"
                    },
                    {
                        "name": "Charly Reux"
                    },
                    {
                        "name": "Mathieu Acher"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Acher"
                },
                "author": "Mathieu Acher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06759v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06759v4",
                "updated": "2025-03-20T13:46:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    46,
                    48,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-10T18:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    57,
                    0,
                    41,
                    0
                ],
                "title": "Rationalization Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationalization Models for Text-to-SQL"
                },
                "summary": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability."
                },
                "authors": [
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Nhan Pham"
                    },
                    {
                        "name": "Michael Glass"
                    },
                    {
                        "name": "Junkyu Lee"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    }
                ],
                "author_detail": {
                    "name": "Dharmashankar Subramanian"
                },
                "author": "Dharmashankar Subramanian",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06759v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06759v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03029v2",
                "updated": "2025-03-20T13:43:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    43,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2024-03-05T15:05:06Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    15,
                    5,
                    6,
                    1,
                    65,
                    0
                ],
                "title": "Socratic Reasoning Improves Positive Text Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic Reasoning Improves Positive Text Rewriting"
                },
                "summary": "Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts."
                },
                "authors": [
                    {
                        "name": "Anmol Goel"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Christian Montag"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16134v1",
                "updated": "2025-03-20T13:32:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    32,
                    27,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:32:27Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    32,
                    27,
                    3,
                    79,
                    0
                ],
                "title": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS\n  Demosaicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS\n  Demosaicing"
                },
                "summary": "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps://github.com/Clausy9/BMTNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps://github.com/Clausy9/BMTNet."
                },
                "authors": [
                    {
                        "name": "Shiyang Zhou"
                    },
                    {
                        "name": "Haijin Zeng"
                    },
                    {
                        "name": "Yunfan Lu"
                    },
                    {
                        "name": "Tong Shao"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Yongyong Chen"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jingyong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jingyong Su"
                },
                "author": "Jingyong Su",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v1",
                "updated": "2025-03-20T13:25:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08144v2",
                "updated": "2025-03-20T13:21:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    21,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-11T08:02:54Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    2,
                    54,
                    1,
                    70,
                    0
                ],
                "title": "Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method"
                },
                "summary": "Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Chengcheng Chen"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Yugang Chang"
                    },
                    {
                        "name": "Weiming Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zeng"
                },
                "author": "Weiming Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16114v1",
                "updated": "2025-03-20T13:00:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "The Impact of Revealing Large Language Model Stochasticity on Trust,\n  Reliability, and Anthropomorphization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Revealing Large Language Model Stochasticity on Trust,\n  Reliability, and Anthropomorphization"
                },
                "summary": "Interfaces for interacting with large language models (LLMs) are often\ndesigned to mimic human conversations, typically presenting a single response\nto user queries. This design choice can obscure the probabilistic and\npredictive nature of these models, potentially fostering undue trust and\nover-anthropomorphization of the underlying model. In this paper, we\ninvestigate (i) the effect of displaying multiple responses simultaneously as a\ncountermeasure to these issues, and (ii) how a cognitive support\nmechanism-highlighting structural and semantic similarities across\nresponses-helps users deal with the increased cognitive load of that\nintervention. We conducted a within-subjects study in which participants\ninspected responses generated by an LLM under three conditions: one response,\nten responses with cognitive support, and ten responses without cognitive\nsupport. Participants then answered questions about workload, trust and\nreliance, and anthropomorphization. We conclude by reporting the results of\nthese studies and discussing future work and design opportunities for future\nLLM interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interfaces for interacting with large language models (LLMs) are often\ndesigned to mimic human conversations, typically presenting a single response\nto user queries. This design choice can obscure the probabilistic and\npredictive nature of these models, potentially fostering undue trust and\nover-anthropomorphization of the underlying model. In this paper, we\ninvestigate (i) the effect of displaying multiple responses simultaneously as a\ncountermeasure to these issues, and (ii) how a cognitive support\nmechanism-highlighting structural and semantic similarities across\nresponses-helps users deal with the increased cognitive load of that\nintervention. We conducted a within-subjects study in which participants\ninspected responses generated by an LLM under three conditions: one response,\nten responses with cognitive support, and ten responses without cognitive\nsupport. Participants then answered questions about workload, trust and\nreliance, and anthropomorphization. We conclude by reporting the results of\nthese studies and discussing future work and design opportunities for future\nLLM interfaces."
                },
                "authors": [
                    {
                        "name": "Chelse Swoopes"
                    },
                    {
                        "name": "Tyler Holloway"
                    },
                    {
                        "name": "Elena L. Glassman"
                    }
                ],
                "author_detail": {
                    "name": "Elena L. Glassman"
                },
                "author": "Elena L. Glassman",
                "arxiv_comment": "Accepted and presented at Trust and Reliance in Evolving Human-AI\n  Workflows (TREW) Workshop, CHI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16094v1",
                "updated": "2025-03-20T12:34:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    34,
                    1,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:34:01Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    34,
                    1,
                    3,
                    79,
                    0
                ],
                "title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning"
                },
                "summary": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances."
                },
                "authors": [
                    {
                        "name": "Reem I. Masoud"
                    },
                    {
                        "name": "Martin Ferianc"
                    },
                    {
                        "name": "Philip Treleaven"
                    },
                    {
                        "name": "Miguel Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rodrigues"
                },
                "author": "Miguel Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15112v2",
                "updated": "2025-03-20T12:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    11,
                    30,
                    3,
                    79,
                    0
                ],
                "published": "2024-09-23T15:20:07Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    20,
                    7,
                    0,
                    267,
                    0
                ],
                "title": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish"
                },
                "summary": "Evaluating the capabilities of Large Language Models (LLMs) to assist\nteachers and students in educational tasks is receiving increasing attention.\nIn this paper, we assess ChatGPT's capacities to solve and grade real\nprogramming exams, from an accredited BSc degree in Computer Science, written\nin Spanish. Our findings suggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in tackling complex problems or\nevaluating solutions authored by others are far from effective. As part of this\nresearch, we also release a new corpus of programming tasks and the\ncorresponding prompts for solving the problems or grading the solutions. This\nresource can be further exploited by other research teams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the capabilities of Large Language Models (LLMs) to assist\nteachers and students in educational tasks is receiving increasing attention.\nIn this paper, we assess ChatGPT's capacities to solve and grade real\nprogramming exams, from an accredited BSc degree in Computer Science, written\nin Spanish. Our findings suggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in tackling complex problems or\nevaluating solutions authored by others are far from effective. As part of this\nresearch, we also release a new corpus of programming tasks and the\ncorresponding prompts for solving the problems or grading the solutions. This\nresource can be further exploited by other research teams."
                },
                "authors": [
                    {
                        "name": "Pablo Saborido-Fernández"
                    },
                    {
                        "name": "Marcos Fernández-Pichel"
                    },
                    {
                        "name": "David E. Losada"
                    }
                ],
                "author_detail": {
                    "name": "David E. Losada"
                },
                "author": "David E. Losada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16071v1",
                "updated": "2025-03-20T12:04:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    4,
                    40,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T12:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    12,
                    4,
                    40,
                    3,
                    79,
                    0
                ],
                "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLMs by RAG Principles: Towards LLM-native Memory"
                },
                "summary": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types."
                },
                "authors": [
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Shuchi Wu"
                    },
                    {
                        "name": "Ruochen Liu"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Fangbo Tao"
                    }
                ],
                "author_detail": {
                    "name": "Fangbo Tao"
                },
                "author": "Fangbo Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v3",
                "updated": "2025-03-20T11:55:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    55,
                    30,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16047v1",
                "updated": "2025-03-20T11:31:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    31,
                    45,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:31:45Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    31,
                    45,
                    3,
                    79,
                    0
                ],
                "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in\n  Network Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in\n  Network Traffic"
                },
                "summary": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
                },
                "authors": [
                    {
                        "name": "Bisola Faith Kayode"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Egonna Anaesiuba-Bristol"
                    },
                    {
                        "name": "Onyekachukwu Ojumah"
                    },
                    {
                        "name": "Oluwagbade Odimayo"
                    },
                    {
                        "name": "Toyese Oloyede"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Teslim Kazeem"
                    },
                    {
                        "name": "Habeeb Alli"
                    },
                    {
                        "name": "Udodirim Ibem Offia"
                    },
                    {
                        "name": "Prisca Chinazor Amajuoyi"
                    }
                ],
                "author_detail": {
                    "name": "Prisca Chinazor Amajuoyi"
                },
                "author": "Prisca Chinazor Amajuoyi",
                "arxiv_comment": "19 Pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01478v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01478v5",
                "updated": "2025-03-20T11:28:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    28,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-03T12:37:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    37,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Lu Dai"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Jinhui Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01478v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01478v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16043v1",
                "updated": "2025-03-20T11:26:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    26,
                    46,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:26:46Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    26,
                    46,
                    3,
                    79,
                    0
                ],
                "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and\n  Utterance Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete Utterance Rewriting with Editing Operation Guidance and\n  Utterance Augmentation"
                },
                "summary": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR."
                },
                "authors": [
                    {
                        "name": "Zhiyu Cao"
                    },
                    {
                        "name": "Peifeng Li"
                    },
                    {
                        "name": "Yaxin Fan"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoming Zhu"
                },
                "author": "Qiaoming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01414v2",
                "updated": "2025-03-20T11:21:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    21,
                    7,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-03T02:47:03Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    2,
                    47,
                    3,
                    6,
                    308,
                    0
                ],
                "title": "A Deep Dive Into Large Language Model Code Generation Mistakes: What and\n  Why?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Dive Into Large Language Model Code Generation Mistakes: What and\n  Why?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nwidespread application in automated code generation. However, these models can\nstill generate defective code that deviates from the specification. Previous\nresearch has mainly focused on the mistakes in LLM-generated standalone\nfunctions, overlooking real-world software development situations where the\nsuccessful generation of the code requires software contexts such as external\ndependencies. In this paper, we considered both of these code generation\nsituations and identified a range of \\textit{non-syntactic mistakes} arising\nfrom LLMs' misunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through extensive manual\nanalyses, four of which were missed by previous works. To better understand\nthese mistakes, we proposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of LLMs in detecting\nmistakes and their reasons. Our evaluation demonstrated that GPT-4 with the\nReAct prompting technique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading function signatures.\nWe believe that these findings offer valuable insights into enhancing the\nquality of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nwidespread application in automated code generation. However, these models can\nstill generate defective code that deviates from the specification. Previous\nresearch has mainly focused on the mistakes in LLM-generated standalone\nfunctions, overlooking real-world software development situations where the\nsuccessful generation of the code requires software contexts such as external\ndependencies. In this paper, we considered both of these code generation\nsituations and identified a range of \\textit{non-syntactic mistakes} arising\nfrom LLMs' misunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through extensive manual\nanalyses, four of which were missed by previous works. To better understand\nthese mistakes, we proposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of LLMs in detecting\nmistakes and their reasons. Our evaluation demonstrated that GPT-4 with the\nReAct prompting technique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading function signatures.\nWe believe that these findings offer valuable insights into enhancing the\nquality of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "QiHong Chen"
                    },
                    {
                        "name": "Jiachen Yu"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Jiecheng Deng"
                    },
                    {
                        "name": "Justin Tian Jin Chen"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16041v1",
                "updated": "2025-03-20T11:19:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:19:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation"
                },
                "summary": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes"
                },
                "authors": [
                    {
                        "name": "Bisola Faith Kayode"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Egonna Anaesiuba-Bristol"
                    },
                    {
                        "name": "Onyekachukwu Ojumah"
                    },
                    {
                        "name": "Oluwagbade Odimayo"
                    },
                    {
                        "name": "Toyese Oloyede"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Teslim Kazeem"
                    },
                    {
                        "name": "Habeeb Alli"
                    },
                    {
                        "name": "Udodirim Ibem Offia"
                    },
                    {
                        "name": "Prisca Chinazor Amajuoyi"
                    }
                ],
                "author_detail": {
                    "name": "Prisca Chinazor Amajuoyi"
                },
                "author": "Prisca Chinazor Amajuoyi",
                "arxiv_comment": "12 Pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16040v1",
                "updated": "2025-03-20T11:14:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    14,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:14:39Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    14,
                    39,
                    3,
                    79,
                    0
                ],
                "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond"
                },
                "summary": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped."
                },
                "authors": [
                    {
                        "name": "Yaoyao Yu"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yinghao Hu"
                    },
                    {
                        "name": "Bin Wei"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16038v1",
                "updated": "2025-03-20T11:12:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    12,
                    54,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:12:54Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    12,
                    54,
                    3,
                    79,
                    0
                ],
                "title": "DevOps Automation Pipeline Deployment with IaC (Infrastructure as Code)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DevOps Automation Pipeline Deployment with IaC (Infrastructure as Code)"
                },
                "summary": "DevOps pipeline is a set of automated tasks or processes or jobs that has\ntasks assigned to execute automatically that allow the Development team and\nOperations team to collaborate for building and deployment of the software or\nservices. DevOps as a culture includes better collaboration between different\nteams within an organization and the removal of silos between them. This paper\naims to streamline the current software development and deployment process that\nis being followed in most of today's generation DevOps deployment as Continuous\nIntegration and Continuous Delivery (CI/CD) pipelines. Centered to the level of\nsoftware development life cycle (SDLC), it also describes the current ambiguous\ndefinition to clarify the implementation of DevOps in practice along a sample\nCI/CD pipeline deployment. The further objective of the paper is to demonstrate\nthe implementation strategy of DevOps Infrastructure as Code (IaC) and Pipeline\nas a code and the removal of ambiguity in the definition of DevOps\nInfrastructure as a Code methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DevOps pipeline is a set of automated tasks or processes or jobs that has\ntasks assigned to execute automatically that allow the Development team and\nOperations team to collaborate for building and deployment of the software or\nservices. DevOps as a culture includes better collaboration between different\nteams within an organization and the removal of silos between them. This paper\naims to streamline the current software development and deployment process that\nis being followed in most of today's generation DevOps deployment as Continuous\nIntegration and Continuous Delivery (CI/CD) pipelines. Centered to the level of\nsoftware development life cycle (SDLC), it also describes the current ambiguous\ndefinition to clarify the implementation of DevOps in practice along a sample\nCI/CD pipeline deployment. The further objective of the paper is to demonstrate\nthe implementation strategy of DevOps Infrastructure as Code (IaC) and Pipeline\nas a code and the removal of ambiguity in the definition of DevOps\nInfrastructure as a Code methodology."
                },
                "authors": [
                    {
                        "name": "Adarsh Saxena"
                    },
                    {
                        "name": "Sudhakar Singh"
                    },
                    {
                        "name": "Shiv Prakash"
                    },
                    {
                        "name": "Tiansheng Yang"
                    },
                    {
                        "name": "Rajkumar Singh Rathore"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Singh Rathore"
                },
                "author": "Rajkumar Singh Rathore",
                "arxiv_doi": "10.1109/SILCON63976.2024.10910699",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SILCON63976.2024.10910699",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 7 figures, 2024 IEEE Silchar Subsection Conference (SILCON\n  2024)",
                "arxiv_journal_ref": "2024 IEEE Silchar Subsection Conference (SILCON 2024), Agartala,\n  India, 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16036v1",
                "updated": "2025-03-20T11:09:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T11:09:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    9,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models"
                },
                "summary": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom."
                },
                "authors": [
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Longxiang Tang"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Chuanbin Liu"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16029v1",
                "updated": "2025-03-20T10:52:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    50,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:52:50Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    50,
                    3,
                    79,
                    0
                ],
                "title": "A Controllable and Realistic Framework for Evaluating Microservice\n  Scheduling in Cloud-Edge Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Controllable and Realistic Framework for Evaluating Microservice\n  Scheduling in Cloud-Edge Continuum"
                },
                "summary": "The transition from traditional architectures to containerized microservices\nwithin the cloud-edge computing continuum introduces significant challenges,\nparticularly in the efficient scheduling of microservices under dynamic\nconditions. Complex and fluctuating call-graph dependencies, varying cross-node\ncommunication latencies, and unpredictable bandwidth conditions substantially\nimpact the performance and reliability of deployed microservices. Consequently,\naccurately evaluating scheduling policies in such dynamic environments remains\nessential yet challenging due to the lack of realistic and controllable\nevaluation frameworks.\n  In this paper, we propose iDynamics, a novel evaluation framework designed\nexplicitly to address these challenges. iDynamics provides comprehensive and\ncontrollable evaluation capabilities by emulating realistic dynamics, including\nconfigurable call-graph topologies, cross-node communication delays, and\nbandwidth variability. The framework is composed of modular components, such as\nthe Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy\nExtender, enabling fine-grained environmental control and facilitating\nsystematic comparisons of different scheduling strategies. Extensive\nexperiments on a real cloud-edge testbed demonstrate that iDynamics effectively\ncaptures diverse dynamic scenarios encountered in microservice deployments,\noffering a robust solution for evaluating and optimizing policy performance\nunder realistic and controllable conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from traditional architectures to containerized microservices\nwithin the cloud-edge computing continuum introduces significant challenges,\nparticularly in the efficient scheduling of microservices under dynamic\nconditions. Complex and fluctuating call-graph dependencies, varying cross-node\ncommunication latencies, and unpredictable bandwidth conditions substantially\nimpact the performance and reliability of deployed microservices. Consequently,\naccurately evaluating scheduling policies in such dynamic environments remains\nessential yet challenging due to the lack of realistic and controllable\nevaluation frameworks.\n  In this paper, we propose iDynamics, a novel evaluation framework designed\nexplicitly to address these challenges. iDynamics provides comprehensive and\ncontrollable evaluation capabilities by emulating realistic dynamics, including\nconfigurable call-graph topologies, cross-node communication delays, and\nbandwidth variability. The framework is composed of modular components, such as\nthe Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy\nExtender, enabling fine-grained environmental control and facilitating\nsystematic comparisons of different scheduling strategies. Extensive\nexperiments on a real cloud-edge testbed demonstrate that iDynamics effectively\ncaptures diverse dynamic scenarios encountered in microservice deployments,\noffering a robust solution for evaluating and optimizing policy performance\nunder realistic and controllable conditions."
                },
                "authors": [
                    {
                        "name": "Ming Chen"
                    },
                    {
                        "name": "Muhammed Tawfiqul Islam"
                    },
                    {
                        "name": "Maria Rodriguez Read"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "14 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16182v2",
                "updated": "2025-03-20T10:52:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    45,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-22T10:59:11Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    59,
                    11,
                    5,
                    53,
                    0
                ],
                "title": "IPO: Your Language Model is Secretly a Preference Classifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPO: Your Language Model is Secretly a Preference Classifier"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences."
                },
                "authors": [
                    {
                        "name": "Shivank Garg"
                    },
                    {
                        "name": "Ayush Singh"
                    },
                    {
                        "name": "Shweta Singh"
                    },
                    {
                        "name": "Paras Chopra"
                    }
                ],
                "author_detail": {
                    "name": "Paras Chopra"
                },
                "author": "Paras Chopra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16024v1",
                "updated": "2025-03-20T10:42:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    42,
                    33,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:42:33Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    42,
                    33,
                    3,
                    79,
                    0
                ],
                "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided\n  Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided\n  Improvement"
                },
                "summary": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16023v1",
                "updated": "2025-03-20T10:39:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    39,
                    51,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:39:51Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    39,
                    51,
                    3,
                    79,
                    0
                ],
                "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language\n  Models"
                },
                "summary": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
                },
                "authors": [
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "This paper is accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16022v1",
                "updated": "2025-03-20T10:39:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    39,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:39:39Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    39,
                    39,
                    3,
                    79,
                    0
                ],
                "title": "Corrective In-Context Learning: Evaluating Self-Correction in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corrective In-Context Learning: Evaluating Self-Correction in Large\n  Language Models"
                },
                "summary": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research."
                },
                "authors": [
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "Accepted to the 6th Workshop on Insights from Negative Results in NLP\n  at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16021v1",
                "updated": "2025-03-20T10:37:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:37:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems"
                },
                "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's potential\nimpact on the diversity and democratic value of information ecosystems. Here,\nwe introduce a large-scale simulation framework to examine AI-based imitation\nin news, a context critically influential for public discourse. By\nsystematically testing two distinct imitation strategies across a range of\ninformation environments varying in initial diversity, we demonstrate that\nAI-generated articles do not uniformly homogenize content. Instead, AI's\ninfluence is strongly context-dependent: AI-generated articles can introduce\nvaluable diversity in originally homogeneous news environments, while\npotentially diminishing diversity in contexts that initially display high\nheterogeneity. These results illustrate that the baseline diversity of an\ninformation space critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens information diversity. Instead, when\ninformation is initially homogeneous, AI-driven imitation can expand\nperspectives, styles, and topics. This is especially important in news\ncontexts, where information diversity fosters richer public debate by exposing\ncitizens to alternative viewpoints, challenging biases, and preventing\nnarrative monopolies, which is essential for a resilient democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's potential\nimpact on the diversity and democratic value of information ecosystems. Here,\nwe introduce a large-scale simulation framework to examine AI-based imitation\nin news, a context critically influential for public discourse. By\nsystematically testing two distinct imitation strategies across a range of\ninformation environments varying in initial diversity, we demonstrate that\nAI-generated articles do not uniformly homogenize content. Instead, AI's\ninfluence is strongly context-dependent: AI-generated articles can introduce\nvaluable diversity in originally homogeneous news environments, while\npotentially diminishing diversity in contexts that initially display high\nheterogeneity. These results illustrate that the baseline diversity of an\ninformation space critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens information diversity. Instead, when\ninformation is initially homogeneous, AI-driven imitation can expand\nperspectives, styles, and topics. This is especially important in news\ncontexts, where information diversity fosters richer public debate by exposing\ncitizens to alternative viewpoints, challenging biases, and preventing\nnarrative monopolies, which is essential for a resilient democracy."
                },
                "authors": [
                    {
                        "name": "Emil Bakkensen Johansen"
                    },
                    {
                        "name": "Oliver Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Baumann"
                },
                "author": "Oliver Baumann",
                "arxiv_comment": "35 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16013v1",
                "updated": "2025-03-20T10:32:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    32,
                    38,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:32:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    32,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping\n  under Flexible Language Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping\n  under Flexible Language Instructions"
                },
                "summary": "Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Chu"
                    },
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Guoliang You"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xingchen Li"
                    },
                    {
                        "name": "Jianmin Ji"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16011v1",
                "updated": "2025-03-20T10:28:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    28,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T10:28:57Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    28,
                    57,
                    3,
                    79,
                    0
                ],
                "title": "\"This could save us months of work\" -- Use Cases of AI and Automation\n  Support in Investigative Journalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"This could save us months of work\" -- Use Cases of AI and Automation\n  Support in Investigative Journalism"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) expand, more researchers\nare studying their adoption in newsrooms. However, much of the research focus\nremains broad and does not address the specific technical needs of\ninvestigative journalists. Therefore, this paper presents several applied use\ncases where automation and AI intersect with investigative journalism. We\nconducted a within-subjects user study with eight investigative journalists. In\ninterviews, we elicited practical use cases using a speculative design approach\nby having journalists react to a prototype of a system that combines LLMs and\nProgramming-by-Demonstration (PbD) to simplify data collection on numerous\nwebsites. Based on user reports, we classified the journalistic processes into\ndata collecting and reporting. Participants indicated they utilize automation\nto handle repetitive tasks like content monitoring, web scraping,\nsummarization, and preliminary data exploration. Following these insights, we\nprovide guidelines on how investigative journalism can benefit from AI and\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) expand, more researchers\nare studying their adoption in newsrooms. However, much of the research focus\nremains broad and does not address the specific technical needs of\ninvestigative journalists. Therefore, this paper presents several applied use\ncases where automation and AI intersect with investigative journalism. We\nconducted a within-subjects user study with eight investigative journalists. In\ninterviews, we elicited practical use cases using a speculative design approach\nby having journalists react to a prototype of a system that combines LLMs and\nProgramming-by-Demonstration (PbD) to simplify data collection on numerous\nwebsites. Based on user reports, we classified the journalistic processes into\ndata collecting and reporting. Participants indicated they utilize automation\nto handle repetitive tasks like content monitoring, web scraping,\nsummarization, and preliminary data exploration. Following these insights, we\nprovide guidelines on how investigative journalism can benefit from AI and\nautomation."
                },
                "authors": [
                    {
                        "name": "Besjon Cifliku"
                    },
                    {
                        "name": "Hendrik Heuer"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Heuer"
                },
                "author": "Hendrik Heuer",
                "arxiv_doi": "10.1145/3706599.3719856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3719856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in Extended Abstracts of the CHI Conference on Human\n  Factors in Computing Systems (CHI EA '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14103v2",
                "updated": "2025-03-20T10:20:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    20,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-18T10:18:07Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model"
                },
                "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research."
                },
                "authors": [
                    {
                        "name": "Jonas Oppenlaender"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Oppenlaender"
                },
                "author": "Jonas Oppenlaender",
                "arxiv_comment": "17 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18532v2",
                "updated": "2025-03-20T09:58:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    58,
                    49,
                    3,
                    79,
                    0
                ],
                "published": "2025-01-30T17:58:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    58,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "Differentially Private Steering for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Steering for Large Language Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques."
                },
                "authors": [
                    {
                        "name": "Anmol Goel"
                    },
                    {
                        "name": "Yaxi Hu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Amartya Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Amartya Sanyal"
                },
                "author": "Amartya Sanyal",
                "arxiv_comment": "ICLR 2025 Camera Ready; Code: https://github.com/UKPLab/iclr2025-psa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15990v1",
                "updated": "2025-03-20T09:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T09:49:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging\n  Knowledge Graph"
                },
                "summary": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce."
                },
                "authors": [
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v2",
                "updated": "2025-03-20T09:49:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    49,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "LumosCore: Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LumosCore: Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03884v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03884v3",
                "updated": "2025-03-20T09:46:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    46,
                    11,
                    3,
                    79,
                    0
                ],
                "published": "2024-11-06T13:00:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    0,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models"
                },
                "summary": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03884v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03884v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01082v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01082v4",
                "updated": "2025-03-20T09:39:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    39,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-01T08:37:25Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    37,
                    25,
                    0,
                    183,
                    0
                ],
                "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs"
                },
                "summary": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its significant impact on improving text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its significant impact on improving text generation quality."
                },
                "authors": [
                    {
                        "name": "Minh Nguyen"
                    },
                    {
                        "name": "Andrew Baker"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Allen Roush"
                    },
                    {
                        "name": "Andreas Kirsch"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "arxiv_comment": "Added acknowledgements and minor rewordings to make the\n  intro/abstract more readable. No major change in length or content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01082v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01082v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15987v1",
                "updated": "2025-03-20T09:37:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    37,
                    24,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T09:37:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    37,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "A Laser-guided Interaction Interface for Providing Effective Robot\n  Assistance to People with Upper Limbs Impairments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Laser-guided Interaction Interface for Providing Effective Robot\n  Assistance to People with Upper Limbs Impairments"
                },
                "summary": "Robotics has shown significant potential in assisting people with\ndisabilities to enhance their independence and involvement in daily activities.\nIndeed, a societal long-term impact is expected in home-care assistance with\nthe deployment of intelligent robotic interfaces. This work presents a\nhuman-robot interface developed to help people with upper limbs impairments,\nsuch as those affected by stroke injuries, in activities of everyday life. The\nproposed interface leverages on a visual servoing guidance component, which\nutilizes an inexpensive but effective laser emitter device. By projecting the\nlaser on a surface within the workspace of the robot, the user is able to guide\nthe robotic manipulator to desired locations, to reach, grasp and manipulate\nobjects. Considering the targeted users, the laser emitter is worn on the head,\nenabling to intuitively control the robot motions with head movements that\npoint the laser in the environment, which projection is detected with a neural\nnetwork based perception module. The interface implements two control\nmodalities: the first allows the user to select specific locations directly,\ncommanding the robot to reach those points; the second employs a paper keyboard\nwith buttons that can be virtually pressed by pointing the laser at them. These\nbuttons enable a more direct control of the Cartesian velocity of the\nend-effector and provides additional functionalities such as commanding the\naction of the gripper. The proposed interface is evaluated in a series of\nmanipulation tasks involving a 6DOF assistive robot manipulator equipped with\n1DOF beak-like gripper. The two interface modalities are combined to\nsuccessfully accomplish tasks requiring bimanual capacity that is usually\naffected in people with upper limbs impairments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotics has shown significant potential in assisting people with\ndisabilities to enhance their independence and involvement in daily activities.\nIndeed, a societal long-term impact is expected in home-care assistance with\nthe deployment of intelligent robotic interfaces. This work presents a\nhuman-robot interface developed to help people with upper limbs impairments,\nsuch as those affected by stroke injuries, in activities of everyday life. The\nproposed interface leverages on a visual servoing guidance component, which\nutilizes an inexpensive but effective laser emitter device. By projecting the\nlaser on a surface within the workspace of the robot, the user is able to guide\nthe robotic manipulator to desired locations, to reach, grasp and manipulate\nobjects. Considering the targeted users, the laser emitter is worn on the head,\nenabling to intuitively control the robot motions with head movements that\npoint the laser in the environment, which projection is detected with a neural\nnetwork based perception module. The interface implements two control\nmodalities: the first allows the user to select specific locations directly,\ncommanding the robot to reach those points; the second employs a paper keyboard\nwith buttons that can be virtually pressed by pointing the laser at them. These\nbuttons enable a more direct control of the Cartesian velocity of the\nend-effector and provides additional functionalities such as commanding the\naction of the gripper. The proposed interface is evaluated in a series of\nmanipulation tasks involving a 6DOF assistive robot manipulator equipped with\n1DOF beak-like gripper. The two interface modalities are combined to\nsuccessfully accomplish tasks requiring bimanual capacity that is usually\naffected in people with upper limbs impairments."
                },
                "authors": [
                    {
                        "name": "Davide Torielli"
                    },
                    {
                        "name": "Liana Bertoni"
                    },
                    {
                        "name": "Luca Muratore"
                    },
                    {
                        "name": "Nikos Tsagarakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Tsagarakis"
                },
                "author": "Nikos Tsagarakis",
                "arxiv_doi": "10.1109/LRA.2024.3430709",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3430709",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.15987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 12 figures",
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 9, no. 9, pp.\n  7653-7660, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15964v1",
                "updated": "2025-03-20T09:05:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    5,
                    16,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T09:05:16Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    9,
                    5,
                    16,
                    3,
                    79,
                    0
                ],
                "title": "Are We There Yet? A Study of Decentralized Identity Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We There Yet? A Study of Decentralized Identity Applications"
                },
                "summary": "The development of Decentralized Identities (DI) and Self-Sovereign\nIdentities (SSI) has seen significant growth in recent years. This is\naccompanied by a numerous academic and commercial contributions to the\ndevelopment of principles, standards, and systems. While several comprehensive\nreviews have been produced, they predominantly focus on academic literature,\nwith few considering grey literature to provide a holistic view of\ntechnological advancements. Furthermore, no existing surveys have thoroughly\nanalyzed real-world deployments to understand the barriers to the widespread\nadoption of decentralized identity models. This paper addresses the gap by\nexploring both academic and grey literature and examining commercial and\ngovernmental initiatives, to present a comprehensive landscape of decentralized\nidentity technologies and their adoption in real-world. Additionally, it\nidentifies the practical challenges and limitations that slowdown the\ntransition from centralized to decentralized identity management systems. By\nshifting the focus from purely technological constraints to real-world\ndeployment issues, this survey identifies the underlying reasons preventing the\nadoption of decentralized identities despite their evident benefits to the data\nowner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Decentralized Identities (DI) and Self-Sovereign\nIdentities (SSI) has seen significant growth in recent years. This is\naccompanied by a numerous academic and commercial contributions to the\ndevelopment of principles, standards, and systems. While several comprehensive\nreviews have been produced, they predominantly focus on academic literature,\nwith few considering grey literature to provide a holistic view of\ntechnological advancements. Furthermore, no existing surveys have thoroughly\nanalyzed real-world deployments to understand the barriers to the widespread\nadoption of decentralized identity models. This paper addresses the gap by\nexploring both academic and grey literature and examining commercial and\ngovernmental initiatives, to present a comprehensive landscape of decentralized\nidentity technologies and their adoption in real-world. Additionally, it\nidentifies the practical challenges and limitations that slowdown the\ntransition from centralized to decentralized identity management systems. By\nshifting the focus from purely technological constraints to real-world\ndeployment issues, this survey identifies the underlying reasons preventing the\nadoption of decentralized identities despite their evident benefits to the data\nowner."
                },
                "authors": [
                    {
                        "name": "Daria Schumm"
                    },
                    {
                        "name": "Katharina O. E. Müller"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15952v1",
                "updated": "2025-03-20T08:48:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    48,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:48:57Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    48,
                    57,
                    3,
                    79,
                    0
                ],
                "title": "Adaptive Group Policy Optimization: Towards Stable Training and\n  Token-Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Group Policy Optimization: Towards Stable Training and\n  Token-Efficient Reasoning"
                },
                "summary": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Nazhou Liu"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "This is an unfinished version and will be updated. We aim to share\n  some findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15947v1",
                "updated": "2025-03-20T08:40:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    40,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:40:41Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    40,
                    41,
                    3,
                    79,
                    0
                ],
                "title": "Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent\n  Reinforcement Learning"
                },
                "summary": "In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL\ngeneral platform based on the Unreal-Engine (UE). Unreal-MAP allows users to\nfreely create multi-agent tasks using the vast visual and physical resources\navailable in the UE community, and deploy state-of-the-art (SOTA) MARL\nalgorithms within them. Unreal-MAP is user-friendly in terms of deployment,\nmodification, and visualization, and all its components are open-source. We\nalso develop an experimental framework compatible with algorithms ranging from\nrule-based to learning-based provided by third-party frameworks. Lastly, we\ndeploy several SOTA algorithms in example tasks developed via Unreal-MAP, and\nconduct corresponding experimental analyses. We believe Unreal-MAP can play an\nimportant role in the MARL field by closely integrating existing algorithms\nwith user-customized tasks, thus advancing the field of MARL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL\ngeneral platform based on the Unreal-Engine (UE). Unreal-MAP allows users to\nfreely create multi-agent tasks using the vast visual and physical resources\navailable in the UE community, and deploy state-of-the-art (SOTA) MARL\nalgorithms within them. Unreal-MAP is user-friendly in terms of deployment,\nmodification, and visualization, and all its components are open-source. We\nalso develop an experimental framework compatible with algorithms ranging from\nrule-based to learning-based provided by third-party frameworks. Lastly, we\ndeploy several SOTA algorithms in example tasks developed via Unreal-MAP, and\nconduct corresponding experimental analyses. We believe Unreal-MAP can play an\nimportant role in the MARL field by closely integrating existing algorithms\nwith user-customized tasks, thus advancing the field of MARL."
                },
                "authors": [
                    {
                        "name": "Tianyi Hu"
                    },
                    {
                        "name": "Qingxu Fu"
                    },
                    {
                        "name": "Zhiqiang Pu"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Tenghai Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Tenghai Qiu"
                },
                "author": "Tenghai Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15944v1",
                "updated": "2025-03-20T08:34:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    34,
                    53,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:34:53Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    34,
                    53,
                    3,
                    79,
                    0
                ],
                "title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained\n  Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained\n  Reasoning in Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation."
                },
                "authors": [
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Rong Cheng"
                    },
                    {
                        "name": "Qiyu Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Fei Ni"
                    },
                    {
                        "name": "Hebin Liang"
                    },
                    {
                        "name": "Yifu Yuan"
                    },
                    {
                        "name": "Hangyu Mao"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07930v2",
                "updated": "2025-03-20T08:31:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    31,
                    58,
                    3,
                    79,
                    0
                ],
                "published": "2025-01-14T08:32:12Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    32,
                    12,
                    1,
                    14,
                    0
                ],
                "title": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN\n  Architectures"
                },
                "summary": "Orthogonal convolutional layers are the workhorse of multiple areas in\nmachine learning, such as adversarial robustness, normalizing flows, GANs, and\nLipschitzconstrained models. Their ability to preserve norms and ensure stable\ngradient propagation makes them valuable for a large range of problems. Despite\ntheir promise, the deployment of orthogonal convolution in large-scale\napplications is a significant challenge due to computational overhead and\nlimited support for modern features like strides, dilations, group\nconvolutions, and transposed convolutions.In this paper, we introduce AOC\n(Adaptative Orthogonal Convolution), a scalable method for constructing\northogonal convolutions, effectively overcoming these limitations. This\nadvancement unlocks the construction of architectures that were previously\nconsidered impractical. We demonstrate through our experiments that our method\nproduces expressive models that become increasingly efficient as they scale. To\nfoster further advancement, we provide an open-source library implementing this\nmethod, available at https://github.com/thib-s/orthogonium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal convolutional layers are the workhorse of multiple areas in\nmachine learning, such as adversarial robustness, normalizing flows, GANs, and\nLipschitzconstrained models. Their ability to preserve norms and ensure stable\ngradient propagation makes them valuable for a large range of problems. Despite\ntheir promise, the deployment of orthogonal convolution in large-scale\napplications is a significant challenge due to computational overhead and\nlimited support for modern features like strides, dilations, group\nconvolutions, and transposed convolutions.In this paper, we introduce AOC\n(Adaptative Orthogonal Convolution), a scalable method for constructing\northogonal convolutions, effectively overcoming these limitations. This\nadvancement unlocks the construction of architectures that were previously\nconsidered impractical. We demonstrate through our experiments that our method\nproduces expressive models that become increasingly efficient as they scale. To\nfoster further advancement, we provide an open-source library implementing this\nmethod, available at https://github.com/thib-s/orthogonium."
                },
                "authors": [
                    {
                        "name": "Thibaut Boissin"
                    },
                    {
                        "name": "Franck Mamalet"
                    },
                    {
                        "name": "Thomas Fel"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Thomas Massena"
                    },
                    {
                        "name": "Mathieu Serrurier"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Serrurier"
                },
                "arxiv_affiliation": "IRIT, ANITI",
                "author": "Mathieu Serrurier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00101v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00101v3",
                "updated": "2025-03-20T08:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    26,
                    21,
                    3,
                    79,
                    0
                ],
                "published": "2024-08-27T12:07:09Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    7,
                    9,
                    1,
                    240,
                    0
                ],
                "title": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals"
                },
                "summary": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm."
                },
                "authors": [
                    {
                        "name": "Wei-Bang Jiang"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Bao-Liang Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00101v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00101v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15937v1",
                "updated": "2025-03-20T08:25:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    25,
                    0,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:25:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    25,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment"
                },
                "summary": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "14 pages, 4 itertions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15931v1",
                "updated": "2025-03-20T08:15:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    15,
                    29,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:15:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    15,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup\n  Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup\n  Tables"
                },
                "summary": "While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT."
                },
                "authors": [
                    {
                        "name": "Sidi Yang"
                    },
                    {
                        "name": "Binxiao Huang"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Dahai Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15924v1",
                "updated": "2025-03-20T08:00:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:00:41Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    0,
                    41,
                    3,
                    79,
                    0
                ],
                "title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for\n  Continual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automatic Continual Learning: A Self-Adaptive Framework for\n  Continual Instruction Tuning"
                },
                "summary": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning."
                },
                "authors": [
                    {
                        "name": "Peiyi Lin"
                    },
                    {
                        "name": "Fukai Zhang"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Hao Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fu"
                },
                "author": "Hao Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15921v1",
                "updated": "2025-03-20T07:57:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    57,
                    57,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T07:57:57Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    57,
                    57,
                    3,
                    79,
                    0
                ],
                "title": "SPIN: Accelerating Large Language Model Inference with Heterogeneous\n  Speculative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIN: Accelerating Large Language Model Inference with Heterogeneous\n  Speculative Models"
                },
                "summary": "Speculative decoding has been shown as an effective way to accelerate Large\nLanguage Model (LLM) inference by using a Small Speculative Model (SSM) to\ngenerate candidate tokens in a so-called speculation phase, which are\nsubsequently verified by the LLM in a verification phase. However, current\nstate-of-the-art speculative decoding approaches have three key limitations:\nhandling requests with varying difficulty using homogeneous SSMs, lack of\nrobust support for batch processing, and insufficient holistic optimization for\nboth speculation and verification phases. In this paper, we introduce SPIN, an\nefficient LLM inference serving system based on speculative decoding, designed\nto address these challenges through three main innovations. First, SPIN\nimproves token speculation by using multiple heterogeneous SSMs, with a\nlearning-based algorithm for SSM selection that operates without prior\nknowledge of request difficulty. Second, SPIN employs a request decomposition\nmethod to minimize batching overhead during LLM verification. Finally, SPIN\norchestrates speculation and verification phases by pipelining their executions\non GPUs to achieve further acceleration. Experimental results demonstrate that\nSPIN significantly outperforms state-of-the-art methods, achieving a\nperformance increase of approximately 2.28X.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has been shown as an effective way to accelerate Large\nLanguage Model (LLM) inference by using a Small Speculative Model (SSM) to\ngenerate candidate tokens in a so-called speculation phase, which are\nsubsequently verified by the LLM in a verification phase. However, current\nstate-of-the-art speculative decoding approaches have three key limitations:\nhandling requests with varying difficulty using homogeneous SSMs, lack of\nrobust support for batch processing, and insufficient holistic optimization for\nboth speculation and verification phases. In this paper, we introduce SPIN, an\nefficient LLM inference serving system based on speculative decoding, designed\nto address these challenges through three main innovations. First, SPIN\nimproves token speculation by using multiple heterogeneous SSMs, with a\nlearning-based algorithm for SSM selection that operates without prior\nknowledge of request difficulty. Second, SPIN employs a request decomposition\nmethod to minimize batching overhead during LLM verification. Finally, SPIN\norchestrates speculation and verification phases by pipelining their executions\non GPUs to achieve further acceleration. Experimental results demonstrate that\nSPIN significantly outperforms state-of-the-art methods, achieving a\nperformance increase of approximately 2.28X."
                },
                "authors": [
                    {
                        "name": "Fahao Chen"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Tom H. Luan"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Jing Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jing Deng"
                },
                "author": "Jing Deng",
                "arxiv_comment": "Accepted by INFOCOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14563v2",
                "updated": "2025-03-20T07:32:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    32,
                    39,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-18T07:45:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    45,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "Workflow for Safe-AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workflow for Safe-AI"
                },
                "summary": "The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment"
                },
                "authors": [
                    {
                        "name": "Suzana Veljanovska"
                    },
                    {
                        "name": "Hans Dermot Doran"
                    }
                ],
                "author_detail": {
                    "name": "Hans Dermot Doran"
                },
                "author": "Hans Dermot Doran",
                "arxiv_comment": "Embedded World Conference, Nuremberg, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15904v1",
                "updated": "2025-03-20T07:15:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    15,
                    45,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T07:15:45Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    15,
                    45,
                    3,
                    79,
                    0
                ],
                "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in\n  LLMs Through Open-Ended Storytelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Structured Prompts to Open Narratives: Measuring Gender Bias in\n  LLMs Through Open-Ended Storytelling"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes."
                },
                "authors": [
                    {
                        "name": "Evan Chen"
                    },
                    {
                        "name": "Run-Jun Zhan"
                    },
                    {
                        "name": "Yan-Bai Lin"
                    },
                    {
                        "name": "Hung-Hsuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hung-Hsuan Chen"
                },
                "author": "Hung-Hsuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02901v2",
                "updated": "2025-03-20T07:02:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    7,
                    2,
                    55,
                    3,
                    79,
                    0
                ],
                "published": "2024-03-05T12:11:07Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    12,
                    11,
                    7,
                    1,
                    65,
                    0
                ],
                "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization\n  with Exploration of LLM-Based Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization\n  with Exploration of LLM-Based Methods"
                },
                "summary": "Automatic Text Summarization (ATS), utilizing Natural Language Processing\n(NLP) algorithms, aims to create concise and accurate summaries, thereby\nsignificantly reducing the human effort required in processing large volumes of\ntext. ATS has drawn considerable interest in both academic and industrial\ncircles. Many studies have been conducted in the past to survey ATS methods;\nhowever, they generally lack practicality for real-world implementations, as\nthey often categorize previous methods from a theoretical standpoint. Moreover,\nthe advent of Large Language Models (LLMs) has altered conventional ATS\nmethods. In this survey, we aim to 1) provide a comprehensive overview of ATS\nfrom a ``Process-Oriented Schema'' perspective, which is best aligned with\nreal-world implementations; 2) comprehensively review the latest LLM-based ATS\nworks; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in\nthe literature. To the best of our knowledge, this is the first survey to\nspecifically investigate LLM-based ATS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Text Summarization (ATS), utilizing Natural Language Processing\n(NLP) algorithms, aims to create concise and accurate summaries, thereby\nsignificantly reducing the human effort required in processing large volumes of\ntext. ATS has drawn considerable interest in both academic and industrial\ncircles. Many studies have been conducted in the past to survey ATS methods;\nhowever, they generally lack practicality for real-world implementations, as\nthey often categorize previous methods from a theoretical standpoint. Moreover,\nthe advent of Large Language Models (LLMs) has altered conventional ATS\nmethods. In this survey, we aim to 1) provide a comprehensive overview of ATS\nfrom a ``Process-Oriented Schema'' perspective, which is best aligned with\nreal-world implementations; 2) comprehensively review the latest LLM-based ATS\nworks; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in\nthe literature. To the best of our knowledge, this is the first survey to\nspecifically investigate LLM-based ATS methods."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Hanlei Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jinghua Tan"
                    }
                ],
                "author_detail": {
                    "name": "Jinghua Tan"
                },
                "author": "Jinghua Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]