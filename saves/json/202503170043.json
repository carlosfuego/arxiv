[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v1",
                "updated": "2025-03-12T17:43:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v1",
                "updated": "2025-03-11T13:10:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05783v1",
                "updated": "2025-02-26T21:03:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T21:03:02Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "title": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity"
                },
                "summary": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues."
                },
                "authors": [
                    {
                        "name": "Eleftheria Katsiri"
                    },
                    {
                        "name": "Alexandros Gazis"
                    },
                    {
                        "name": "Angelos Protopapas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Protopapas"
                },
                "author": "Angelos Protopapas",
                "arxiv_comment": "23 pages, 11 figures, 76 references. This article is under review at\n  WSEAS Transactions on Information Science and Applications from 02.2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.10636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10636v2",
                "updated": "2025-03-14T06:35:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    35,
                    23,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation"
                },
                "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT"
                },
                "authors": [
                    {
                        "name": "Ho Kei Cheng"
                    },
                    {
                        "name": "Alexander Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Schwing"
                },
                "author": "Alexander Schwing",
                "arxiv_comment": "Project page: https://hkchengrex.github.io/C2OT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10637v1",
                "updated": "2025-03-13T17:59:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    56,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "Distilling Diversity and Control in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Diversity and Control in Diffusion Models"
                },
                "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info"
                },
                "authors": [
                    {
                        "name": "Rohit Gandikota"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Project Page: https://distillation.baulab.info",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10630v1",
                "updated": "2025-03-13T17:59:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:59:48Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"
                },
                "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Lingqing Zhao"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10625v1",
                "updated": "2025-03-13T17:59:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    21,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:59:21Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    21,
                    3,
                    72,
                    0
                ],
                "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds"
                },
                "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability."
                },
                "authors": [
                    {
                        "name": "Lingteng Qiu"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Peihao Li"
                    },
                    {
                        "name": "Qi Zuo"
                    },
                    {
                        "name": "Weichao Shen"
                    },
                    {
                        "name": "Junfei Zhang"
                    },
                    {
                        "name": "Kejie Qiu"
                    },
                    {
                        "name": "Weihao Yuan"
                    },
                    {
                        "name": "Guanying Chen"
                    },
                    {
                        "name": "Zilong Dong"
                    },
                    {
                        "name": "Liefeng Bo"
                    }
                ],
                "author_detail": {
                    "name": "Liefeng Bo"
                },
                "author": "Liefeng Bo",
                "arxiv_comment": "Project Page: https://lingtengqiu.github.io/LHM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10621v1",
                "updated": "2025-03-13T17:59:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    1,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:59:01Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    1,
                    3,
                    72,
                    0
                ],
                "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model\n  for Driving Scenario Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model\n  for Driving Scenario Understanding"
                },
                "summary": "While large multimodal models (LMMs) have demonstrated strong performance\nacross various Visual Question Answering (VQA) tasks, certain challenges\nrequire complex multi-step reasoning to reach accurate answers. One\nparticularly challenging task is autonomous driving, which demands thorough\ncognitive processing before decisions can be made. In this domain, a sequential\nand interpretive understanding of visual cues is essential for effective\nperception, prediction, and planning. Nevertheless, common VQA benchmarks often\nfocus on the accuracy of the final answer while overlooking the reasoning\nprocess that enables the generation of accurate responses. Moreover, existing\nmethods lack a comprehensive framework for evaluating step-by-step reasoning in\nrealistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new\ndataset and benchmark specifically designed to advance step-wise visual\nreasoning for autonomous driving. Our benchmark features over 18k VQA examples\nin the training set and more than 4k in the test set, covering diverse\nquestions on perception, prediction, and planning, each enriched with\nstep-by-step reasoning to ensure logical inference in autonomous driving\nscenarios. We further introduce a large multimodal model that is fine-tuned on\nour reasoning dataset, demonstrating robust performance in complex driving\nscenarios. In addition, we benchmark various open-source and closed-source\nmethods on our proposed dataset, systematically comparing their reasoning\ncapabilities for autonomous driving tasks. Our model achieves a +7.49% gain in\nfinal answer accuracy, along with a 3.62% improvement in reasoning score over\nthe previous best open-source model. Our framework, dataset, and model are\navailable at https://github.com/ayesha-ishaq/DriveLMM-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large multimodal models (LMMs) have demonstrated strong performance\nacross various Visual Question Answering (VQA) tasks, certain challenges\nrequire complex multi-step reasoning to reach accurate answers. One\nparticularly challenging task is autonomous driving, which demands thorough\ncognitive processing before decisions can be made. In this domain, a sequential\nand interpretive understanding of visual cues is essential for effective\nperception, prediction, and planning. Nevertheless, common VQA benchmarks often\nfocus on the accuracy of the final answer while overlooking the reasoning\nprocess that enables the generation of accurate responses. Moreover, existing\nmethods lack a comprehensive framework for evaluating step-by-step reasoning in\nrealistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new\ndataset and benchmark specifically designed to advance step-wise visual\nreasoning for autonomous driving. Our benchmark features over 18k VQA examples\nin the training set and more than 4k in the test set, covering diverse\nquestions on perception, prediction, and planning, each enriched with\nstep-by-step reasoning to ensure logical inference in autonomous driving\nscenarios. We further introduce a large multimodal model that is fine-tuned on\nour reasoning dataset, demonstrating robust performance in complex driving\nscenarios. In addition, we benchmark various open-source and closed-source\nmethods on our proposed dataset, systematically comparing their reasoning\ncapabilities for autonomous driving tasks. Our model achieves a +7.49% gain in\nfinal answer accuracy, along with a 3.62% improvement in reasoning score over\nthe previous best open-source model. Our framework, dataset, and model are\navailable at https://github.com/ayesha-ishaq/DriveLMM-o1."
                },
                "authors": [
                    {
                        "name": "Ayesha Ishaq"
                    },
                    {
                        "name": "Jean Lahoud"
                    },
                    {
                        "name": "Ketan More"
                    },
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Ritesh Thawkar"
                    },
                    {
                        "name": "Dinura Dissanayake"
                    },
                    {
                        "name": "Noor Ahsan"
                    },
                    {
                        "name": "Yuhao Li"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "8 pages, 4 figures, 3 tables, github:\n  https://github.com/ayesha-ishaq/DriveLMM-o1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10619v1",
                "updated": "2025-03-13T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search"
                },
                "summary": "We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Andy Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Andy Zhou"
                },
                "author": "Andy Zhou",
                "arxiv_comment": "Accepted to ICLR 2025 Trustworthy LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10620v1",
                "updated": "2025-03-13T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community."
                },
                "authors": [
                    {
                        "name": "Kshitij Ambilduke"
                    },
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Sonal Sannigrahi"
                    },
                    {
                        "name": "Anil Keshwani"
                    },
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "Marcely Zanon Boito"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10616v1",
                "updated": "2025-03-13T17:56:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    56,
                    10,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:56:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    56,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with\n  Transformer"
                },
                "summary": "Open-vocabulary multiple object tracking aims to generalize trackers to\nunseen categories during training, enabling their application across a variety\nof real-world scenarios. However, the existing open-vocabulary tracker is\nconstrained by its framework structure, isolated frame-level perception, and\ninsufficient modal interactions, which hinder its performance in\nopen-vocabulary classification and tracking. In this paper, we propose OVTR\n(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the\nfirst end-to-end open-vocabulary tracker that models motion, appearance, and\ncategory simultaneously. To achieve stable classification and continuous\ntracking, we design the CIP (Category Information Propagation) strategy, which\nestablishes multiple high-level category information priors for subsequent\nframes. Additionally, we introduce a dual-branch structure for generalization\ncapability and deep multimodal interaction, and incorporate protective\nstrategies in the decoder to enhance performance. Experimental results show\nthat our method surpasses previous trackers on the open-vocabulary MOT\nbenchmark while also achieving faster inference speeds and significantly\nreducing preprocessing requirements. Moreover, the experiment transferring the\nmodel to another dataset demonstrates its strong adaptability. Models and code\nare released at https://github.com/jinyanglii/OVTR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary multiple object tracking aims to generalize trackers to\nunseen categories during training, enabling their application across a variety\nof real-world scenarios. However, the existing open-vocabulary tracker is\nconstrained by its framework structure, isolated frame-level perception, and\ninsufficient modal interactions, which hinder its performance in\nopen-vocabulary classification and tracking. In this paper, we propose OVTR\n(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the\nfirst end-to-end open-vocabulary tracker that models motion, appearance, and\ncategory simultaneously. To achieve stable classification and continuous\ntracking, we design the CIP (Category Information Propagation) strategy, which\nestablishes multiple high-level category information priors for subsequent\nframes. Additionally, we introduce a dual-branch structure for generalization\ncapability and deep multimodal interaction, and incorporate protective\nstrategies in the decoder to enhance performance. Experimental results show\nthat our method surpasses previous trackers on the open-vocabulary MOT\nbenchmark while also achieving faster inference speeds and significantly\nreducing preprocessing requirements. Moreover, the experiment transferring the\nmodel to another dataset demonstrates its strong adaptability. Models and code\nare released at https://github.com/jinyanglii/OVTR."
                },
                "authors": [
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Wenbing Tao"
                    }
                ],
                "author_detail": {
                    "name": "Wenbing Tao"
                },
                "author": "Wenbing Tao",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10614v1",
                "updated": "2025-03-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    58,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:55:58Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    58,
                    3,
                    72,
                    0
                ],
                "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer"
                },
                "summary": "Style transfer involves transferring the style from a reference image to the\ncontent of a target image. Recent advancements in LoRA-based (Low-Rank\nAdaptation) methods have shown promise in effectively capturing the style of a\nsingle image. However, these approaches still face significant challenges such\nas content inconsistency, style misalignment, and content leakage. In this\npaper, we comprehensively analyze the limitations of the standard diffusion\nparameterization, which learns to predict noise, in the context of style\ntransfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method\nthat enhances both content and style consistency by optimizing the LoRA weights\nto predict the original image rather than noise. We also propose a two-step\ntraining strategy that decouples the learning of content and style from the\nreference image. To effectively capture both the global structure and local\ndetails of the content image, we introduce a stepwise loss transition strategy.\nAdditionally, we present an inference guidance method that enables continuous\ncontrol over content and style strengths during inference. Through both\nqualitative and quantitative evaluations, our method demonstrates significant\nimprovements in content and style consistency while effectively reducing\ncontent leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style transfer involves transferring the style from a reference image to the\ncontent of a target image. Recent advancements in LoRA-based (Low-Rank\nAdaptation) methods have shown promise in effectively capturing the style of a\nsingle image. However, these approaches still face significant challenges such\nas content inconsistency, style misalignment, and content leakage. In this\npaper, we comprehensively analyze the limitations of the standard diffusion\nparameterization, which learns to predict noise, in the context of style\ntransfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method\nthat enhances both content and style consistency by optimizing the LoRA weights\nto predict the original image rather than noise. We also propose a two-step\ntraining strategy that decouples the learning of content and style from the\nreference image. To effectively capture both the global structure and local\ndetails of the content image, we introduce a stepwise loss transition strategy.\nAdditionally, we present an inference guidance method that enables continuous\ncontrol over content and style strengths during inference. Through both\nqualitative and quantitative evaluations, our method demonstrates significant\nimprovements in content and style consistency while effectively reducing\ncontent leakage."
                },
                "authors": [
                    {
                        "name": "Bolin Chen"
                    },
                    {
                        "name": "Baoquan Zhao"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "Yi Cai"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Xudong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Mao"
                },
                "author": "Xudong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10613v1",
                "updated": "2025-03-13T17:55:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:55:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing"
                },
                "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference."
                },
                "authors": [
                    {
                        "name": "Advait Gupta"
                    },
                    {
                        "name": "NandaKiran Velaga"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10608v1",
                "updated": "2025-03-13T17:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    52,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    52,
                    43,
                    3,
                    72,
                    0
                ],
                "title": "Hierarchical Bayesian inference for uncertainty quantification of\n  thermal grease rheology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Bayesian inference for uncertainty quantification of\n  thermal grease rheology"
                },
                "summary": "Rheologically complex soft solids such as thermal greases consist of filler\nparticles within a polymer matrix. These materials find applications in\nimproving the conformity of solid-solid contacts and enhancing heat transfer.\nComplex soft solids exhibit a transient non-Newtonian rheological response,\nincluding thixotropy and viscoelasticity. Previously, stress relaxation and\nbuildup in sheared commercial thermal greases were successfully captured using\na nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic\n(TEVP). However, the previous model calibration methods ignored parameter\nuncertainty, providing only single values of the rheological parameters, and\ndid not quantitatively address the chosen model's identifiability from the data\nor credibility of the calibration. We address these limitations via\nhierarchical Bayesian inference, accounting for uncertainties arising from\nepistemic and aleatoric sources. Importantly, the hierarchical approach allows\nus to assimilate experiments measuring the stress responses at various startup\nshear rates by allowing the models' parameters to vary across different shear\nrates. Then, a global distribution and the associated uncertainty are obtained\nby pooling. We also propagate uncertainties to the transient shear stress\nresponse predicted by the models. Overall, we demonstrate that the chosen NEVP\nand NEVP models are identifiable from rheometric startup data. However, for the\nTEVP model, the uncertainty of the parameters is lower (narrower distributions)\nwhen higher shear rates are used for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheologically complex soft solids such as thermal greases consist of filler\nparticles within a polymer matrix. These materials find applications in\nimproving the conformity of solid-solid contacts and enhancing heat transfer.\nComplex soft solids exhibit a transient non-Newtonian rheological response,\nincluding thixotropy and viscoelasticity. Previously, stress relaxation and\nbuildup in sheared commercial thermal greases were successfully captured using\na nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic\n(TEVP). However, the previous model calibration methods ignored parameter\nuncertainty, providing only single values of the rheological parameters, and\ndid not quantitatively address the chosen model's identifiability from the data\nor credibility of the calibration. We address these limitations via\nhierarchical Bayesian inference, accounting for uncertainties arising from\nepistemic and aleatoric sources. Importantly, the hierarchical approach allows\nus to assimilate experiments measuring the stress responses at various startup\nshear rates by allowing the models' parameters to vary across different shear\nrates. Then, a global distribution and the associated uncertainty are obtained\nby pooling. We also propagate uncertainties to the transient shear stress\nresponse predicted by the models. Overall, we demonstrate that the chosen NEVP\nand NEVP models are identifiable from rheometric startup data. However, for the\nTEVP model, the uncertainty of the parameters is lower (narrower distributions)\nwhen higher shear rates are used for inference."
                },
                "authors": [
                    {
                        "name": "Pranay P. Nagrani"
                    },
                    {
                        "name": "Akshay J. Thomas"
                    },
                    {
                        "name": "Amy M. Marconnet"
                    },
                    {
                        "name": "Ivan C. Christov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan C. Christov"
                },
                "author": "Ivan C. Christov",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10602v1",
                "updated": "2025-03-13T17:46:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:46:06Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention"
                },
                "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "15 pages, 9 figures, the first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10591v1",
                "updated": "2025-03-13T17:40:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "Analysis and sample-size determination for $2^K$ audit experiments with\n  binary response and application to identification of effect of racial\n  discrimination on access to justice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and sample-size determination for $2^K$ audit experiments with\n  binary response and application to identification of effect of racial\n  discrimination on access to justice"
                },
                "summary": "Social scientists have increasingly turned to audit experiments to\ninvestigate discrimination in the market for jobs, loans, housing and other\nopportunities. In a typical audit experiment, researchers assign ``signals''\n(the treatment) to subjects at random and compare success rates across\ntreatment conditions. In the recent past there has been increased interest in\nusing randomized multifactor designs for audit experiments, popularly called\nfactorial experiments, in which combinations of multiple signals are assigned\nto subjects. Although social scientists have manipulated multiple factors like\nrace, gender and income, the analyses have been mostly exploratory in nature.\nIn this paper we lay out a comprehensive methodology for design and analysis of\n$2^K$ factorial designs with binary response using model-free,\nrandomization-based Neymanian inference and demonstrate its application by\nanalyzing the audit experiment reported in Libgober (2020). Specifically, we\nintegrate and extend several sections of the randomization-based,\nfinite-population literature for binary outcomes, including sample size and\npower calculations, and non-linear factorial estimators, extending results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social scientists have increasingly turned to audit experiments to\ninvestigate discrimination in the market for jobs, loans, housing and other\nopportunities. In a typical audit experiment, researchers assign ``signals''\n(the treatment) to subjects at random and compare success rates across\ntreatment conditions. In the recent past there has been increased interest in\nusing randomized multifactor designs for audit experiments, popularly called\nfactorial experiments, in which combinations of multiple signals are assigned\nto subjects. Although social scientists have manipulated multiple factors like\nrace, gender and income, the analyses have been mostly exploratory in nature.\nIn this paper we lay out a comprehensive methodology for design and analysis of\n$2^K$ factorial designs with binary response using model-free,\nrandomization-based Neymanian inference and demonstrate its application by\nanalyzing the audit experiment reported in Libgober (2020). Specifically, we\nintegrate and extend several sections of the randomization-based,\nfinite-population literature for binary outcomes, including sample size and\npower calculations, and non-linear factorial estimators, extending results."
                },
                "authors": [
                    {
                        "name": "Nicole Pashley"
                    },
                    {
                        "name": "Brian Libgober"
                    },
                    {
                        "name": "Tirthankar Dasgupta"
                    }
                ],
                "author_detail": {
                    "name": "Tirthankar Dasgupta"
                },
                "author": "Tirthankar Dasgupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18466v2",
                "updated": "2025-03-13T17:39:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    39,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-27T15:58:07Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    58,
                    7,
                    2,
                    332,
                    0
                ],
                "title": "Complexity Experts are Task-Discriminative Learners for Any Image\n  Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity Experts are Task-Discriminative Learners for Any Image\n  Restoration"
                },
                "summary": "Recent advancements in all-in-one image restoration models have\nrevolutionized the ability to address diverse degradations through a unified\nframework. However, parameters tied to specific tasks often remain inactive for\nother tasks, making mixture-of-experts (MoE) architectures a natural extension.\nDespite this, MoEs often show inconsistent behavior, with some experts\nunexpectedly generalizing across tasks while others struggle within their\nintended scope. This hinders leveraging MoEs' computational benefits by\nbypassing irrelevant experts during inference. We attribute this undesired\nbehavior to the uniform and rigid architecture of traditional MoEs. To address\nthis, we introduce ``complexity experts\" -- flexible expert blocks with varying\ncomputational complexity and receptive fields. A key challenge is assigning\ntasks to each expert, as degradation complexity is unknown in advance. Thus, we\nexecute tasks with a simple bias toward lower complexity. To our surprise, this\npreference effectively drives task-specific allocation, assigning tasks to\nexperts with the appropriate complexity. Extensive experiments validate our\napproach, demonstrating the ability to bypass irrelevant experts during\ninference while maintaining superior performance. The proposed MoCE-IR model\noutperforms state-of-the-art methods, affirming its efficiency and practical\napplicability. The source code and models are publicly available at\n\\href{https://eduardzamfir.github.io/moceir/}{\\texttt{eduardzamfir.github.io/MoCE-IR/}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in all-in-one image restoration models have\nrevolutionized the ability to address diverse degradations through a unified\nframework. However, parameters tied to specific tasks often remain inactive for\nother tasks, making mixture-of-experts (MoE) architectures a natural extension.\nDespite this, MoEs often show inconsistent behavior, with some experts\nunexpectedly generalizing across tasks while others struggle within their\nintended scope. This hinders leveraging MoEs' computational benefits by\nbypassing irrelevant experts during inference. We attribute this undesired\nbehavior to the uniform and rigid architecture of traditional MoEs. To address\nthis, we introduce ``complexity experts\" -- flexible expert blocks with varying\ncomputational complexity and receptive fields. A key challenge is assigning\ntasks to each expert, as degradation complexity is unknown in advance. Thus, we\nexecute tasks with a simple bias toward lower complexity. To our surprise, this\npreference effectively drives task-specific allocation, assigning tasks to\nexperts with the appropriate complexity. Extensive experiments validate our\napproach, demonstrating the ability to bypass irrelevant experts during\ninference while maintaining superior performance. The proposed MoCE-IR model\noutperforms state-of-the-art methods, affirming its efficiency and practical\napplicability. The source code and models are publicly available at\n\\href{https://eduardzamfir.github.io/moceir/}{\\texttt{eduardzamfir.github.io/MoCE-IR/}}"
                },
                "authors": [
                    {
                        "name": "Eduard Zamfir"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Yuedong Tan"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "Accepted at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06215v3",
                "updated": "2025-03-13T17:30:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    30,
                    48,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-08T17:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    20,
                    37,
                    1,
                    282,
                    0
                ],
                "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback"
                },
                "summary": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "ICLR 2025 Spotlight; Project Page: https://DataEnvGym.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10573v1",
                "updated": "2025-03-13T17:23:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative\n  Study of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative\n  Study of Large Language Models"
                },
                "summary": "With the rapid evolution of Artificial Intelligence (AI), Large Language\nModels (LLMs) have reshaped the frontiers of various fields, spanning\nhealthcare, public health, engineering, science, agriculture, education, arts,\nhumanities, and mathematical reasoning. Among these advancements, DeepSeek\nmodels have emerged as noteworthy contenders, demonstrating promising\ncapabilities that set them apart from their peers. While previous studies have\nconducted comparative analyses of LLMs, few have delivered a comprehensive\nevaluation of mathematical reasoning across a broad spectrum of LLMs. In this\nwork, we aim to bridge this gap by conducting an in-depth comparative study,\nfocusing on the strengths and limitations of DeepSeek models in relation to\ntheir leading counterparts. In particular, our study systematically evaluates\nthe mathematical reasoning performance of two DeepSeek models alongside five\nprominent LLMs across three independent benchmark datasets. The findings reveal\nseveral key insights: 1). DeepSeek-R1 consistently achieved the highest\naccuracy on two of the three datasets, demonstrating strong mathematical\nreasoning capabilities. 2). The distilled variant of LLMs significantly\nunderperformed compared to its peers, highlighting potential drawbacks in using\ndistillation techniques. 3). In terms of response time, Gemini 2.0 Flash\ndemonstrated the fastest processing speed, outperforming other models in\nefficiency, which is a crucial factor for real-time applications. Beyond these\nquantitative assessments, we delve into how architecture, training, and\noptimization impact LLMs' mathematical reasoning. Moreover, our study goes\nbeyond mere performance comparison by identifying key areas for future\nadvancements in LLM-driven mathematical reasoning. This research enhances our\nunderstanding of LLMs' mathematical reasoning and lays the groundwork for\nfuture advancements",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of Artificial Intelligence (AI), Large Language\nModels (LLMs) have reshaped the frontiers of various fields, spanning\nhealthcare, public health, engineering, science, agriculture, education, arts,\nhumanities, and mathematical reasoning. Among these advancements, DeepSeek\nmodels have emerged as noteworthy contenders, demonstrating promising\ncapabilities that set them apart from their peers. While previous studies have\nconducted comparative analyses of LLMs, few have delivered a comprehensive\nevaluation of mathematical reasoning across a broad spectrum of LLMs. In this\nwork, we aim to bridge this gap by conducting an in-depth comparative study,\nfocusing on the strengths and limitations of DeepSeek models in relation to\ntheir leading counterparts. In particular, our study systematically evaluates\nthe mathematical reasoning performance of two DeepSeek models alongside five\nprominent LLMs across three independent benchmark datasets. The findings reveal\nseveral key insights: 1). DeepSeek-R1 consistently achieved the highest\naccuracy on two of the three datasets, demonstrating strong mathematical\nreasoning capabilities. 2). The distilled variant of LLMs significantly\nunderperformed compared to its peers, highlighting potential drawbacks in using\ndistillation techniques. 3). In terms of response time, Gemini 2.0 Flash\ndemonstrated the fastest processing speed, outperforming other models in\nefficiency, which is a crucial factor for real-time applications. Beyond these\nquantitative assessments, we delve into how architecture, training, and\noptimization impact LLMs' mathematical reasoning. Moreover, our study goes\nbeyond mere performance comparison by identifying key areas for future\nadvancements in LLM-driven mathematical reasoning. This research enhances our\nunderstanding of LLMs' mathematical reasoning and lays the groundwork for\nfuture advancements"
                },
                "authors": [
                    {
                        "name": "Afrar Jahin"
                    },
                    {
                        "name": "Arif Hassan Zidan"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Shizhe Liang"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10571v1",
                "updated": "2025-03-13T17:23:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    10,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:23:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "Radar: Fast Long-Context Decoding for Any Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radar: Fast Long-Context Decoding for Any Transformer"
                },
                "summary": "Transformer models have demonstrated exceptional performance across a wide\nrange of applications. Though forming the foundation of Transformer models, the\ndot-product attention does not scale well to long-context data since its time\nrequirement grows quadratically with context length. In this work, we propose\nRadar, a training-free approach that accelerates inference by dynamically\nsearching for the most important context tokens. For any pre-trained\nTransformer, Radar can reduce the decoding time complexity without training or\nheuristically evicting tokens. Moreover, we provide theoretical justification\nfor our approach, demonstrating that Radar can reliably identify the most\nimportant tokens with high probability. We conduct extensive comparisons with\nthe previous methods on a wide range of tasks. The results demonstrate that\nRadar achieves the state-of-the-art performance across different architectures\nwith reduced time complexity, offering a practical solution for efficient\nlong-context processing of Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have demonstrated exceptional performance across a wide\nrange of applications. Though forming the foundation of Transformer models, the\ndot-product attention does not scale well to long-context data since its time\nrequirement grows quadratically with context length. In this work, we propose\nRadar, a training-free approach that accelerates inference by dynamically\nsearching for the most important context tokens. For any pre-trained\nTransformer, Radar can reduce the decoding time complexity without training or\nheuristically evicting tokens. Moreover, we provide theoretical justification\nfor our approach, demonstrating that Radar can reliably identify the most\nimportant tokens with high probability. We conduct extensive comparisons with\nthe previous methods on a wide range of tasks. The results demonstrate that\nRadar achieves the state-of-the-art performance across different architectures\nwith reduced time complexity, offering a practical solution for efficient\nlong-context processing of Transformers."
                },
                "authors": [
                    {
                        "name": "Yongchang Hao"
                    },
                    {
                        "name": "Mengyao Zhai"
                    },
                    {
                        "name": "Hossein Hajimirsadeghi"
                    },
                    {
                        "name": "Sepidehsadat Hosseini"
                    },
                    {
                        "name": "Frederick Tung"
                    }
                ],
                "author_detail": {
                    "name": "Frederick Tung"
                },
                "author": "Frederick Tung",
                "arxiv_comment": "Accepted @ ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09590v2",
                "updated": "2025-03-13T17:14:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    14,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering"
                },
                "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Torresani"
                },
                "author": "Lorenzo Torresani",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10546v1",
                "updated": "2025-03-13T16:59:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    59,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:59:17Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    59,
                    17,
                    3,
                    72,
                    0
                ],
                "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation"
                },
                "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io."
                },
                "authors": [
                    {
                        "name": "Zixian Liu"
                    },
                    {
                        "name": "Mingtong Zhang"
                    },
                    {
                        "name": "Yunzhu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Li"
                },
                "author": "Yunzhu Li",
                "arxiv_comment": "Project website: http://kuda-dynamics.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10544v1",
                "updated": "2025-03-13T16:58:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    58,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:58:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    58,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "DP-GPL: Differentially Private Graph Prompt Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-GPL: Differentially Private Graph Prompt Learning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in various\napplications. Recently, graph prompt learning has emerged as a powerful GNN\ntraining paradigm, inspired by advances in language and vision foundation\nmodels. Here, a GNN is pre-trained on public data and then adapted to sensitive\ntasks using lightweight graph prompts. However, using prompts from sensitive\ndata poses privacy risks. In this work, we are the first to investigate these\npractical risks in graph prompts by instantiating a membership inference attack\nthat reveals significant privacy leakage. We also find that the standard\nprivacy method, DP-SGD, fails to provide practical privacy-utility trade-offs\nin graph prompt learning, likely due to the small number of sensitive data\npoints used to learn the prompts. As a solution, we propose DP-GPL for\ndifferentially private graph prompt learning based on the PATE framework, that\ngenerates a graph prompt with differential privacy guarantees. Our evaluation\nacross various graph prompt learning methods, GNN architectures, and\npre-training strategies demonstrates that our algorithm achieves high utility\nat strong privacy, effectively mitigating privacy concerns while preserving the\npowerful capabilities of prompted GNNs as powerful foundation models in the\ngraph domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable performance in various\napplications. Recently, graph prompt learning has emerged as a powerful GNN\ntraining paradigm, inspired by advances in language and vision foundation\nmodels. Here, a GNN is pre-trained on public data and then adapted to sensitive\ntasks using lightweight graph prompts. However, using prompts from sensitive\ndata poses privacy risks. In this work, we are the first to investigate these\npractical risks in graph prompts by instantiating a membership inference attack\nthat reveals significant privacy leakage. We also find that the standard\nprivacy method, DP-SGD, fails to provide practical privacy-utility trade-offs\nin graph prompt learning, likely due to the small number of sensitive data\npoints used to learn the prompts. As a solution, we propose DP-GPL for\ndifferentially private graph prompt learning based on the PATE framework, that\ngenerates a graph prompt with differential privacy guarantees. Our evaluation\nacross various graph prompt learning methods, GNN architectures, and\npre-training strategies demonstrates that our algorithm achieves high utility\nat strong privacy, effectively mitigating privacy concerns while preserving the\npowerful capabilities of prompted GNNs as powerful foundation models in the\ngraph domain."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Iyiola Emmanuel Olatunji"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10515v1",
                "updated": "2025-03-13T16:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    20,
                    25,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    20,
                    25,
                    3,
                    72,
                    0
                ],
                "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set"
                },
                "summary": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses."
                },
                "authors": [
                    {
                        "name": "Florian Eichin"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "arxiv_comment": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05039v2",
                "updated": "2025-03-13T16:17:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    17,
                    21,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-06T17:58:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    58,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification"
                },
                "summary": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06846v4",
                "updated": "2025-03-13T16:17:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    17,
                    19,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-09T13:06:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity"
                },
                "summary": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "18 pages, 5 figures; ICLR 2025 camera ready. Code:\n  https://github.com/idiap/linearize-distill-pretrained-transformers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10512v1",
                "updated": "2025-03-13T16:16:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    16,
                    23,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:16:23Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    16,
                    23,
                    3,
                    72,
                    0
                ],
                "title": "Conformal Prediction Sets for Deep Generative Models via Reduction to\n  Conformal Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction Sets for Deep Generative Models via Reduction to\n  Conformal Regression"
                },
                "summary": "We consider the problem of generating valid and small prediction sets by\nsampling outputs (e.g., software code and natural language text) from a\nblack-box deep generative model for a given input (e.g., textual prompt). The\nvalidity of a prediction set is determined by a user-defined binary\nadmissibility function depending on the target application. For example,\nrequiring at least one program in the set to pass all test cases in code\ngeneration application. To address this problem, we develop a simple and\neffective conformal inference algorithm referred to as Generative Prediction\nSets (GPS). Given a set of calibration examples and black-box access to a deep\ngenerative model, GPS can generate prediction sets with provable guarantees.\nThe key insight behind GPS is to exploit the inherent structure within the\ndistribution over the minimum number of samples needed to obtain an admissible\noutput to develop a simple conformal regression approach over the minimum\nnumber of samples. Experiments on multiple datasets for code and math word\nproblems using different large language models demonstrate the efficacy of GPS\nover state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of generating valid and small prediction sets by\nsampling outputs (e.g., software code and natural language text) from a\nblack-box deep generative model for a given input (e.g., textual prompt). The\nvalidity of a prediction set is determined by a user-defined binary\nadmissibility function depending on the target application. For example,\nrequiring at least one program in the set to pass all test cases in code\ngeneration application. To address this problem, we develop a simple and\neffective conformal inference algorithm referred to as Generative Prediction\nSets (GPS). Given a set of calibration examples and black-box access to a deep\ngenerative model, GPS can generate prediction sets with provable guarantees.\nThe key insight behind GPS is to exploit the inherent structure within the\ndistribution over the minimum number of samples needed to obtain an admissible\noutput to develop a simple conformal regression approach over the minimum\nnumber of samples. Experiments on multiple datasets for code and math word\nproblems using different large language models demonstrate the efficacy of GPS\nover state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Hooman Shahrokhi"
                    },
                    {
                        "name": "Devjeet Raj Roy"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Venera Arnaoudova"
                    },
                    {
                        "name": "Janaradhan Rao Doppa"
                    }
                ],
                "author_detail": {
                    "name": "Janaradhan Rao Doppa"
                },
                "author": "Janaradhan Rao Doppa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13640v2",
                "updated": "2025-03-13T16:16:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    16,
                    12,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-17T15:09:24Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    9,
                    24,
                    3,
                    291,
                    0
                ],
                "title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation"
                },
                "summary": "LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v2",
                "updated": "2025-03-13T16:11:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    11,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10509v1",
                "updated": "2025-03-13T16:10:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models"
                },
                "summary": "Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks."
                },
                "authors": [
                    {
                        "name": "Sahar Admoni"
                    },
                    {
                        "name": "Omer Ben-Porat"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10497v1",
                "updated": "2025-03-13T15:59:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    59,
                    20,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:59:20Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    59,
                    20,
                    3,
                    72,
                    0
                ],
                "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation"
                },
                "summary": "Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities."
                },
                "authors": [
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Junjue Wang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Edison Marrese-Taylor"
                    },
                    {
                        "name": "Shijian Lu"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10486v1",
                "updated": "2025-03-13T15:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    54,
                    26,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    54,
                    26,
                    3,
                    72,
                    0
                ],
                "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare."
                },
                "authors": [
                    {
                        "name": "Gaurav Kumar Gupta"
                    },
                    {
                        "name": "Pranal Pande"
                    }
                ],
                "author_detail": {
                    "name": "Pranal Pande"
                },
                "author": "Pranal Pande",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10480v1",
                "updated": "2025-03-13T15:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    49,
                    56,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    49,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning"
                },
                "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Shiduo Zhang"
                    },
                    {
                        "name": "Panpan Cai"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10470v1",
                "updated": "2025-03-13T15:42:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:42:44Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    44,
                    3,
                    72,
                    0
                ],
                "title": "Statistical Analysis of Sentence Structures through ASCII, Lexical\n  Alignment and PCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Analysis of Sentence Structures through ASCII, Lexical\n  Alignment and PCA"
                },
                "summary": "While utilizing syntactic tools such as parts-of-speech (POS) tagging has\nhelped us understand sentence structures and their distribution across diverse\ncorpora, it is quite complex and poses a challenge in natural language\nprocessing (NLP). This study focuses on understanding sentence structure\nbalance - usages of nouns, verbs, determiners, etc - harmoniously without\nrelying on such tools. It proposes a novel statistical method that uses\nAmerican Standard Code for Information Interchange (ASCII) codes to represent\ntext of 11 text corpora from various sources and their lexical category\nalignment after using their compressed versions through PCA, and analyzes the\nresults through histograms and normality tests such as Shapiro-Wilk and\nAnderson-Darling Tests. By focusing on ASCII codes, this approach simplifies\ntext processing, although not replacing any syntactic tools but complementing\nthem by offering it as a resource-efficient tool for assessing text balance.\nThe story generated by Grok shows near normality indicating balanced sentence\nstructures in LLM outputs, whereas 4 out of the remaining 10 pass the normality\ntests. Further research could explore potential applications in text quality\nevaluation and style analysis with syntactic integration for more broader\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While utilizing syntactic tools such as parts-of-speech (POS) tagging has\nhelped us understand sentence structures and their distribution across diverse\ncorpora, it is quite complex and poses a challenge in natural language\nprocessing (NLP). This study focuses on understanding sentence structure\nbalance - usages of nouns, verbs, determiners, etc - harmoniously without\nrelying on such tools. It proposes a novel statistical method that uses\nAmerican Standard Code for Information Interchange (ASCII) codes to represent\ntext of 11 text corpora from various sources and their lexical category\nalignment after using their compressed versions through PCA, and analyzes the\nresults through histograms and normality tests such as Shapiro-Wilk and\nAnderson-Darling Tests. By focusing on ASCII codes, this approach simplifies\ntext processing, although not replacing any syntactic tools but complementing\nthem by offering it as a resource-efficient tool for assessing text balance.\nThe story generated by Grok shows near normality indicating balanced sentence\nstructures in LLM outputs, whereas 4 out of the remaining 10 pass the normality\ntests. Further research could explore potential applications in text quality\nevaluation and style analysis with syntactic integration for more broader\ntasks."
                },
                "authors": [
                    {
                        "name": "Abhijeet Sahdev"
                    }
                ],
                "author_detail": {
                    "name": "Abhijeet Sahdev"
                },
                "author": "Abhijeet Sahdev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19363v2",
                "updated": "2025-03-13T15:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-26T18:01:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "DataMan: Data Manager for Pre-training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMan: Data Manager for Pre-training Large Language Models"
                },
                "summary": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources."
                },
                "authors": [
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Yawen Zeng"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "ICLR2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10456v1",
                "updated": "2025-03-13T15:24:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    24,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:24:49Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    24,
                    49,
                    3,
                    72,
                    0
                ],
                "title": "Ionization memory of plasma emiters in a solar prominence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ionization memory of plasma emiters in a solar prominence"
                },
                "summary": "Aims. In the low-collisional, partially ionized plasma (PIP) of solar\nprominences, uncharged emitters might show different signatures of magnetic\nline broadening than charged emitters. We investigate if the widths of weak\nmetall emissions in prominences exceed the thermal line broadening by a\ndifferent amount for charged and for uncharged emitters.\n  Methods. We simultaneously observe five optically thin, weak metall lines in\nthe brightness center of a quiescent prominence and compare their observed\nwidths with the thermal broadening.\n  Results. The inferred non-thermal broadening of the metall lines does not\nindicate systematic differences between the uncharged Mg b2 and Na D1 and the\ncharged Fe II emitters, only Sr II is broader.\n  Conclusions. The additional line broadening of charged emitters is reasonably\nattributed to magnetic forces. That of uncharged emitters can then come from\ntheir temporary state as ion before recombination. Magnetically induced\nvelocities will retain some time after recombination. Modelling partially\nionized plasmas then requires consideration of a memory of previous ionization\nstates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. In the low-collisional, partially ionized plasma (PIP) of solar\nprominences, uncharged emitters might show different signatures of magnetic\nline broadening than charged emitters. We investigate if the widths of weak\nmetall emissions in prominences exceed the thermal line broadening by a\ndifferent amount for charged and for uncharged emitters.\n  Methods. We simultaneously observe five optically thin, weak metall lines in\nthe brightness center of a quiescent prominence and compare their observed\nwidths with the thermal broadening.\n  Results. The inferred non-thermal broadening of the metall lines does not\nindicate systematic differences between the uncharged Mg b2 and Na D1 and the\ncharged Fe II emitters, only Sr II is broader.\n  Conclusions. The additional line broadening of charged emitters is reasonably\nattributed to magnetic forces. That of uncharged emitters can then come from\ntheir temporary state as ion before recombination. Magnetically induced\nvelocities will retain some time after recombination. Modelling partially\nionized plasmas then requires consideration of a memory of previous ionization\nstates."
                },
                "authors": [
                    {
                        "name": "E. Wiehr"
                    },
                    {
                        "name": "H. Balthasar"
                    },
                    {
                        "name": "G. Stellmacher"
                    },
                    {
                        "name": "M. Bianda"
                    }
                ],
                "author_detail": {
                    "name": "M. Bianda"
                },
                "arxiv_affiliation": "Istituto ricerque solari Aldo e Cele Daccó",
                "author": "M. Bianda",
                "arxiv_comment": "5 pages, 4 figures, accepted for publication by Astronomy and\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00263v2",
                "updated": "2025-03-13T15:21:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    21,
                    36,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-30T22:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    21,
                    5,
                    0,
                    274,
                    0
                ],
                "title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation"
                },
                "summary": "Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP"
                },
                "authors": [
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Vinkle Srivastav"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Nicolas Padoy"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Padoy"
                },
                "author": "Nicolas Padoy",
                "arxiv_comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10452v1",
                "updated": "2025-03-13T15:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    18,
                    56,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    18,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code."
                },
                "authors": [
                    {
                        "name": "Wenhao Hu"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Chunchen Wei"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10448v1",
                "updated": "2025-03-13T15:13:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    13,
                    9,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:13:09Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    13,
                    9,
                    3,
                    72,
                    0
                ],
                "title": "Estimating relapse time distribution from longitudinal biomarker\n  trajectories using iterative regression and continuous time Markov processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating relapse time distribution from longitudinal biomarker\n  trajectories using iterative regression and continuous time Markov processes"
                },
                "summary": "Biomarker measurements obtained by blood sampling are often used as a\nnon-invasive means of monitoring tumour progression in cancer patients.\nDiseases evolve dynamically over time, and studying longitudinal observations\nof specific biomarkers can help to understand patients response to treatment\nand predict disease progression. We propose a novel iterative regression-based\nmethod to estimate changes in patients status within a cohort that includes\ncensored patients, and illustrate it on clinical data from myeloma cases. We\nformulate the relapse time estimation problem in the framework of Piecewise\nDeterministic Markov processes (PDMP), where the Euclidean component is a\nsurrogate biomarker for patient state. This approach enables continuous-time\nestimation of the status-change dates, which in turn allows for accurate\ninference of the relapse time distribution. A key challenge lies in the partial\nobservability of the process, a complexity that has been rarely addressed in\nprevious studies. . We evaluate the performance of our procedure through a\nsimulation study and compare it with different approaches. This work is a proof\nof concept on biomarker trajectories with simple behaviour, but our method can\neasily be extended to more complex dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomarker measurements obtained by blood sampling are often used as a\nnon-invasive means of monitoring tumour progression in cancer patients.\nDiseases evolve dynamically over time, and studying longitudinal observations\nof specific biomarkers can help to understand patients response to treatment\nand predict disease progression. We propose a novel iterative regression-based\nmethod to estimate changes in patients status within a cohort that includes\ncensored patients, and illustrate it on clinical data from myeloma cases. We\nformulate the relapse time estimation problem in the framework of Piecewise\nDeterministic Markov processes (PDMP), where the Euclidean component is a\nsurrogate biomarker for patient state. This approach enables continuous-time\nestimation of the status-change dates, which in turn allows for accurate\ninference of the relapse time distribution. A key challenge lies in the partial\nobservability of the process, a complexity that has been rarely addressed in\nprevious studies. . We evaluate the performance of our procedure through a\nsimulation study and compare it with different approaches. This work is a proof\nof concept on biomarker trajectories with simple behaviour, but our method can\neasily be extended to more complex dynamics."
                },
                "authors": [
                    {
                        "name": "Alice Cleynen"
                    },
                    {
                        "name": "Benoîte de Saporta"
                    },
                    {
                        "name": "Amélie Vernay"
                    }
                ],
                "author_detail": {
                    "name": "Amélie Vernay"
                },
                "author": "Amélie Vernay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08093v2",
                "updated": "2025-03-13T15:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    9,
                    6,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    6,
                    53,
                    27,
                    1,
                    70,
                    0
                ],
                "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface\n  Reconstruction"
                },
                "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nhigh-quality rendering capabilities, ultra-fast training, and inference speeds.\nHowever, when we apply 3DGS to surface reconstruction tasks, especially in\nenvironments with dynamic objects and distractors, the method suffers from\nfloating artifacts and color errors due to inconsistency from different\nviewpoints. To address this challenge, we propose Multi-View Consistency\nGaussian Splatting for the domain of Robust Surface Reconstruction\n(\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a\n{heuristics-guided distractor masking} strategy for robust surface\nreconstruction in non-static environments. Compared to existing methods that\nrely on MLPs for distractor segmentation strategies, our approach separates\ndistractors from static scene elements by comparing multi-view feature\nconsistency, allowing us to obtain precise distractor masks early in training.\nFurthermore, we introduce a pruning measure based on multi-view contributions\nto reset transmittance, effectively reducing floating artifacts. Finally, a\nmulti-view consistency loss is applied to achieve high-quality performance in\nsurface reconstruction tasks. Experimental results demonstrate that MVGSR\nachieves competitive geometric accuracy and rendering fidelity compared to the\nstate-of-the-art surface reconstruction algorithms. More information is\navailable on our project page (https://mvgsr.github.io).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nhigh-quality rendering capabilities, ultra-fast training, and inference speeds.\nHowever, when we apply 3DGS to surface reconstruction tasks, especially in\nenvironments with dynamic objects and distractors, the method suffers from\nfloating artifacts and color errors due to inconsistency from different\nviewpoints. To address this challenge, we propose Multi-View Consistency\nGaussian Splatting for the domain of Robust Surface Reconstruction\n(\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a\n{heuristics-guided distractor masking} strategy for robust surface\nreconstruction in non-static environments. Compared to existing methods that\nrely on MLPs for distractor segmentation strategies, our approach separates\ndistractors from static scene elements by comparing multi-view feature\nconsistency, allowing us to obtain precise distractor masks early in training.\nFurthermore, we introduce a pruning measure based on multi-view contributions\nto reset transmittance, effectively reducing floating artifacts. Finally, a\nmulti-view consistency loss is applied to achieve high-quality performance in\nsurface reconstruction tasks. Experimental results demonstrate that MVGSR\nachieves competitive geometric accuracy and rendering fidelity compared to the\nstate-of-the-art surface reconstruction algorithms. More information is\navailable on our project page (https://mvgsr.github.io)."
                },
                "authors": [
                    {
                        "name": "Chenfeng Hou"
                    },
                    {
                        "name": "Qi Xun Yeo"
                    },
                    {
                        "name": "Mengqi Guo"
                    },
                    {
                        "name": "Yongxin Su"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "project page https://mvgsr.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v4",
                "updated": "2025-03-13T14:59:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    59,
                    54,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10775v2",
                "updated": "2025-03-13T14:56:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    56,
                    47,
                    3,
                    72,
                    0
                ],
                "published": "2024-04-16T17:59:11Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    59,
                    11,
                    1,
                    107,
                    0
                ],
                "title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation"
                },
                "summary": "In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only egocentric\nviews of the world. To effectively plan in this setting, in contrast to\nlearning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video conditioned on the world state. By leveraging this\ncompositional world model, in combination with Vision Language Models to infer\nthe actions of other agents, we can use a tree search procedure to integrate\nthese modules and facilitate online cooperative planning. We evaluate our\nmethods on three challenging benchmarks with 2-4 agents. The results show our\ncompositional world model is effective and the framework enables the embodied\nagents to cooperate efficiently with different agents across various tasks and\nan arbitrary number of agents, showing the promising future of our proposed\nmethods. More videos can be found at https://embodied-agi.cs.umass.edu/combo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only egocentric\nviews of the world. To effectively plan in this setting, in contrast to\nlearning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video conditioned on the world state. By leveraging this\ncompositional world model, in combination with Vision Language Models to infer\nthe actions of other agents, we can use a tree search procedure to integrate\nthese modules and facilitate online cooperative planning. We evaluate our\nmethods on three challenging benchmarks with 2-4 agents. The results show our\ncompositional world model is effective and the framework enables the embodied\nagents to cooperate efficiently with different agents across various tasks and\nan arbitrary number of agents, showing the promising future of our proposed\nmethods. More videos can be found at https://embodied-agi.cs.umass.edu/combo/."
                },
                "authors": [
                    {
                        "name": "Hongxin Zhang"
                    },
                    {
                        "name": "Zeyuan Wang"
                    },
                    {
                        "name": "Qiushi Lyu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Sunli Chen"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Behzad Dariush"
                    },
                    {
                        "name": "Kwonjoon Lee"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "Published at ICLR 2025. 24 pages. The first three authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10433v1",
                "updated": "2025-03-13T14:56:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    56,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:56:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    56,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA"
                },
                "summary": "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states."
                },
                "authors": [
                    {
                        "name": "Guy Nason"
                    },
                    {
                        "name": "Daniel Salnikov"
                    },
                    {
                        "name": "Mario Cortina-Borja"
                    }
                ],
                "author_detail": {
                    "name": "Mario Cortina-Borja"
                },
                "author": "Mario Cortina-Borja",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2401.09381",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10432v1",
                "updated": "2025-03-13T14:55:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    55,
                    59,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:55:59Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    55,
                    59,
                    3,
                    72,
                    0
                ],
                "title": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models"
                },
                "summary": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability."
                },
                "authors": [
                    {
                        "name": "Can Zheng"
                    },
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Guofa Cai"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Chung G. Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chung G. Kang"
                },
                "author": "Chung G. Kang",
                "arxiv_comment": "6 pages, 7 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10431v1",
                "updated": "2025-03-13T14:53:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    53,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:53:00Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    53,
                    0,
                    3,
                    72,
                    0
                ],
                "title": "Low Complexity Point Tracking of the Myocardium in 2D Echocardiography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Complexity Point Tracking of the Myocardium in 2D Echocardiography"
                },
                "summary": "Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime."
                },
                "authors": [
                    {
                        "name": "Artem Chernyshov"
                    },
                    {
                        "name": "John Nyberg"
                    },
                    {
                        "name": "Vegard Holmstrøm"
                    },
                    {
                        "name": "Md Abulkalam Azad"
                    },
                    {
                        "name": "Bjørnar Grenne"
                    },
                    {
                        "name": "Håvard Dalen"
                    },
                    {
                        "name": "Svein Arne Aase"
                    },
                    {
                        "name": "Lasse Lovstakken"
                    },
                    {
                        "name": "Andreas Østvik"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Østvik"
                },
                "author": "Andreas Østvik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10422v1",
                "updated": "2025-03-13T14:43:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    43,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:43:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    43,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "Category Prompt Mamba Network for Nuclei Segmentation and Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category Prompt Mamba Network for Nuclei Segmentation and Classification"
                },
                "summary": "Nuclei segmentation and classification provide an essential basis for tumor\nimmune microenvironment analysis. The previous nuclei segmentation and\nclassification models require splitting large images into smaller patches for\ntraining, leading to two significant issues. First, nuclei at the borders of\nadjacent patches often misalign during inference. Second, this patch-based\napproach significantly increases the model's training and inference time.\nRecently, Mamba has garnered attention for its ability to model large-scale\nimages with linear time complexity and low memory consumption. It offers a\npromising solution for training nuclei segmentation and classification models\non full-sized images. However, the Mamba orientation-based scanning method\nlacks account for category-specific features, resulting in sub-optimal\nperformance in scenarios with imbalanced class distributions. To address these\nchallenges, this paper introduces a novel scanning strategy based on category\nprobability sorting, which independently ranks and scans features for each\ncategory according to confidence from high to low. This approach enhances the\nfeature representation of uncertain samples and mitigates the issues caused by\nimbalanced distributions. Extensive experiments conducted on four public\ndatasets demonstrate that our method outperforms state-of-the-art approaches,\ndelivering superior performance in nuclei segmentation and classification\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclei segmentation and classification provide an essential basis for tumor\nimmune microenvironment analysis. The previous nuclei segmentation and\nclassification models require splitting large images into smaller patches for\ntraining, leading to two significant issues. First, nuclei at the borders of\nadjacent patches often misalign during inference. Second, this patch-based\napproach significantly increases the model's training and inference time.\nRecently, Mamba has garnered attention for its ability to model large-scale\nimages with linear time complexity and low memory consumption. It offers a\npromising solution for training nuclei segmentation and classification models\non full-sized images. However, the Mamba orientation-based scanning method\nlacks account for category-specific features, resulting in sub-optimal\nperformance in scenarios with imbalanced class distributions. To address these\nchallenges, this paper introduces a novel scanning strategy based on category\nprobability sorting, which independently ranks and scans features for each\ncategory according to confidence from high to low. This approach enhances the\nfeature representation of uncertain samples and mitigates the issues caused by\nimbalanced distributions. Extensive experiments conducted on four public\ndatasets demonstrate that our method outperforms state-of-the-art approaches,\ndelivering superior performance in nuclei segmentation and classification\ntasks."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zijie Fang"
                    },
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Lingbo Zhang"
                    },
                    {
                        "name": "Xianchao Guan"
                    },
                    {
                        "name": "Yongbing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongbing Zhang"
                },
                "author": "Yongbing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03636v2",
                "updated": "2025-03-13T14:38:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    38,
                    58,
                    3,
                    72,
                    0
                ],
                "published": "2024-05-06T16:55:20Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    16,
                    55,
                    20,
                    0,
                    127,
                    0
                ],
                "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy\n  Attacks, Defenses, Applications, and Policy Landscape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Federation Strikes Back: A Survey of Federated Learning Privacy\n  Attacks, Defenses, Applications, and Policy Landscape"
                },
                "summary": "Deep learning has shown incredible potential across a wide array of tasks,\nand accompanied by this growth has been an insatiable appetite for data.\nHowever, a large amount of data needed for enabling deep learning is stored on\npersonal devices, and recent concerns on privacy have further highlighted\nchallenges for accessing such data. As a result, federated learning (FL) has\nemerged as an important privacy-preserving technology that enables\ncollaborative training of machine learning models without the need to send the\nraw, potentially sensitive, data to a central server. However, the fundamental\npremise that sending model updates to a server is privacy-preserving only holds\nif the updates cannot be \"reverse engineered\" to infer information about the\nprivate training data. It has been shown under a wide variety of settings that\nthis privacy premise does not hold. In this survey paper, we provide a\ncomprehensive literature review of the different privacy attacks and defense\nmethods in FL. We identify the current limitations of these attacks and\nhighlight the settings in which the privacy of ann FL client can be broken. We\nfurther dissect some of the successful industry applications of FL and draw\nlessons for future successful adoption. We survey the emerging landscape of\nprivacy regulation for FL and conclude with future directions for taking FL\ntoward the cherished goal of generating accurate models while preserving the\nprivacy of the data from its participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has shown incredible potential across a wide array of tasks,\nand accompanied by this growth has been an insatiable appetite for data.\nHowever, a large amount of data needed for enabling deep learning is stored on\npersonal devices, and recent concerns on privacy have further highlighted\nchallenges for accessing such data. As a result, federated learning (FL) has\nemerged as an important privacy-preserving technology that enables\ncollaborative training of machine learning models without the need to send the\nraw, potentially sensitive, data to a central server. However, the fundamental\npremise that sending model updates to a server is privacy-preserving only holds\nif the updates cannot be \"reverse engineered\" to infer information about the\nprivate training data. It has been shown under a wide variety of settings that\nthis privacy premise does not hold. In this survey paper, we provide a\ncomprehensive literature review of the different privacy attacks and defense\nmethods in FL. We identify the current limitations of these attacks and\nhighlight the settings in which the privacy of ann FL client can be broken. We\nfurther dissect some of the successful industry applications of FL and draw\nlessons for future successful adoption. We survey the emerging landscape of\nprivacy regulation for FL and conclude with future directions for taking FL\ntoward the cherished goal of generating accurate models while preserving the\nprivacy of the data from its participants."
                },
                "authors": [
                    {
                        "name": "Joshua C. Zhao"
                    },
                    {
                        "name": "Saurabh Bagchi"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Kevin S. Chan"
                    },
                    {
                        "name": "Somali Chaterji"
                    },
                    {
                        "name": "Dimitris Dimitriadis"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Ninghui Li"
                    },
                    {
                        "name": "Arash Nourian"
                    },
                    {
                        "name": "Holger R. Roth"
                    }
                ],
                "author_detail": {
                    "name": "Holger R. Roth"
                },
                "author": "Holger R. Roth",
                "arxiv_comment": "Accepted to ACM Computing Surveys; 35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; H.4; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10408v1",
                "updated": "2025-03-13T14:32:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "title": "Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning"
                },
                "summary": "We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Jonathan Shaki"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Sarit Kraus"
                    }
                ],
                "author_detail": {
                    "name": "Sarit Kraus"
                },
                "author": "Sarit Kraus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10406v1",
                "updated": "2025-03-13T14:31:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    31,
                    52,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    31,
                    52,
                    3,
                    72,
                    0
                ],
                "title": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models"
                },
                "summary": "Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps://lyne1.github.io/RealGeneral/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps://lyne1.github.io/RealGeneral/"
                },
                "authors": [
                    {
                        "name": "Yijing Lin"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Shuhan Zhuang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v6",
                "updated": "2025-03-13T14:04:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    4,
                    12,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10378v1",
                "updated": "2025-03-13T13:55:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    56,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:55:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "3D non-LTE Ca II line formation in metal-poor FGK stars. I. Abundance\n  corrections, radial velocity corrections, and synthetic spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D non-LTE Ca II line formation in metal-poor FGK stars. I. Abundance\n  corrections, radial velocity corrections, and synthetic spectra"
                },
                "summary": "The Ca II resonance doublet (HK) and the near-infrared triplet (CaT) are\namong the strongest features in stellar spectra of FGK-type stars. These\nspectral lines remain prominent down to extremely low metallicities and are\nthus useful for providing stellar parameters via ionisation balance and as\nradial velocity diagnostics. However, the majority of studies that model these\nlines in late-type stars still rely on one dimensional (1D) hydrostatic model\natmospheres and the assumption of local thermodynamic equilibrium (LTE). We\npresent 3D non-LTE radiative transfer calculations of the CaT and HK lines in\nan extended grid of 3D model atmospheres of metal-poor FGK-type. We investigate\nthe impact of 3D non-LTE effects on abundances, line bisectors and radial\nvelocities. We used a subset of 3D model atmospheres from the recently\npublished STAGGER-grid to synthesize spectra in 3D (non-)LTE. For comparison,\nsimilar calculations were performed in 1D (non-)LTE using models from the MARCS\ngrid. Abundance corrections for the CaT lines relative to 1D LTE range from\n+0.1 to -1.0 dex, with more severe corrections for strong lines in giants. With\nfixed line strength, the abundance corrections become more negative with\nincreasing effective temperature and decreasing surface gravity. Radial\nvelocity corrections relative to 1D LTE based on cross-correlation of the whole\nline profile range from -0.2 km/s to +1.5 km/s, with more severe corrections\nwhere the CaT lines are strongest. The corrections are even more severe if the\nline core alone is used to infer the radial velocity. The line strengths and\nshapes, and consequently the abundance and radial velocity corrections, are\nstrongly affected by the chosen radiative transfer assumption, 1/3D (non)-LTE.\nWe release grids of theoretical spectra that can be used to improve the\naccuracy of stellar spectroscopic analyses based on the Ca II triplet lines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ca II resonance doublet (HK) and the near-infrared triplet (CaT) are\namong the strongest features in stellar spectra of FGK-type stars. These\nspectral lines remain prominent down to extremely low metallicities and are\nthus useful for providing stellar parameters via ionisation balance and as\nradial velocity diagnostics. However, the majority of studies that model these\nlines in late-type stars still rely on one dimensional (1D) hydrostatic model\natmospheres and the assumption of local thermodynamic equilibrium (LTE). We\npresent 3D non-LTE radiative transfer calculations of the CaT and HK lines in\nan extended grid of 3D model atmospheres of metal-poor FGK-type. We investigate\nthe impact of 3D non-LTE effects on abundances, line bisectors and radial\nvelocities. We used a subset of 3D model atmospheres from the recently\npublished STAGGER-grid to synthesize spectra in 3D (non-)LTE. For comparison,\nsimilar calculations were performed in 1D (non-)LTE using models from the MARCS\ngrid. Abundance corrections for the CaT lines relative to 1D LTE range from\n+0.1 to -1.0 dex, with more severe corrections for strong lines in giants. With\nfixed line strength, the abundance corrections become more negative with\nincreasing effective temperature and decreasing surface gravity. Radial\nvelocity corrections relative to 1D LTE based on cross-correlation of the whole\nline profile range from -0.2 km/s to +1.5 km/s, with more severe corrections\nwhere the CaT lines are strongest. The corrections are even more severe if the\nline core alone is used to infer the radial velocity. The line strengths and\nshapes, and consequently the abundance and radial velocity corrections, are\nstrongly affected by the chosen radiative transfer assumption, 1/3D (non)-LTE.\nWe release grids of theoretical spectra that can be used to improve the\naccuracy of stellar spectroscopic analyses based on the Ca II triplet lines."
                },
                "authors": [
                    {
                        "name": "Cis Lagae"
                    },
                    {
                        "name": "Anish M. Amarsi"
                    },
                    {
                        "name": "Karin Lind"
                    }
                ],
                "author_detail": {
                    "name": "Karin Lind"
                },
                "author": "Karin Lind",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10377v1",
                "updated": "2025-03-13T13:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:55:22Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    22,
                    3,
                    72,
                    0
                ],
                "title": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs."
                },
                "authors": [
                    {
                        "name": "Qiaoling Chen"
                    },
                    {
                        "name": "Shenggui Li"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08179v3",
                "updated": "2025-03-13T13:54:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    54,
                    27,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-11T08:43:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    43,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models"
                },
                "summary": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks."
                },
                "authors": [
                    {
                        "name": "Zicheng Ma"
                    },
                    {
                        "name": "Chuanliu Fan"
                    },
                    {
                        "name": "Zhicong Wang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Yanheng Li"
                    },
                    {
                        "name": "Shihao Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Yi Qin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Qin Gao"
                },
                "author": "Yi Qin Gao",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18008v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18008v4",
                "updated": "2025-03-13T13:50:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    50,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-25T09:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    12,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms"
                },
                "summary": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation."
                },
                "authors": [
                    {
                        "name": "Yashan Wang"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Jianhuai Hu"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Yueqi Peng"
                    },
                    {
                        "name": "Yongxin Huang"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Xiaobing Li"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18008v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18008v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10367v1",
                "updated": "2025-03-13T13:47:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:47:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "G-Boost: Boosting Private SLMs with General LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Boost: Boosting Private SLMs with General LLMs"
                },
                "summary": "Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs."
                },
                "authors": [
                    {
                        "name": "Yijiang Fan"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Longbin Lai"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08658v2",
                "updated": "2025-03-13T13:42:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    42,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-09T05:10:46Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    5,
                    10,
                    46,
                    6,
                    40,
                    0
                ],
                "title": "Knowledge-data fusion dominated vehicle platoon dynamics modeling and\n  analysis: A physics-encoded deep learning approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-data fusion dominated vehicle platoon dynamics modeling and\n  analysis: A physics-encoded deep learning approach"
                },
                "summary": "Recently, artificial intelligence (AI)-enabled nonlinear vehicle platoon\ndynamics modeling plays a crucial role in predicting and optimizing the\ninteractions between vehicles. Existing efforts lack the extraction and capture\nof vehicle behavior interaction features at the platoon scale. More\nimportantly, maintaining high modeling accuracy without losing physical\nanalyzability remains to be solved. To this end, this paper proposes a novel\nphysics-encoded deep learning network, named PeMTFLN, to model the nonlinear\nvehicle platoon dynamics. Specifically, an analyzable parameters encoded\ncomputational graph (APeCG) is designed to guide the platoon to respond to the\ndriving behavior of the lead vehicle while ensuring local stability. Besides, a\nmulti-scale trajectory feature learning network (MTFLN) is constructed to\ncapture platoon following patterns and infer the physical parameters required\nfor APeCG from trajectory data. The human-driven vehicle trajectory datasets\n(HIGHSIM) were used to train the proposed PeMTFLN. The trajectories prediction\nexperiments show that PeMTFLN exhibits superior compared to the baseline models\nin terms of predictive accuracy in speed and gap. The stability analysis result\nshows that the physical parameters in APeCG is able to reproduce the platoon\nstability in real-world condition. In simulation experiments, PeMTFLN performs\nlow inference error in platoon trajectories generation. Moreover, PeMTFLN also\naccurately reproduces ground-truth safety statistics. The code of proposed\nPeMTFLN is open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, artificial intelligence (AI)-enabled nonlinear vehicle platoon\ndynamics modeling plays a crucial role in predicting and optimizing the\ninteractions between vehicles. Existing efforts lack the extraction and capture\nof vehicle behavior interaction features at the platoon scale. More\nimportantly, maintaining high modeling accuracy without losing physical\nanalyzability remains to be solved. To this end, this paper proposes a novel\nphysics-encoded deep learning network, named PeMTFLN, to model the nonlinear\nvehicle platoon dynamics. Specifically, an analyzable parameters encoded\ncomputational graph (APeCG) is designed to guide the platoon to respond to the\ndriving behavior of the lead vehicle while ensuring local stability. Besides, a\nmulti-scale trajectory feature learning network (MTFLN) is constructed to\ncapture platoon following patterns and infer the physical parameters required\nfor APeCG from trajectory data. The human-driven vehicle trajectory datasets\n(HIGHSIM) were used to train the proposed PeMTFLN. The trajectories prediction\nexperiments show that PeMTFLN exhibits superior compared to the baseline models\nin terms of predictive accuracy in speed and gap. The stability analysis result\nshows that the physical parameters in APeCG is able to reproduce the platoon\nstability in real-world condition. In simulation experiments, PeMTFLN performs\nlow inference error in platoon trajectories generation. Moreover, PeMTFLN also\naccurately reproduces ground-truth safety statistics. The code of proposed\nPeMTFLN is open source."
                },
                "authors": [
                    {
                        "name": "Hao Lyu"
                    },
                    {
                        "name": "Yanyong Guo"
                    },
                    {
                        "name": "Pan Liu"
                    },
                    {
                        "name": "Shuo Feng"
                    },
                    {
                        "name": "Weilin Ren"
                    },
                    {
                        "name": "Quansheng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Yue"
                },
                "author": "Quansheng Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13163v2",
                "updated": "2025-03-13T13:39:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    39,
                    9,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-20T09:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding"
                },
                "summary": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development."
                },
                "authors": [
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Caterina Tozzi"
                    },
                    {
                        "name": "Andrea Hita Ardiaca"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "James Weatherall"
                    },
                    {
                        "name": "Adam Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Adam Taylor"
                },
                "author": "Adam Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v7",
                "updated": "2025-03-13T13:37:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    57,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10357v1",
                "updated": "2025-03-13T13:37:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    54,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:37:54Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    54,
                    3,
                    72,
                    0
                ],
                "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark"
                },
                "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Alina Lobanova"
                    },
                    {
                        "name": "Ekaterina Neminova"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Irina Nikishina"
                    }
                ],
                "author_detail": {
                    "name": "Irina Nikishina"
                },
                "author": "Irina Nikishina",
                "arxiv_comment": "Labeled data and generated image Wordnet are published at\n  https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10351v1",
                "updated": "2025-03-13T13:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:27:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Trends for Modern Machine Translation with Large Reasoning Models"
                },
                "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it."
                },
                "authors": [
                    {
                        "name": "Sinuo Liu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12029v2",
                "updated": "2025-03-13T13:22:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    22,
                    46,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-17T17:02:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    2,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath."
                },
                "authors": [
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Hongyu Yang"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Xinwei Yao"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13191v2",
                "updated": "2025-03-13T13:20:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    20,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-20T03:47:54Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    47,
                    54,
                    4,
                    264,
                    0
                ],
                "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management"
                },
                "summary": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica."
                },
                "authors": [
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Zhen Ying"
                    },
                    {
                        "name": "Muyang He"
                    },
                    {
                        "name": "Yutong Chen"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Yanzhe Hong"
                    },
                    {
                        "name": "Jiaping Lu"
                    },
                    {
                        "name": "Kaipeng Zheng"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Xiaoying Li"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "arxiv_comment": "Accepted by ICLR 2025 SCI-FM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v2",
                "updated": "2025-03-13T13:13:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    13,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01727v2",
                "updated": "2025-03-13T13:09:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    9,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-02T16:37:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    37,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing"
                },
                "summary": "Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements."
                },
                "authors": [
                    {
                        "name": "Yilmazcan Ozyurt"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10331v1",
                "updated": "2025-03-13T13:07:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions"
                },
                "summary": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Our code is available at\nhttps://be2rlab.github.io/OSMa-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Our code is available at\nhttps://be2rlab.github.io/OSMa-Bench/."
                },
                "authors": [
                    {
                        "name": "Maxim Popov"
                    },
                    {
                        "name": "Regina Kurkova"
                    },
                    {
                        "name": "Mikhail Iumanov"
                    },
                    {
                        "name": "Jaafar Mahmoud"
                    },
                    {
                        "name": "Sergey Kolyubin"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Kolyubin"
                },
                "author": "Sergey Kolyubin",
                "arxiv_comment": "Project page: https://be2rlab.github.io/OSMa-Bench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11716v2",
                "updated": "2025-03-13T13:05:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    5,
                    23,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-15T15:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    48,
                    27,
                    1,
                    289,
                    0
                ],
                "title": "Randomization-based Inference for MCP-Mod",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization-based Inference for MCP-Mod"
                },
                "summary": "Dose selection is critical in pharmaceutical drug development, as it directly\nimpacts therapeutic efficacy and patient safety of a drug. The Generalized\nMultiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used\nin Phase II trials for testing and estimation of dose-response relationships.\nHowever, its effectiveness in small sample sizes, particularly with binary\nendpoints, is hindered by issues like complete separation in logistic\nregression, leading to non-existence of estimates. Motivated by an actual\nclinical trial using the MCP-Mod approach, this paper introduces penalized\nmaximum likelihood estimation (MLE) and randomization-based inference\ntechniques to address these challenges. Randomization-based inference allows\nfor exact finite sample inference, while population-based inference for MCP-Mod\ntypically relies on asymptotic approximations. Simulation studies demonstrate\nthat randomization-based tests can enhance statistical power in small to\nmedium-sized samples while maintaining control over type-I error rates, even in\nthe presence of time trends. Our results show that residual-based randomization\ntests using penalized MLEs not only improve computational efficiency but also\noutperform standard randomization-based methods, making them an adequate choice\nfor dose-finding analyses within the MCP-Mod framework. Additionally, we apply\nthese methods to pharmacometric settings, demonstrating their effectiveness in\nsuch scenarios. The results in this paper underscore the potential of\nrandomization-based inference for the analysis of dose-finding trials,\nparticularly in small sample contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dose selection is critical in pharmaceutical drug development, as it directly\nimpacts therapeutic efficacy and patient safety of a drug. The Generalized\nMultiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used\nin Phase II trials for testing and estimation of dose-response relationships.\nHowever, its effectiveness in small sample sizes, particularly with binary\nendpoints, is hindered by issues like complete separation in logistic\nregression, leading to non-existence of estimates. Motivated by an actual\nclinical trial using the MCP-Mod approach, this paper introduces penalized\nmaximum likelihood estimation (MLE) and randomization-based inference\ntechniques to address these challenges. Randomization-based inference allows\nfor exact finite sample inference, while population-based inference for MCP-Mod\ntypically relies on asymptotic approximations. Simulation studies demonstrate\nthat randomization-based tests can enhance statistical power in small to\nmedium-sized samples while maintaining control over type-I error rates, even in\nthe presence of time trends. Our results show that residual-based randomization\ntests using penalized MLEs not only improve computational efficiency but also\noutperform standard randomization-based methods, making them an adequate choice\nfor dose-finding analyses within the MCP-Mod framework. Additionally, we apply\nthese methods to pharmacometric settings, demonstrating their effectiveness in\nsuch scenarios. The results in this paper underscore the potential of\nrandomization-based inference for the analysis of dose-finding trials,\nparticularly in small sample contexts."
                },
                "authors": [
                    {
                        "name": "Lukas Pin"
                    },
                    {
                        "name": "Oleksandr Sverdlov"
                    },
                    {
                        "name": "Frank Bretz"
                    },
                    {
                        "name": "Björn Bornkamp"
                    }
                ],
                "author_detail": {
                    "name": "Björn Bornkamp"
                },
                "author": "Björn Bornkamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10325v1",
                "updated": "2025-03-13T13:03:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:03:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Speculative Inference for Efficient LLM Inference Serving"
                },
                "summary": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Luyao Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10322v1",
                "updated": "2025-03-13T12:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    58,
                    40,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T12:58:40Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    58,
                    40,
                    3,
                    72,
                    0
                ],
                "title": "Towards Fast, Memory-based and Data-Efficient Vision-Language Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast, Memory-based and Data-Efficient Vision-Language Policy"
                },
                "summary": "Vision Language Models (VLMs) pretrained on Internet-scale vision-language\ndata have demonstrated the potential to transfer their knowledge to robotic\nlearning. However, the existing paradigm encounters three critical challenges:\n(1) expensive inference cost resulting from large-scale model parameters, (2)\nfrequent domain shifts caused by mismatched data modalities, and (3) limited\ncapacity to handle past or future experiences. In this work, we propose\nLiteVLP, a lightweight, memory-based, and general-purpose vision-language\npolicy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM\nand fine-tuned on a tiny-scale and conversation-style robotic dataset. Through\nextensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art\nvision-language policy on VIMA-Bench, with minimal training time. Furthermore,\nLiteVLP exhibits superior inference speed while maintaining exceptional high\naccuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable\nmemory ability, outperforming the best-performing baseline model by 18.8%.\nThese results highlight LiteVLP as a promising model to integrating the\nintelligence of VLMs into robotic learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) pretrained on Internet-scale vision-language\ndata have demonstrated the potential to transfer their knowledge to robotic\nlearning. However, the existing paradigm encounters three critical challenges:\n(1) expensive inference cost resulting from large-scale model parameters, (2)\nfrequent domain shifts caused by mismatched data modalities, and (3) limited\ncapacity to handle past or future experiences. In this work, we propose\nLiteVLP, a lightweight, memory-based, and general-purpose vision-language\npolicy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM\nand fine-tuned on a tiny-scale and conversation-style robotic dataset. Through\nextensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art\nvision-language policy on VIMA-Bench, with minimal training time. Furthermore,\nLiteVLP exhibits superior inference speed while maintaining exceptional high\naccuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable\nmemory ability, outperforming the best-performing baseline model by 18.8%.\nThese results highlight LiteVLP as a promising model to integrating the\nintelligence of VLMs into robotic learning."
                },
                "authors": [
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Sixu Yan"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "11 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10310v1",
                "updated": "2025-03-13T12:39:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    39,
                    4,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T12:39:04Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    39,
                    4,
                    3,
                    72,
                    0
                ],
                "title": "Capturing Semantic Flow of ML-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Semantic Flow of ML-based Systems"
                },
                "summary": "ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems."
                },
                "authors": [
                    {
                        "name": "Shin Yoo"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Somin Kim"
                    },
                    {
                        "name": "Naryeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Naryeong Kim"
                },
                "author": "Naryeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07384v2",
                "updated": "2025-03-13T12:37:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    37,
                    37,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-10T14:32:56Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs"
                },
                "summary": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies."
                },
                "authors": [
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Aythami Morales"
                    }
                ],
                "author_detail": {
                    "name": "Aythami Morales"
                },
                "author": "Aythami Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13250v2",
                "updated": "2025-03-13T12:29:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    29,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-17T19:00:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    19,
                    0,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "Cosmology with Supernova Encore in the lensing cluster MACS J0138$-$2155\n  -- Spectroscopy with MUSE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology with Supernova Encore in the lensing cluster MACS J0138$-$2155\n  -- Spectroscopy with MUSE"
                },
                "summary": "We present a spectroscopic analysis of MACS J0138$-$2155, at $z=0.336$, the\nfirst galaxy cluster hosting two strongly-lensed supernovae (SNe), Requiem and\nEncore, providing us with a chance to obtain a reliable $H_0$ measurement from\nthe time delays between the multiple images. We take advantage of new data from\nthe Multi Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope,\ncovering a central $1 \\rm \\, arcmin^2$ of the lensing cluster, for a total\ndepth of 3.7 hours, including 2.9 hours recently obtained by our Target of\nOpportunity programme. Our new spectroscopic catalogue contains reliable\nredshifts for 107 objects, including 50 galaxy cluster members with secure\nredshift values in the range $0.324 < z < 0.349$, and 13 lensed multiple images\nfrom four background sources between $0.767\\leq z \\leq 3.420$, including four\nimages of the host galaxy of the two SNe. We exploit the MUSE data to study the\nstellar kinematics of 14 bright cluster members and two background galaxies,\nobtaining reliable measurements of their line-of-sight velocity dispersion.\nFinally, we combine these results with measurements of the total magnitude of\nthe cluster members in the Hubble Space Telescope F160W band to calibrate the\nFaber-Jackson relation between luminosity and stellar velocity dispersion ($L\n\\propto \\sigma^{1/\\alpha}$) for the early-type cluster member galaxies,\nmeasuring a slope $\\alpha=0.25^{+0.05}_{-0.05}$. A pure and complete sample of\ncluster member galaxies and a reliable characterisation of their total mass\nstructure are key to building accurate total mass maps of the cluster,\nmitigating the impact of parametric degeneracies, which is necessary for\ninferring the value of $H_0$ from the measured time delays between the lensed\nimages of the two SNe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a spectroscopic analysis of MACS J0138$-$2155, at $z=0.336$, the\nfirst galaxy cluster hosting two strongly-lensed supernovae (SNe), Requiem and\nEncore, providing us with a chance to obtain a reliable $H_0$ measurement from\nthe time delays between the multiple images. We take advantage of new data from\nthe Multi Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope,\ncovering a central $1 \\rm \\, arcmin^2$ of the lensing cluster, for a total\ndepth of 3.7 hours, including 2.9 hours recently obtained by our Target of\nOpportunity programme. Our new spectroscopic catalogue contains reliable\nredshifts for 107 objects, including 50 galaxy cluster members with secure\nredshift values in the range $0.324 < z < 0.349$, and 13 lensed multiple images\nfrom four background sources between $0.767\\leq z \\leq 3.420$, including four\nimages of the host galaxy of the two SNe. We exploit the MUSE data to study the\nstellar kinematics of 14 bright cluster members and two background galaxies,\nobtaining reliable measurements of their line-of-sight velocity dispersion.\nFinally, we combine these results with measurements of the total magnitude of\nthe cluster members in the Hubble Space Telescope F160W band to calibrate the\nFaber-Jackson relation between luminosity and stellar velocity dispersion ($L\n\\propto \\sigma^{1/\\alpha}$) for the early-type cluster member galaxies,\nmeasuring a slope $\\alpha=0.25^{+0.05}_{-0.05}$. A pure and complete sample of\ncluster member galaxies and a reliable characterisation of their total mass\nstructure are key to building accurate total mass maps of the cluster,\nmitigating the impact of parametric degeneracies, which is necessary for\ninferring the value of $H_0$ from the measured time delays between the lensed\nimages of the two SNe."
                },
                "authors": [
                    {
                        "name": "G. Granata"
                    },
                    {
                        "name": "G. B. Caminha"
                    },
                    {
                        "name": "S. Ertl"
                    },
                    {
                        "name": "C. Grillo"
                    },
                    {
                        "name": "S. Schuldt"
                    },
                    {
                        "name": "S. H. Suyu"
                    },
                    {
                        "name": "A. Acebron"
                    },
                    {
                        "name": "P. Bergamini"
                    },
                    {
                        "name": "R. Cañameras"
                    },
                    {
                        "name": "P. Rosati"
                    },
                    {
                        "name": "S. Taubenberger"
                    }
                ],
                "author_detail": {
                    "name": "S. Taubenberger"
                },
                "author": "S. Taubenberger",
                "arxiv_comment": "Astronomy and Astrophysics, accepted. 16 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12196v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12196v4",
                "updated": "2025-03-13T12:16:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    16,
                    27,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T01:51:16Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    1,
                    51,
                    16,
                    1,
                    170,
                    0
                ],
                "title": "CITADEL: Context Similarity Based Deep Learning Framework Bug Finding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITADEL: Context Similarity Based Deep Learning Framework Bug Finding"
                },
                "summary": "With deep learning (DL) technology becoming an integral part of the new\nintelligent software, tools of DL framework testing and bug-finding are in high\ndemand. Existing DL framework testing tools have limited coverage on bug types.\nFor example, they lack the capability of finding performance bugs, which are\ncritical for DL model training and inference regarding performance, economics,\nand the environment. This problem is challenging due to the difficulty of\ngetting test oracles of performance bugs. Moreover, existing tools are\ninefficient, generating hundreds of test cases with few trigger bugs. In this\npaper, we propose Citadel, a method that accelerates the finding of bugs in\nterms of efficiency and effectiveness. We observe that many DL framework bugs\nare similar due to the similarity of operators and algorithms belonging to the\nsame family (e.g., Conv2D and Conv3D). Orthogonal to existing bug-finding\ntools, Citadel aims to find new bugs that are similar to reported ones that\nhave known test oracles. It works by first collecting existing bug reports and\nidentifying problematic APIs. Citadel defines context similarity to measure the\nsimilarity of DL framework API pairs and automatically generates test cases\nwith oracles for APIs that are similar to the problematic APIs in existing bug\nreports. Citadel respectively covers 1,436 PyTorch and 5,380 TensorFlow APIs\nand effectively detects 77 and 74 API bugs, many of which, e.g., 11 performance\nbugs, cannot be detected by existing tools. Moreover, a remarkable 35.40% of\nthe test cases generated by Citadel can trigger bugs, which significantly\ntranscends the state-of-the-art method (3.90%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With deep learning (DL) technology becoming an integral part of the new\nintelligent software, tools of DL framework testing and bug-finding are in high\ndemand. Existing DL framework testing tools have limited coverage on bug types.\nFor example, they lack the capability of finding performance bugs, which are\ncritical for DL model training and inference regarding performance, economics,\nand the environment. This problem is challenging due to the difficulty of\ngetting test oracles of performance bugs. Moreover, existing tools are\ninefficient, generating hundreds of test cases with few trigger bugs. In this\npaper, we propose Citadel, a method that accelerates the finding of bugs in\nterms of efficiency and effectiveness. We observe that many DL framework bugs\nare similar due to the similarity of operators and algorithms belonging to the\nsame family (e.g., Conv2D and Conv3D). Orthogonal to existing bug-finding\ntools, Citadel aims to find new bugs that are similar to reported ones that\nhave known test oracles. It works by first collecting existing bug reports and\nidentifying problematic APIs. Citadel defines context similarity to measure the\nsimilarity of DL framework API pairs and automatically generates test cases\nwith oracles for APIs that are similar to the problematic APIs in existing bug\nreports. Citadel respectively covers 1,436 PyTorch and 5,380 TensorFlow APIs\nand effectively detects 77 and 74 API bugs, many of which, e.g., 11 performance\nbugs, cannot be detected by existing tools. Moreover, a remarkable 35.40% of\nthe test cases generated by Citadel can trigger bugs, which significantly\ntranscends the state-of-the-art method (3.90%)."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Shiwei Wang"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12196v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12196v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02702v2",
                "updated": "2025-03-13T11:47:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    47,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-04T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises"
                },
                "summary": "Internal threat detection (IDT) aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely.In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by approximately 90% in the Xiaohongshu Security Operation Center.\nTherefore, our RedChronos system demonstrates exceptional performance in\nhandling IDT tasks, providing innovative solutions for these challenges. We\nbelieve that future research can continue to enhance the system's performance\nin IDT tasks while also reducing the response time to internal risk events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal threat detection (IDT) aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely.In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by approximately 90% in the Xiaohongshu Security Operation Center.\nTherefore, our RedChronos system demonstrates exceptional performance in\nhandling IDT tasks, providing innovative solutions for these challenges. We\nbelieve that future research can continue to enhance the system's performance\nin IDT tasks while also reducing the response time to internal risk events."
                },
                "authors": [
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Zhengjia Zhu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Xiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Zhang"
                },
                "author": "Xiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09672v3",
                "updated": "2025-03-13T11:47:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    47,
                    5,
                    3,
                    72,
                    0
                ],
                "published": "2023-12-15T10:34:53Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    10,
                    34,
                    53,
                    4,
                    349,
                    0
                ],
                "title": "InstructPipe: Generating Visual Blocks Pipelines with Human Instructions\n  and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructPipe: Generating Visual Blocks Pipelines with Human Instructions\n  and LLMs"
                },
                "summary": "Visual programming has the potential of providing novice programmers with a\nlow-code experience to build customized processing pipelines. Existing systems\ntypically require users to build pipelines from scratch, implying that novice\nusers are expected to set up and link appropriate nodes from a blank workspace.\nIn this paper, we introduce InstructPipe, an AI assistant for prototyping\nmachine learning (ML) pipelines with text instructions. We contribute two large\nlanguage model (LLM) modules and a code interpreter as part of our framework.\nThe LLM modules generate pseudocode for a target pipeline, and the interpreter\nrenders the pipeline in the node-graph editor for further human-AI\ncollaboration. Both technical and user evaluation (N=16) shows that\nInstructPipe empowers users to streamline their ML pipeline workflow, reduce\ntheir learning curve, and leverage open-ended commands to spark innovative\nideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual programming has the potential of providing novice programmers with a\nlow-code experience to build customized processing pipelines. Existing systems\ntypically require users to build pipelines from scratch, implying that novice\nusers are expected to set up and link appropriate nodes from a blank workspace.\nIn this paper, we introduce InstructPipe, an AI assistant for prototyping\nmachine learning (ML) pipelines with text instructions. We contribute two large\nlanguage model (LLM) modules and a code interpreter as part of our framework.\nThe LLM modules generate pseudocode for a target pipeline, and the interpreter\nrenders the pipeline in the node-graph editor for further human-AI\ncollaboration. Both technical and user evaluation (N=16) shows that\nInstructPipe empowers users to streamline their ML pipeline workflow, reduce\ntheir learning curve, and leverage open-ended commands to spark innovative\nideas."
                },
                "authors": [
                    {
                        "name": "Zhongyi Zhou"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Vrushank Phadnis"
                    },
                    {
                        "name": "Xiuxiu Yuan"
                    },
                    {
                        "name": "Jun Jiang"
                    },
                    {
                        "name": "Xun Qian"
                    },
                    {
                        "name": "Kristen Wright"
                    },
                    {
                        "name": "Mark Sherwood"
                    },
                    {
                        "name": "Jason Mayes"
                    },
                    {
                        "name": "Jingtao Zhou"
                    },
                    {
                        "name": "Yiyi Huang"
                    },
                    {
                        "name": "Zheng Xu"
                    },
                    {
                        "name": "Yinda Zhang"
                    },
                    {
                        "name": "Johnny Lee"
                    },
                    {
                        "name": "Alex Olwal"
                    },
                    {
                        "name": "David Kim"
                    },
                    {
                        "name": "Ram Iyengar"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ruofei Du"
                    }
                ],
                "author_detail": {
                    "name": "Ruofei Du"
                },
                "author": "Ruofei Du",
                "arxiv_comment": "CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12770v2",
                "updated": "2025-03-13T11:34:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    34,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-17T10:33:13Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "title": "A Survey on Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sequential Recommendation"
                },
                "summary": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield."
                },
                "authors": [
                    {
                        "name": "Liwei Pan"
                    },
                    {
                        "name": "Weike Pan"
                    },
                    {
                        "name": "Meiyan Wei"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Zhong Ming"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Ming"
                },
                "author": "Zhong Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10251v1",
                "updated": "2025-03-13T10:53:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    53,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:53:17Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    53,
                    17,
                    3,
                    72,
                    0
                ],
                "title": "Numerical Error Analysis of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical Error Analysis of Large Language Models"
                },
                "summary": "Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference."
                },
                "authors": [
                    {
                        "name": "Stanislav Budzinskiy"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Longbin Zeng"
                    },
                    {
                        "name": "Philipp Petersen"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Petersen"
                },
                "author": "Philipp Petersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10248v1",
                "updated": "2025-03-13T10:47:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:47:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns"
                },
                "summary": "We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies."
                },
                "authors": [
                    {
                        "name": "Idan Horowitz"
                    },
                    {
                        "name": "Ori Plonsky"
                    }
                ],
                "author_detail": {
                    "name": "Ori Plonsky"
                },
                "author": "Ori Plonsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10246v1",
                "updated": "2025-03-13T10:45:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    45,
                    54,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:45:54Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    45,
                    54,
                    3,
                    72,
                    0
                ],
                "title": "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation"
                },
                "summary": "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation."
                },
                "authors": [
                    {
                        "name": "Samuel Pawel"
                    },
                    {
                        "name": "Małgorzata Roos"
                    },
                    {
                        "name": "Leonhard Held"
                    }
                ],
                "author_detail": {
                    "name": "Leonhard Held"
                },
                "author": "Leonhard Held",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09158v2",
                "updated": "2025-03-13T10:45:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    45,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T08:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    33,
                    46,
                    2,
                    71,
                    0
                ],
                "title": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with\n  Multimodal Large Language Models"
                },
                "summary": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Linrui Xu"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Danfeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Danfeng Yan"
                },
                "author": "Danfeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10243v1",
                "updated": "2025-03-13T10:42:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    42,
                    52,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:42:52Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    42,
                    52,
                    3,
                    72,
                    0
                ],
                "title": "Composition structure of polyconvolution associated with index\n  Kontorovich-Lebedev transform and Fourier integrals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composition structure of polyconvolution associated with index\n  Kontorovich-Lebedev transform and Fourier integrals"
                },
                "summary": "Using Kakichev's classical concept and extending Yakubovich-Britvina's\napproach (\\textit{Results. Math.} 55(1-2):175-197, 2009) and (\\textit{Integral\nTransforms Spec. Funct.} 21(4):259--276, 2010) for setting up\nKontorovich-Lebedev convolution operators, this paper proposes a new\npolyconvolution structure associated with the KL-transform and Fourier\nintegrals. Our main contributions include demonstrating a one-dimensional\nWatson-type transform, providing necessary and sufficient conditions for this\ntransform to serve as unitary on $L_2(\\mathbb{R}_+)$, and inferring its inverse\noperator in symmetric form. The existence of this structure over specific\nfunction spaces and its connection with previously known convolutions are\npointed out. Establish the Plancherel-type theorem, prove the convergence in\nthe mean-square sense in $L_2(\\mathbb{R}_+)$, and prove the boundedness of dual\nspaces via Riesz-Thorin's theorem. Derives new weighted $L_p$-norm inequalities\nand boundedness in a three-parametric family of Lebesgue spaces. These\ntheoretical findings are applied to solve specific classes of the\nToeplitz-Hankel equation, providing a priori estimations based on the\nestablished conditions for $L_1$ solvability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Kakichev's classical concept and extending Yakubovich-Britvina's\napproach (\\textit{Results. Math.} 55(1-2):175-197, 2009) and (\\textit{Integral\nTransforms Spec. Funct.} 21(4):259--276, 2010) for setting up\nKontorovich-Lebedev convolution operators, this paper proposes a new\npolyconvolution structure associated with the KL-transform and Fourier\nintegrals. Our main contributions include demonstrating a one-dimensional\nWatson-type transform, providing necessary and sufficient conditions for this\ntransform to serve as unitary on $L_2(\\mathbb{R}_+)$, and inferring its inverse\noperator in symmetric form. The existence of this structure over specific\nfunction spaces and its connection with previously known convolutions are\npointed out. Establish the Plancherel-type theorem, prove the convergence in\nthe mean-square sense in $L_2(\\mathbb{R}_+)$, and prove the boundedness of dual\nspaces via Riesz-Thorin's theorem. Derives new weighted $L_p$-norm inequalities\nand boundedness in a three-parametric family of Lebesgue spaces. These\ntheoretical findings are applied to solve specific classes of the\nToeplitz-Hankel equation, providing a priori estimations based on the\nestablished conditions for $L_1$ solvability."
                },
                "authors": [
                    {
                        "name": "Trinh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Trinh Tuan"
                },
                "author": "Trinh Tuan",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "42A38, 44A20, 44A35, 45A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v3",
                "updated": "2025-03-13T10:41:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    41,
                    26,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments.Benefiting from our end-to-end methodology\nand efficient network design, our approach surpasses previous methods in model\ninference efficiency and achieves real-time 6-Dof grasp detection on edge\ndevices. Furthermore, real-world experiments validate the effectiveness of our\nmethod, achieving a satisfactory 94% object grasping success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments.Benefiting from our end-to-end methodology\nand efficient network design, our approach surpasses previous methods in model\ninference efficiency and achieves real-time 6-Dof grasp detection on edge\ndevices. Furthermore, real-world experiments validate the effectiveness of our\nmethod, achieving a satisfactory 94% object grasping success rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "arxiv_comment": "Accepted by 2025 IEEE International Symposium on Circuits and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v4",
                "updated": "2025-03-13T10:41:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    41,
                    4,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12455v2",
                "updated": "2025-03-13T10:40:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    40,
                    9,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-18T02:37:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    37,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs"
                },
                "summary": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design."
                },
                "authors": [
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08708v2",
                "updated": "2025-03-13T10:37:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    37,
                    18,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-10T02:55:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    55,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors"
                },
                "summary": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10242v1",
                "updated": "2025-03-13T10:34:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    34,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:34:43Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    34,
                    43,
                    3,
                    72,
                    0
                ],
                "title": "MinorBench: A hand-built benchmark for content-based risks for children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MinorBench: A hand-built benchmark for content-based risks for children"
                },
                "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users."
                },
                "authors": [
                    {
                        "name": "Shaun Khoo"
                    },
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Rachel Shong"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Shong"
                },
                "author": "Rachel Shong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10241v1",
                "updated": "2025-03-13T10:32:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    32,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    32,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning"
                },
                "summary": "Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning."
                },
                "authors": [
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Franca Garzotto"
                    },
                    {
                        "name": "Giuseppe Vizzari"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Simone Colombani"
                    }
                ],
                "author_detail": {
                    "name": "Simone Colombani"
                },
                "author": "Simone Colombani",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10239v1",
                "updated": "2025-03-13T10:29:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    29,
                    40,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:29:40Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    29,
                    40,
                    3,
                    72,
                    0
                ],
                "title": "I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app\n  Interaction History in Super-apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app\n  Interaction History in Super-apps"
                },
                "summary": "Super-apps have emerged as comprehensive platforms integrating various\nmini-apps to provide diverse services. While super-apps offer convenience and\nenriched functionality, they can introduce new privacy risks. This paper\nreveals a new privacy leakage source in super-apps: mini-app interaction\nhistory, including mini-app usage history (Mini-H) and operation history\n(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as\ntheir frequency and categories. Op-H captures user interactions within\nmini-apps, including button clicks, bar drags, and image views. Super-apps can\nnaturally collect these data without instrumentation due to the web-based\nfeature of mini-apps. We identify these data types as novel and unexplored\nprivacy risks through a literature review of 30 papers and an empirical\nanalysis of 31 super-apps. We design a mini-app interaction history-oriented\ninference attack (THEFT), to exploit this new vulnerability. Using THEFT, the\ninsider threats within the low-privilege business department of the super-app\nvendor acting as the adversary can achieve more than 95.5% accuracy in\ninferring privacy attributes of over 16.1% of users. THEFT only requires a\nsmall training dataset of 200 users from public breached databases on the\nInternet. We also engage with super-app vendors and a standards association to\nincrease industry awareness and commitment to protect this data. Our\ncontributions are significant in identifying overlooked privacy risks,\ndemonstrating the effectiveness of a new attack, and influencing industry\npractices toward better privacy protection in the super-app ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-apps have emerged as comprehensive platforms integrating various\nmini-apps to provide diverse services. While super-apps offer convenience and\nenriched functionality, they can introduce new privacy risks. This paper\nreveals a new privacy leakage source in super-apps: mini-app interaction\nhistory, including mini-app usage history (Mini-H) and operation history\n(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as\ntheir frequency and categories. Op-H captures user interactions within\nmini-apps, including button clicks, bar drags, and image views. Super-apps can\nnaturally collect these data without instrumentation due to the web-based\nfeature of mini-apps. We identify these data types as novel and unexplored\nprivacy risks through a literature review of 30 papers and an empirical\nanalysis of 31 super-apps. We design a mini-app interaction history-oriented\ninference attack (THEFT), to exploit this new vulnerability. Using THEFT, the\ninsider threats within the low-privilege business department of the super-app\nvendor acting as the adversary can achieve more than 95.5% accuracy in\ninferring privacy attributes of over 16.1% of users. THEFT only requires a\nsmall training dataset of 200 users from public breached databases on the\nInternet. We also engage with super-app vendors and a standards association to\nincrease industry awareness and commitment to protect this data. Our\ncontributions are significant in identifying overlooked privacy risks,\ndemonstrating the effectiveness of a new attack, and influencing industry\npractices toward better privacy protection in the super-app ecosystem."
                },
                "authors": [
                    {
                        "name": "Yifeng Cai"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Mengyu Yao"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Xinyi Fu"
                    },
                    {
                        "name": "Ruoyu Li"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Xiangqun Chen"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Ding Li"
                    }
                ],
                "author_detail": {
                    "name": "Ding Li"
                },
                "author": "Ding Li",
                "arxiv_comment": "Accepted by USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15658v2",
                "updated": "2025-03-13T10:15:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    15,
                    59,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-24T01:47:23Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    47,
                    23,
                    1,
                    268,
                    0
                ],
                "title": "Long-horizon Embodied Planning with Implicit Logical Inference and\n  Hallucination Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Embodied Planning with Implicit Logical Inference and\n  Hallucination Mitigation"
                },
                "summary": "Long-horizon embodied planning underpins embodied AI. To accomplish\nlong-horizon tasks, one of the most feasible ways is to decompose abstract\ninstructions into a sequence of actionable steps. Foundation models still face\nlogical errors and hallucinations in long-horizon planning, unless provided\nwith highly relevant examples to the tasks. However, providing highly relevant\nexamples for any random task is unpractical. Therefore, we present ReLEP, a\nnovel framework for Real-time Long-horizon Embodied Planning. ReLEP can\ncomplete a wide range of long-horizon tasks without in-context examples by\nlearning implicit logical inference through fine-tuning. The fine-tuned large\nvision-language model formulates plans as sequences of skill functions. These\nfunctions are selected from a carefully designed skill library. ReLEP is also\nequipped with a Memory module for plan and status recall, and a Robot\nConfiguration module for versatility across robot types. In addition, we\npropose a data generation pipeline to tackle dataset scarcity. When\nconstructing the dataset, we considered the implicit logical relationships,\nenabling the model to learn implicit logical relationships and dispel\nhallucinations. Through comprehensive evaluations across various long-horizon\ntasks, ReLEP demonstrates high success rates and compliance to execution even\non unseen tasks and outperforms state-of-the-art baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon embodied planning underpins embodied AI. To accomplish\nlong-horizon tasks, one of the most feasible ways is to decompose abstract\ninstructions into a sequence of actionable steps. Foundation models still face\nlogical errors and hallucinations in long-horizon planning, unless provided\nwith highly relevant examples to the tasks. However, providing highly relevant\nexamples for any random task is unpractical. Therefore, we present ReLEP, a\nnovel framework for Real-time Long-horizon Embodied Planning. ReLEP can\ncomplete a wide range of long-horizon tasks without in-context examples by\nlearning implicit logical inference through fine-tuning. The fine-tuned large\nvision-language model formulates plans as sequences of skill functions. These\nfunctions are selected from a carefully designed skill library. ReLEP is also\nequipped with a Memory module for plan and status recall, and a Robot\nConfiguration module for versatility across robot types. In addition, we\npropose a data generation pipeline to tackle dataset scarcity. When\nconstructing the dataset, we considered the implicit logical relationships,\nenabling the model to learn implicit logical relationships and dispel\nhallucinations. Through comprehensive evaluations across various long-horizon\ntasks, ReLEP demonstrates high success rates and compliance to execution even\non unseen tasks and outperforms state-of-the-art baseline methods."
                },
                "authors": [
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Sicheng Xiang"
                    },
                    {
                        "name": "Zibo Wang"
                    },
                    {
                        "name": "Dingsheng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Dingsheng Luo"
                },
                "author": "Dingsheng Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10232v1",
                "updated": "2025-03-13T10:15:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    15,
                    40,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:15:40Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    15,
                    40,
                    3,
                    72,
                    0
                ],
                "title": "Flows on convex polytopes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flows on convex polytopes"
                },
                "summary": "We present a framework for modeling complex, high-dimensional distributions\non convex polytopes by leveraging recent advances in discrete and continuous\nnormalizing flows on Riemannian manifolds. We show that any full-dimensional\npolytope is homeomorphic to a unit ball, and our approach harnesses flows\ndefined on the ball, mapping them back to the original polytope. Furthermore,\nwe introduce a strategy to construct flows when only the vertex representation\nof a polytope is available, employing maximum entropy barycentric coordinates\nand Aitchison geometry. Our experiments take inspiration from applications in\nmetabolic flux analysis and demonstrate that our methods achieve competitive\ndensity estimation, sampling accuracy, as well as fast training and inference\ntimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework for modeling complex, high-dimensional distributions\non convex polytopes by leveraging recent advances in discrete and continuous\nnormalizing flows on Riemannian manifolds. We show that any full-dimensional\npolytope is homeomorphic to a unit ball, and our approach harnesses flows\ndefined on the ball, mapping them back to the original polytope. Furthermore,\nwe introduce a strategy to construct flows when only the vertex representation\nof a polytope is available, employing maximum entropy barycentric coordinates\nand Aitchison geometry. Our experiments take inspiration from applications in\nmetabolic flux analysis and demonstrate that our methods achieve competitive\ndensity estimation, sampling accuracy, as well as fast training and inference\ntimes."
                },
                "authors": [
                    {
                        "name": "Tomek Diederen"
                    },
                    {
                        "name": "Nicola Zamboni"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Zamboni"
                },
                "author": "Nicola Zamboni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10225v1",
                "updated": "2025-03-13T10:08:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    8,
                    18,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:08:18Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    8,
                    18,
                    3,
                    72,
                    0
                ],
                "title": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA"
                },
                "summary": "Amodal segmentation aims to infer the complete shape of occluded objects,\neven when the occluded region's appearance is unavailable. However, current\namodal segmentation methods lack the capability to interact with users through\ntext input and struggle to understand or reason about implicit and complex\npurposes. While methods like LISA integrate multi-modal large language models\n(LLMs) with segmentation for reasoning tasks, they are limited to predicting\nonly visible object regions and face challenges in handling complex occlusion\nscenarios. To address these limitations, we propose a novel task named amodal\nreasoning segmentation, aiming to predict the complete amodal shape of occluded\nobjects while providing answers with elaborations based on user text input. We\ndevelop a generalizable dataset generation pipeline and introduce a new dataset\nfocusing on daily life scenarios, encompassing diverse real-world occlusions.\nFurthermore, we present AURA (Amodal Understanding and Reasoning Assistant), a\nnovel model with advanced global and spatial-level designs specifically\ntailored to handle complex occlusions. Extensive experiments validate AURA's\neffectiveness on the proposed dataset. The code, model, and dataset will be\npublicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amodal segmentation aims to infer the complete shape of occluded objects,\neven when the occluded region's appearance is unavailable. However, current\namodal segmentation methods lack the capability to interact with users through\ntext input and struggle to understand or reason about implicit and complex\npurposes. While methods like LISA integrate multi-modal large language models\n(LLMs) with segmentation for reasoning tasks, they are limited to predicting\nonly visible object regions and face challenges in handling complex occlusion\nscenarios. To address these limitations, we propose a novel task named amodal\nreasoning segmentation, aiming to predict the complete amodal shape of occluded\nobjects while providing answers with elaborations based on user text input. We\ndevelop a generalizable dataset generation pipeline and introduce a new dataset\nfocusing on daily life scenarios, encompassing diverse real-world occlusions.\nFurthermore, we present AURA (Amodal Understanding and Reasoning Assistant), a\nnovel model with advanced global and spatial-level designs specifically\ntailored to handle complex occlusions. Extensive experiments validate AURA's\neffectiveness on the proposed dataset. The code, model, and dataset will be\npublicly released."
                },
                "authors": [
                    {
                        "name": "Zhixuan Li"
                    },
                    {
                        "name": "Hyunse Yoon"
                    },
                    {
                        "name": "Sanghoon Lee"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "arxiv_comment": "11 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10219v1",
                "updated": "2025-03-13T10:01:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    1,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:01:00Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    1,
                    0,
                    3,
                    72,
                    0
                ],
                "title": "Probability-Flow ODE in Infinite-Dimensional Function Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability-Flow ODE in Infinite-Dimensional Function Spaces"
                },
                "summary": "Recent advances in infinite-dimensional diffusion models have demonstrated\ntheir effectiveness and scalability in function generation tasks where the\nunderlying structure is inherently infinite-dimensional. To accelerate\ninference in such models, we derive, for the first time, an analog of the\nprobability-flow ODE (PF-ODE) in infinite-dimensional function spaces.\nLeveraging this newly formulated PF-ODE, we reduce the number of function\nevaluations while maintaining sample quality in function generation tasks,\nincluding applications to PDEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in infinite-dimensional diffusion models have demonstrated\ntheir effectiveness and scalability in function generation tasks where the\nunderlying structure is inherently infinite-dimensional. To accelerate\ninference in such models, we derive, for the first time, an analog of the\nprobability-flow ODE (PF-ODE) in infinite-dimensional function spaces.\nLeveraging this newly formulated PF-ODE, we reduce the number of function\nevaluations while maintaining sample quality in function generation tasks,\nincluding applications to PDEs."
                },
                "authors": [
                    {
                        "name": "Kunwoo Na"
                    },
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Sungbin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Sungbin Lim"
                },
                "author": "Sungbin Lim",
                "arxiv_comment": "26 pages, 8 figures. Accepted to the ICLR 2025 DeLTa Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10217v1",
                "updated": "2025-03-13T09:59:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    16,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:59:16Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    16,
                    3,
                    72,
                    0
                ],
                "title": "Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout"
                },
                "summary": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Xianjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xianjun Gao"
                },
                "author": "Xianjun Gao",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10216v1",
                "updated": "2025-03-13T09:59:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    5,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:59:05Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    5,
                    3,
                    72,
                    0
                ],
                "title": "CoStoDet-DDPM: Collaborative Training of Stochastic and Deterministic\n  Models Improves Surgical Workflow Anticipation and Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoStoDet-DDPM: Collaborative Training of Stochastic and Deterministic\n  Models Improves Surgical Workflow Anticipation and Recognition"
                },
                "summary": "Anticipating and recognizing surgical workflows are critical for intelligent\nsurgical assistance systems. However, existing methods rely on deterministic\ndecision-making, struggling to generalize across the large anatomical and\nprocedural variations inherent in real-world surgeries.In this paper, we\nintroduce an innovative framework that incorporates stochastic modeling through\na denoising diffusion probabilistic model (DDPM) into conventional\ndeterministic learning for surgical workflow analysis. At the heart of our\napproach is a collaborative co-training paradigm: the DDPM branch captures\nprocedural uncertainties to enrich feature representations, while the task\nbranch focuses on predicting surgical phases and instrument\nusage.Theoretically, we demonstrate that this mutual refinement mechanism\nbenefits both branches: the DDPM reduces prediction errors in uncertain\nscenarios, and the task branch directs the DDPM toward clinically meaningful\nrepresentations. Notably, the DDPM branch is discarded during inference,\nenabling real-time predictions without sacrificing accuracy.Experiments on the\nCholec80 dataset show that for the anticipation task, our method achieves a 16%\nreduction in eMAE compared to state-of-the-art approaches, and for phase\nrecognition, it improves the Jaccard score by 1.0%. Additionally, on the\nAutoLaparo dataset, our method achieves a 1.5% improvement in the Jaccard score\nfor phase recognition, while also exhibiting robust generalization to\npatient-specific variations. Our code and weight are available at\nhttps://github.com/kk42yy/CoStoDet-DDPM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anticipating and recognizing surgical workflows are critical for intelligent\nsurgical assistance systems. However, existing methods rely on deterministic\ndecision-making, struggling to generalize across the large anatomical and\nprocedural variations inherent in real-world surgeries.In this paper, we\nintroduce an innovative framework that incorporates stochastic modeling through\na denoising diffusion probabilistic model (DDPM) into conventional\ndeterministic learning for surgical workflow analysis. At the heart of our\napproach is a collaborative co-training paradigm: the DDPM branch captures\nprocedural uncertainties to enrich feature representations, while the task\nbranch focuses on predicting surgical phases and instrument\nusage.Theoretically, we demonstrate that this mutual refinement mechanism\nbenefits both branches: the DDPM reduces prediction errors in uncertain\nscenarios, and the task branch directs the DDPM toward clinically meaningful\nrepresentations. Notably, the DDPM branch is discarded during inference,\nenabling real-time predictions without sacrificing accuracy.Experiments on the\nCholec80 dataset show that for the anticipation task, our method achieves a 16%\nreduction in eMAE compared to state-of-the-art approaches, and for phase\nrecognition, it improves the Jaccard score by 1.0%. Additionally, on the\nAutoLaparo dataset, our method achieves a 1.5% improvement in the Jaccard score\nfor phase recognition, while also exhibiting robust generalization to\npatient-specific variations. Our code and weight are available at\nhttps://github.com/kk42yy/CoStoDet-DDPM."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10211v1",
                "updated": "2025-03-13T09:54:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    54,
                    35,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:54:35Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    54,
                    35,
                    3,
                    72,
                    0
                ],
                "title": "Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation"
                },
                "summary": "Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning."
                },
                "authors": [
                    {
                        "name": "Henglyu Liu"
                    },
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Yuan Qiu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01143v2",
                "updated": "2025-03-13T09:41:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    41,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-02T10:27:47Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    27,
                    47,
                    0,
                    246,
                    0
                ],
                "title": "HexiScale: Accommodating Large Language Model Training over\n  Heterogeneous Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexiScale: Accommodating Large Language Model Training over\n  Heterogeneous Environment"
                },
                "summary": "Training large language model (LLM) is a computationally intensive task,\nwhich is typically conducted in data centers with homogeneous high-performance\nGPUs. We explore an alternative approach by deploying training computations\nacross heterogeneous GPUs to enable better flexibility and efficiency for\nheterogeneous resource utilization. To achieve this goal, we propose a novel\nsystem, HexiScale, that can flexibly support asymmetric partition of training\ncomputations in the scope of data-, pipeline-, and tensor model parallelism. We\nfurther formalize the allocation of asymmetric partitioned training\ncomputations over a set of heterogeneous GPUs as a constrained optimization\nproblem and propose an efficient hierarchical graph partitioning algorithm. Our\napproach effectively allocates training computations across GPUs, fully\nleveraging the available computational power. We conduct empirical studies to\nevaluate the performance of HexiScale with state-of-the-art homogeneous and\nheterogeneous training systems. When training LLMs at different scales (from 7B\nto 30B), HexiScale achieves comparable MFU when running over heterogeneous GPUs\ncompared to state-of-the-art training systems running over homogeneous\nhigh-performance GPUs with the same total peak FLOPS. The percentage gaps in\nMFU between HexiScale and comparable homogeneous settings are as low as\n$0.3\\%$, with an average of $3.5\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language model (LLM) is a computationally intensive task,\nwhich is typically conducted in data centers with homogeneous high-performance\nGPUs. We explore an alternative approach by deploying training computations\nacross heterogeneous GPUs to enable better flexibility and efficiency for\nheterogeneous resource utilization. To achieve this goal, we propose a novel\nsystem, HexiScale, that can flexibly support asymmetric partition of training\ncomputations in the scope of data-, pipeline-, and tensor model parallelism. We\nfurther formalize the allocation of asymmetric partitioned training\ncomputations over a set of heterogeneous GPUs as a constrained optimization\nproblem and propose an efficient hierarchical graph partitioning algorithm. Our\napproach effectively allocates training computations across GPUs, fully\nleveraging the available computational power. We conduct empirical studies to\nevaluate the performance of HexiScale with state-of-the-art homogeneous and\nheterogeneous training systems. When training LLMs at different scales (from 7B\nto 30B), HexiScale achieves comparable MFU when running over heterogeneous GPUs\ncompared to state-of-the-art training systems running over homogeneous\nhigh-performance GPUs with the same total peak FLOPS. The percentage gaps in\nMFU between HexiScale and comparable homogeneous settings are as low as\n$0.3\\%$, with an average of $3.5\\%$."
                },
                "authors": [
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19339v2",
                "updated": "2025-03-13T09:40:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    40,
                    42,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-26T17:32:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets"
                },
                "summary": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types."
                },
                "authors": [
                    {
                        "name": "Tohida Rehman"
                    },
                    {
                        "name": "Soumabha Ghosh"
                    },
                    {
                        "name": "Kuntal Das"
                    },
                    {
                        "name": "Souvik Bhattacharjee"
                    },
                    {
                        "name": "Debarshi Kumar Sanyal"
                    },
                    {
                        "name": "Samiran Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Samiran Chattopadhyay"
                },
                "author": "Samiran Chattopadhyay",
                "arxiv_comment": "5 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10199v1",
                "updated": "2025-03-13T09:34:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods"
                },
                "summary": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples."
                },
                "authors": [
                    {
                        "name": "Ruibiao Song"
                    },
                    {
                        "name": "Liying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liying Zhang"
                },
                "author": "Liying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10181v1",
                "updated": "2025-03-13T09:09:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    9,
                    20,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:09:20Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    9,
                    20,
                    3,
                    72,
                    0
                ],
                "title": "Bayesian analysis of a (3+1)D hybrid approach with initial conditions\n  from hadronic transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian analysis of a (3+1)D hybrid approach with initial conditions\n  from hadronic transport"
                },
                "summary": "This study aims to apply statistical learning, specifically Bayesian\ninference, to the (3+1)D SMASH-vHLLE-hybrid model using initial conditions\ngenerated by the SMASH transport code itself, with the objective of\nconstraining model parameters and gaining deeper insight on the temperature and\nbaryochemical potential dependence of both the shear and the bulk viscosity.\nThis study is performed in the hybrid approach SMASH-vHLLE, composed of the\nhadronic transport approach SMASH and the (3+1)D viscous hydrodynamic code\nvHLLE. A Bayesian framework is employed, utilizing Markov Chain Monte Carlo\n(MCMC) sampling to explore the parameter space. The analysis compares model\npredictions against experimental observables, including particle yields,\nmomentum and flow coefficients both at midrapidity as well as in forward and\nbackward direction. We find that the SMASH-vHLLE-hybrid framework, using\nhadronic initial conditions for Au+Au collisions at different beam energies,\ncan reproduce a variety of experimental observables at midrapidity and\nforward/backward rapidities. Notably, the preferred posterior distribution\nsuggests a near-vanishing specific shear viscosity in the high-temperature QGP\nphase, combined with moderate-to-large bulk viscosity around the phase\ntransition region, although the constraints on baryochemical potential\ndependence are weak. Our findings reveal that a hadronic initial condition\nconstrains the evolution more strictly at intermediate energies, making\nparameters such as the hydrodynamic onset time highly sensitive. Intriguingly,\nthe extracted shear viscosity differs substantially from previous Bayesian\nanalyses, motivating further systematic studies with higher-statistics data\nsets and refined modeling assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to apply statistical learning, specifically Bayesian\ninference, to the (3+1)D SMASH-vHLLE-hybrid model using initial conditions\ngenerated by the SMASH transport code itself, with the objective of\nconstraining model parameters and gaining deeper insight on the temperature and\nbaryochemical potential dependence of both the shear and the bulk viscosity.\nThis study is performed in the hybrid approach SMASH-vHLLE, composed of the\nhadronic transport approach SMASH and the (3+1)D viscous hydrodynamic code\nvHLLE. A Bayesian framework is employed, utilizing Markov Chain Monte Carlo\n(MCMC) sampling to explore the parameter space. The analysis compares model\npredictions against experimental observables, including particle yields,\nmomentum and flow coefficients both at midrapidity as well as in forward and\nbackward direction. We find that the SMASH-vHLLE-hybrid framework, using\nhadronic initial conditions for Au+Au collisions at different beam energies,\ncan reproduce a variety of experimental observables at midrapidity and\nforward/backward rapidities. Notably, the preferred posterior distribution\nsuggests a near-vanishing specific shear viscosity in the high-temperature QGP\nphase, combined with moderate-to-large bulk viscosity around the phase\ntransition region, although the constraints on baryochemical potential\ndependence are weak. Our findings reveal that a hadronic initial condition\nconstrains the evolution more strictly at intermediate energies, making\nparameters such as the hydrodynamic onset time highly sensitive. Intriguingly,\nthe extracted shear viscosity differs substantially from previous Bayesian\nanalyses, motivating further systematic studies with higher-statistics data\nsets and refined modeling assumptions."
                },
                "authors": [
                    {
                        "name": "Niklas Götz"
                    },
                    {
                        "name": "Iurii Karpenko"
                    },
                    {
                        "name": "Hannah Elfner"
                    }
                ],
                "author_detail": {
                    "name": "Hannah Elfner"
                },
                "author": "Hannah Elfner",
                "arxiv_comment": "24 pages,20 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.10630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10630v1",
                "updated": "2025-03-13T17:59:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:59:48Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"
                },
                "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Lingqing Zhao"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10619v1",
                "updated": "2025-03-13T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search"
                },
                "summary": "We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Andy Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Andy Zhou"
                },
                "author": "Andy Zhou",
                "arxiv_comment": "Accepted to ICLR 2025 Trustworthy LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10620v1",
                "updated": "2025-03-13T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community."
                },
                "authors": [
                    {
                        "name": "Kshitij Ambilduke"
                    },
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Sonal Sannigrahi"
                    },
                    {
                        "name": "Anil Keshwani"
                    },
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "Marcely Zanon Boito"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10613v1",
                "updated": "2025-03-13T17:55:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:55:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    55,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing"
                },
                "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference."
                },
                "authors": [
                    {
                        "name": "Advait Gupta"
                    },
                    {
                        "name": "NandaKiran Velaga"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10602v1",
                "updated": "2025-03-13T17:46:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:46:06Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention"
                },
                "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "15 pages, 9 figures, the first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06215v3",
                "updated": "2025-03-13T17:30:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    30,
                    48,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-08T17:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    20,
                    37,
                    1,
                    282,
                    0
                ],
                "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback"
                },
                "summary": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "ICLR 2025 Spotlight; Project Page: https://DataEnvGym.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10573v1",
                "updated": "2025-03-13T17:23:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative\n  Study of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative\n  Study of Large Language Models"
                },
                "summary": "With the rapid evolution of Artificial Intelligence (AI), Large Language\nModels (LLMs) have reshaped the frontiers of various fields, spanning\nhealthcare, public health, engineering, science, agriculture, education, arts,\nhumanities, and mathematical reasoning. Among these advancements, DeepSeek\nmodels have emerged as noteworthy contenders, demonstrating promising\ncapabilities that set them apart from their peers. While previous studies have\nconducted comparative analyses of LLMs, few have delivered a comprehensive\nevaluation of mathematical reasoning across a broad spectrum of LLMs. In this\nwork, we aim to bridge this gap by conducting an in-depth comparative study,\nfocusing on the strengths and limitations of DeepSeek models in relation to\ntheir leading counterparts. In particular, our study systematically evaluates\nthe mathematical reasoning performance of two DeepSeek models alongside five\nprominent LLMs across three independent benchmark datasets. The findings reveal\nseveral key insights: 1). DeepSeek-R1 consistently achieved the highest\naccuracy on two of the three datasets, demonstrating strong mathematical\nreasoning capabilities. 2). The distilled variant of LLMs significantly\nunderperformed compared to its peers, highlighting potential drawbacks in using\ndistillation techniques. 3). In terms of response time, Gemini 2.0 Flash\ndemonstrated the fastest processing speed, outperforming other models in\nefficiency, which is a crucial factor for real-time applications. Beyond these\nquantitative assessments, we delve into how architecture, training, and\noptimization impact LLMs' mathematical reasoning. Moreover, our study goes\nbeyond mere performance comparison by identifying key areas for future\nadvancements in LLM-driven mathematical reasoning. This research enhances our\nunderstanding of LLMs' mathematical reasoning and lays the groundwork for\nfuture advancements",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of Artificial Intelligence (AI), Large Language\nModels (LLMs) have reshaped the frontiers of various fields, spanning\nhealthcare, public health, engineering, science, agriculture, education, arts,\nhumanities, and mathematical reasoning. Among these advancements, DeepSeek\nmodels have emerged as noteworthy contenders, demonstrating promising\ncapabilities that set them apart from their peers. While previous studies have\nconducted comparative analyses of LLMs, few have delivered a comprehensive\nevaluation of mathematical reasoning across a broad spectrum of LLMs. In this\nwork, we aim to bridge this gap by conducting an in-depth comparative study,\nfocusing on the strengths and limitations of DeepSeek models in relation to\ntheir leading counterparts. In particular, our study systematically evaluates\nthe mathematical reasoning performance of two DeepSeek models alongside five\nprominent LLMs across three independent benchmark datasets. The findings reveal\nseveral key insights: 1). DeepSeek-R1 consistently achieved the highest\naccuracy on two of the three datasets, demonstrating strong mathematical\nreasoning capabilities. 2). The distilled variant of LLMs significantly\nunderperformed compared to its peers, highlighting potential drawbacks in using\ndistillation techniques. 3). In terms of response time, Gemini 2.0 Flash\ndemonstrated the fastest processing speed, outperforming other models in\nefficiency, which is a crucial factor for real-time applications. Beyond these\nquantitative assessments, we delve into how architecture, training, and\noptimization impact LLMs' mathematical reasoning. Moreover, our study goes\nbeyond mere performance comparison by identifying key areas for future\nadvancements in LLM-driven mathematical reasoning. This research enhances our\nunderstanding of LLMs' mathematical reasoning and lays the groundwork for\nfuture advancements"
                },
                "authors": [
                    {
                        "name": "Afrar Jahin"
                    },
                    {
                        "name": "Arif Hassan Zidan"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Shizhe Liang"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09590v2",
                "updated": "2025-03-13T17:14:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    14,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering"
                },
                "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Torresani"
                },
                "author": "Lorenzo Torresani",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10546v1",
                "updated": "2025-03-13T16:59:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    59,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:59:17Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    59,
                    17,
                    3,
                    72,
                    0
                ],
                "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation"
                },
                "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io."
                },
                "authors": [
                    {
                        "name": "Zixian Liu"
                    },
                    {
                        "name": "Mingtong Zhang"
                    },
                    {
                        "name": "Yunzhu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Li"
                },
                "author": "Yunzhu Li",
                "arxiv_comment": "Project website: http://kuda-dynamics.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10515v1",
                "updated": "2025-03-13T16:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    20,
                    25,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    20,
                    25,
                    3,
                    72,
                    0
                ],
                "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set"
                },
                "summary": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses."
                },
                "authors": [
                    {
                        "name": "Florian Eichin"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "arxiv_comment": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05039v2",
                "updated": "2025-03-13T16:17:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    17,
                    21,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-06T17:58:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    58,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification"
                },
                "summary": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13640v2",
                "updated": "2025-03-13T16:16:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    16,
                    12,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-17T15:09:24Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    9,
                    24,
                    3,
                    291,
                    0
                ],
                "title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation"
                },
                "summary": "LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v2",
                "updated": "2025-03-13T16:11:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    11,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10509v1",
                "updated": "2025-03-13T16:10:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models"
                },
                "summary": "Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks."
                },
                "authors": [
                    {
                        "name": "Sahar Admoni"
                    },
                    {
                        "name": "Omer Ben-Porat"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19141v2",
                "updated": "2025-03-13T15:59:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    59,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-24T20:18:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    20,
                    18,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "Versatile Demonstration Interface: Toward More Flexible Robot\n  Demonstration Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Versatile Demonstration Interface: Toward More Flexible Robot\n  Demonstration Collection"
                },
                "summary": "Previous methods for Learning from Demonstration leverage several approaches\nfor a human to teach motions to a robot, including teleoperation, kinesthetic\nteaching, and natural demonstrations. However, little previous work has\nexplored more general interfaces that allow for multiple demonstration types.\nGiven the varied preferences of human demonstrators and task characteristics, a\nflexible tool that enables multiple demonstration types could be crucial for\nbroader robot skill training. In this work, we propose Versatile Demonstration\nInterface (VDI), an attachment for collaborative robots that simplifies the\ncollection of three common types of demonstrations. Designed for flexible\ndeployment in industrial settings, our tool requires no additional\ninstrumentation of the environment. Our prototype interface captures human\ndemonstrations through a combination of vision, force sensing, and state\ntracking (e.g., through the robot proprioception or AprilTag tracking). Through\na user study where we deployed our prototype VDI at a local manufacturing\ninnovation center with manufacturing experts, we demonstrated VDI in\nrepresentative industrial tasks. Interactions from our study highlight the\npractical value of VDI's varied demonstration types, expose a range of\nindustrial use cases for VDI, and provide insights for future tool design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous methods for Learning from Demonstration leverage several approaches\nfor a human to teach motions to a robot, including teleoperation, kinesthetic\nteaching, and natural demonstrations. However, little previous work has\nexplored more general interfaces that allow for multiple demonstration types.\nGiven the varied preferences of human demonstrators and task characteristics, a\nflexible tool that enables multiple demonstration types could be crucial for\nbroader robot skill training. In this work, we propose Versatile Demonstration\nInterface (VDI), an attachment for collaborative robots that simplifies the\ncollection of three common types of demonstrations. Designed for flexible\ndeployment in industrial settings, our tool requires no additional\ninstrumentation of the environment. Our prototype interface captures human\ndemonstrations through a combination of vision, force sensing, and state\ntracking (e.g., through the robot proprioception or AprilTag tracking). Through\na user study where we deployed our prototype VDI at a local manufacturing\ninnovation center with manufacturing experts, we demonstrated VDI in\nrepresentative industrial tasks. Interactions from our study highlight the\npractical value of VDI's varied demonstration types, expose a range of\nindustrial use cases for VDI, and provide insights for future tool design."
                },
                "authors": [
                    {
                        "name": "Michael Hagenow"
                    },
                    {
                        "name": "Dimosthenis Kontogiorgos"
                    },
                    {
                        "name": "Yanwei Wang"
                    },
                    {
                        "name": "Julie Shah"
                    }
                ],
                "author_detail": {
                    "name": "Julie Shah"
                },
                "author": "Julie Shah",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10497v1",
                "updated": "2025-03-13T15:59:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    59,
                    20,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:59:20Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    59,
                    20,
                    3,
                    72,
                    0
                ],
                "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation"
                },
                "summary": "Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities."
                },
                "authors": [
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Junjue Wang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Edison Marrese-Taylor"
                    },
                    {
                        "name": "Shijian Lu"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10486v1",
                "updated": "2025-03-13T15:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    54,
                    26,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    54,
                    26,
                    3,
                    72,
                    0
                ],
                "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare."
                },
                "authors": [
                    {
                        "name": "Gaurav Kumar Gupta"
                    },
                    {
                        "name": "Pranal Pande"
                    }
                ],
                "author_detail": {
                    "name": "Pranal Pande"
                },
                "author": "Pranal Pande",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10470v1",
                "updated": "2025-03-13T15:42:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:42:44Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    44,
                    3,
                    72,
                    0
                ],
                "title": "Statistical Analysis of Sentence Structures through ASCII, Lexical\n  Alignment and PCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Analysis of Sentence Structures through ASCII, Lexical\n  Alignment and PCA"
                },
                "summary": "While utilizing syntactic tools such as parts-of-speech (POS) tagging has\nhelped us understand sentence structures and their distribution across diverse\ncorpora, it is quite complex and poses a challenge in natural language\nprocessing (NLP). This study focuses on understanding sentence structure\nbalance - usages of nouns, verbs, determiners, etc - harmoniously without\nrelying on such tools. It proposes a novel statistical method that uses\nAmerican Standard Code for Information Interchange (ASCII) codes to represent\ntext of 11 text corpora from various sources and their lexical category\nalignment after using their compressed versions through PCA, and analyzes the\nresults through histograms and normality tests such as Shapiro-Wilk and\nAnderson-Darling Tests. By focusing on ASCII codes, this approach simplifies\ntext processing, although not replacing any syntactic tools but complementing\nthem by offering it as a resource-efficient tool for assessing text balance.\nThe story generated by Grok shows near normality indicating balanced sentence\nstructures in LLM outputs, whereas 4 out of the remaining 10 pass the normality\ntests. Further research could explore potential applications in text quality\nevaluation and style analysis with syntactic integration for more broader\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While utilizing syntactic tools such as parts-of-speech (POS) tagging has\nhelped us understand sentence structures and their distribution across diverse\ncorpora, it is quite complex and poses a challenge in natural language\nprocessing (NLP). This study focuses on understanding sentence structure\nbalance - usages of nouns, verbs, determiners, etc - harmoniously without\nrelying on such tools. It proposes a novel statistical method that uses\nAmerican Standard Code for Information Interchange (ASCII) codes to represent\ntext of 11 text corpora from various sources and their lexical category\nalignment after using their compressed versions through PCA, and analyzes the\nresults through histograms and normality tests such as Shapiro-Wilk and\nAnderson-Darling Tests. By focusing on ASCII codes, this approach simplifies\ntext processing, although not replacing any syntactic tools but complementing\nthem by offering it as a resource-efficient tool for assessing text balance.\nThe story generated by Grok shows near normality indicating balanced sentence\nstructures in LLM outputs, whereas 4 out of the remaining 10 pass the normality\ntests. Further research could explore potential applications in text quality\nevaluation and style analysis with syntactic integration for more broader\ntasks."
                },
                "authors": [
                    {
                        "name": "Abhijeet Sahdev"
                    }
                ],
                "author_detail": {
                    "name": "Abhijeet Sahdev"
                },
                "author": "Abhijeet Sahdev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19363v2",
                "updated": "2025-03-13T15:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    42,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-26T18:01:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "DataMan: Data Manager for Pre-training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMan: Data Manager for Pre-training Large Language Models"
                },
                "summary": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources."
                },
                "authors": [
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Yawen Zeng"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "ICLR2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00263v2",
                "updated": "2025-03-13T15:21:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    21,
                    36,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-30T22:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    21,
                    5,
                    0,
                    274,
                    0
                ],
                "title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation"
                },
                "summary": "Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP"
                },
                "authors": [
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Vinkle Srivastav"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Nicolas Padoy"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Padoy"
                },
                "author": "Nicolas Padoy",
                "arxiv_comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10452v1",
                "updated": "2025-03-13T15:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    18,
                    56,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    18,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code."
                },
                "authors": [
                    {
                        "name": "Wenhao Hu"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Chunchen Wei"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v4",
                "updated": "2025-03-13T14:59:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    59,
                    54,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10432v1",
                "updated": "2025-03-13T14:55:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    55,
                    59,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:55:59Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    55,
                    59,
                    3,
                    72,
                    0
                ],
                "title": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models"
                },
                "summary": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability."
                },
                "authors": [
                    {
                        "name": "Can Zheng"
                    },
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Guofa Cai"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Chung G. Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chung G. Kang"
                },
                "author": "Chung G. Kang",
                "arxiv_comment": "6 pages, 7 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10408v1",
                "updated": "2025-03-13T14:32:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    32,
                    30,
                    3,
                    72,
                    0
                ],
                "title": "Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning"
                },
                "summary": "We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Jonathan Shaki"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Sarit Kraus"
                    }
                ],
                "author_detail": {
                    "name": "Sarit Kraus"
                },
                "author": "Sarit Kraus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10406v1",
                "updated": "2025-03-13T14:31:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    31,
                    52,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    31,
                    52,
                    3,
                    72,
                    0
                ],
                "title": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models"
                },
                "summary": "Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps://lyne1.github.io/RealGeneral/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps://lyne1.github.io/RealGeneral/"
                },
                "authors": [
                    {
                        "name": "Yijing Lin"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Shuhan Zhuang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v6",
                "updated": "2025-03-13T14:04:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    4,
                    12,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10377v1",
                "updated": "2025-03-13T13:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:55:22Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    55,
                    22,
                    3,
                    72,
                    0
                ],
                "title": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs."
                },
                "authors": [
                    {
                        "name": "Qiaoling Chen"
                    },
                    {
                        "name": "Shenggui Li"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08179v3",
                "updated": "2025-03-13T13:54:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    54,
                    27,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-11T08:43:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    43,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models"
                },
                "summary": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks."
                },
                "authors": [
                    {
                        "name": "Zicheng Ma"
                    },
                    {
                        "name": "Chuanliu Fan"
                    },
                    {
                        "name": "Zhicong Wang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Yanheng Li"
                    },
                    {
                        "name": "Shihao Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Yi Qin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Qin Gao"
                },
                "author": "Yi Qin Gao",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18008v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18008v4",
                "updated": "2025-03-13T13:50:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    50,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-25T09:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    12,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms"
                },
                "summary": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation."
                },
                "authors": [
                    {
                        "name": "Yashan Wang"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Jianhuai Hu"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Yueqi Peng"
                    },
                    {
                        "name": "Yongxin Huang"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Xiaobing Li"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18008v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18008v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10367v1",
                "updated": "2025-03-13T13:47:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:47:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "G-Boost: Boosting Private SLMs with General LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Boost: Boosting Private SLMs with General LLMs"
                },
                "summary": "Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs."
                },
                "authors": [
                    {
                        "name": "Yijiang Fan"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Longbin Lai"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13163v2",
                "updated": "2025-03-13T13:39:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    39,
                    9,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-20T09:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding"
                },
                "summary": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development."
                },
                "authors": [
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Caterina Tozzi"
                    },
                    {
                        "name": "Andrea Hita Ardiaca"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "James Weatherall"
                    },
                    {
                        "name": "Adam Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Adam Taylor"
                },
                "author": "Adam Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20245v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20245v3",
                "updated": "2025-03-13T13:38:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    38,
                    25,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-30T12:34:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    34,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems"
                },
                "summary": "This paper introduces a novel framework that leverages the Kullback-Leibler\nDivergence (KLD) metric to analyse and optimise performance trade-offs in\nintegrated sensing and communication (ISAC) systems. We consider a\nmultiple-input multiple-output (MIMO) base station that simultaneously serves\ncommunication user equipments (UEs) and detects multiple targets using a shared\nantenna deployment. We apply this framework to the widely used zero-forcing\n(ZF) communication beamforming technique, to assess their impact on the radar\nsubsystem's performance. Additionally, two optimisation problems are\nformulated: the first optimise the radar subsystem's KLD under communication\nconstraints, and the second focuses on communication waveform KLD optimisation\nwith constraints on the radar KLD. These problems are solved using a projected\ngradient method with adaptive penalties for the radar waveforms and a\ngradient-assisted interior point method (IPM) for the communication waveforms.\nAs performance benchmarks, we propose two additional optimisation frameworks: a\nradar waveform optimisation with bit error rate (BE) constraints and a\ncommunication waveform optimisation that minimises BER under radar KLD\nconstraints. Theoretical derivations and simulations show that our KLD-based\napproach effectively characterises and optimises the ISAC performance\ntrade-offs. Results indicate significant improvements in both radar detection\nand communication performance over conventional ZF beamforming and the identity\ncovariance radar design, while achieving performance comparable to BER-based\noptimisation but with notably lower computational complexity. These findings\nhighlight the advantages of KLD-based optimisation in balancing radar and\ncommunication performance for next-generation wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework that leverages the Kullback-Leibler\nDivergence (KLD) metric to analyse and optimise performance trade-offs in\nintegrated sensing and communication (ISAC) systems. We consider a\nmultiple-input multiple-output (MIMO) base station that simultaneously serves\ncommunication user equipments (UEs) and detects multiple targets using a shared\nantenna deployment. We apply this framework to the widely used zero-forcing\n(ZF) communication beamforming technique, to assess their impact on the radar\nsubsystem's performance. Additionally, two optimisation problems are\nformulated: the first optimise the radar subsystem's KLD under communication\nconstraints, and the second focuses on communication waveform KLD optimisation\nwith constraints on the radar KLD. These problems are solved using a projected\ngradient method with adaptive penalties for the radar waveforms and a\ngradient-assisted interior point method (IPM) for the communication waveforms.\nAs performance benchmarks, we propose two additional optimisation frameworks: a\nradar waveform optimisation with bit error rate (BE) constraints and a\ncommunication waveform optimisation that minimises BER under radar KLD\nconstraints. Theoretical derivations and simulations show that our KLD-based\napproach effectively characterises and optimises the ISAC performance\ntrade-offs. Results indicate significant improvements in both radar detection\nand communication performance over conventional ZF beamforming and the identity\ncovariance radar design, while achieving performance comparable to BER-based\noptimisation but with notably lower computational complexity. These findings\nhighlight the advantages of KLD-based optimisation in balancing radar and\ncommunication performance for next-generation wireless networks."
                },
                "authors": [
                    {
                        "name": "Yousef Kloob"
                    },
                    {
                        "name": "Mohammad Al-Jarrah"
                    },
                    {
                        "name": "Emad Alsusa"
                    }
                ],
                "author_detail": {
                    "name": "Emad Alsusa"
                },
                "author": "Emad Alsusa",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20245v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20245v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v7",
                "updated": "2025-03-13T13:37:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    57,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10357v1",
                "updated": "2025-03-13T13:37:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    54,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:37:54Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    37,
                    54,
                    3,
                    72,
                    0
                ],
                "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark"
                },
                "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Alina Lobanova"
                    },
                    {
                        "name": "Ekaterina Neminova"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Irina Nikishina"
                    }
                ],
                "author_detail": {
                    "name": "Irina Nikishina"
                },
                "author": "Irina Nikishina",
                "arxiv_comment": "Labeled data and generated image Wordnet are published at\n  https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10351v1",
                "updated": "2025-03-13T13:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:27:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Trends for Modern Machine Translation with Large Reasoning Models"
                },
                "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it."
                },
                "authors": [
                    {
                        "name": "Sinuo Liu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12029v2",
                "updated": "2025-03-13T13:22:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    22,
                    46,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-17T17:02:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    2,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath."
                },
                "authors": [
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Hongyu Yang"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Xinwei Yao"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13191v2",
                "updated": "2025-03-13T13:20:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    20,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-20T03:47:54Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    47,
                    54,
                    4,
                    264,
                    0
                ],
                "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management"
                },
                "summary": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica."
                },
                "authors": [
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Zhen Ying"
                    },
                    {
                        "name": "Muyang He"
                    },
                    {
                        "name": "Yutong Chen"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Yanzhe Hong"
                    },
                    {
                        "name": "Jiaping Lu"
                    },
                    {
                        "name": "Kaipeng Zheng"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Xiaoying Li"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "arxiv_comment": "Accepted by ICLR 2025 SCI-FM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v2",
                "updated": "2025-03-13T13:13:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    13,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01727v2",
                "updated": "2025-03-13T13:09:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    9,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-02T16:37:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    37,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing"
                },
                "summary": "Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements."
                },
                "authors": [
                    {
                        "name": "Yilmazcan Ozyurt"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10331v1",
                "updated": "2025-03-13T13:07:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions"
                },
                "summary": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Our code is available at\nhttps://be2rlab.github.io/OSMa-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Our code is available at\nhttps://be2rlab.github.io/OSMa-Bench/."
                },
                "authors": [
                    {
                        "name": "Maxim Popov"
                    },
                    {
                        "name": "Regina Kurkova"
                    },
                    {
                        "name": "Mikhail Iumanov"
                    },
                    {
                        "name": "Jaafar Mahmoud"
                    },
                    {
                        "name": "Sergey Kolyubin"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Kolyubin"
                },
                "author": "Sergey Kolyubin",
                "arxiv_comment": "Project page: https://be2rlab.github.io/OSMa-Bench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10325v1",
                "updated": "2025-03-13T13:03:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:03:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Speculative Inference for Efficient LLM Inference Serving"
                },
                "summary": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Luyao Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10310v1",
                "updated": "2025-03-13T12:39:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    39,
                    4,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T12:39:04Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    39,
                    4,
                    3,
                    72,
                    0
                ],
                "title": "Capturing Semantic Flow of ML-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Semantic Flow of ML-based Systems"
                },
                "summary": "ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems."
                },
                "authors": [
                    {
                        "name": "Shin Yoo"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Somin Kim"
                    },
                    {
                        "name": "Naryeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Naryeong Kim"
                },
                "author": "Naryeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07384v2",
                "updated": "2025-03-13T12:37:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    12,
                    37,
                    37,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-10T14:32:56Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs"
                },
                "summary": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies."
                },
                "authors": [
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Aythami Morales"
                    }
                ],
                "author_detail": {
                    "name": "Aythami Morales"
                },
                "author": "Aythami Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02702v2",
                "updated": "2025-03-13T11:47:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    47,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-04T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises"
                },
                "summary": "Internal threat detection (IDT) aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely.In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by approximately 90% in the Xiaohongshu Security Operation Center.\nTherefore, our RedChronos system demonstrates exceptional performance in\nhandling IDT tasks, providing innovative solutions for these challenges. We\nbelieve that future research can continue to enhance the system's performance\nin IDT tasks while also reducing the response time to internal risk events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal threat detection (IDT) aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely.In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by approximately 90% in the Xiaohongshu Security Operation Center.\nTherefore, our RedChronos system demonstrates exceptional performance in\nhandling IDT tasks, providing innovative solutions for these challenges. We\nbelieve that future research can continue to enhance the system's performance\nin IDT tasks while also reducing the response time to internal risk events."
                },
                "authors": [
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Zhengjia Zhu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Xiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Zhang"
                },
                "author": "Xiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09672v3",
                "updated": "2025-03-13T11:47:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    47,
                    5,
                    3,
                    72,
                    0
                ],
                "published": "2023-12-15T10:34:53Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    10,
                    34,
                    53,
                    4,
                    349,
                    0
                ],
                "title": "InstructPipe: Generating Visual Blocks Pipelines with Human Instructions\n  and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructPipe: Generating Visual Blocks Pipelines with Human Instructions\n  and LLMs"
                },
                "summary": "Visual programming has the potential of providing novice programmers with a\nlow-code experience to build customized processing pipelines. Existing systems\ntypically require users to build pipelines from scratch, implying that novice\nusers are expected to set up and link appropriate nodes from a blank workspace.\nIn this paper, we introduce InstructPipe, an AI assistant for prototyping\nmachine learning (ML) pipelines with text instructions. We contribute two large\nlanguage model (LLM) modules and a code interpreter as part of our framework.\nThe LLM modules generate pseudocode for a target pipeline, and the interpreter\nrenders the pipeline in the node-graph editor for further human-AI\ncollaboration. Both technical and user evaluation (N=16) shows that\nInstructPipe empowers users to streamline their ML pipeline workflow, reduce\ntheir learning curve, and leverage open-ended commands to spark innovative\nideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual programming has the potential of providing novice programmers with a\nlow-code experience to build customized processing pipelines. Existing systems\ntypically require users to build pipelines from scratch, implying that novice\nusers are expected to set up and link appropriate nodes from a blank workspace.\nIn this paper, we introduce InstructPipe, an AI assistant for prototyping\nmachine learning (ML) pipelines with text instructions. We contribute two large\nlanguage model (LLM) modules and a code interpreter as part of our framework.\nThe LLM modules generate pseudocode for a target pipeline, and the interpreter\nrenders the pipeline in the node-graph editor for further human-AI\ncollaboration. Both technical and user evaluation (N=16) shows that\nInstructPipe empowers users to streamline their ML pipeline workflow, reduce\ntheir learning curve, and leverage open-ended commands to spark innovative\nideas."
                },
                "authors": [
                    {
                        "name": "Zhongyi Zhou"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Vrushank Phadnis"
                    },
                    {
                        "name": "Xiuxiu Yuan"
                    },
                    {
                        "name": "Jun Jiang"
                    },
                    {
                        "name": "Xun Qian"
                    },
                    {
                        "name": "Kristen Wright"
                    },
                    {
                        "name": "Mark Sherwood"
                    },
                    {
                        "name": "Jason Mayes"
                    },
                    {
                        "name": "Jingtao Zhou"
                    },
                    {
                        "name": "Yiyi Huang"
                    },
                    {
                        "name": "Zheng Xu"
                    },
                    {
                        "name": "Yinda Zhang"
                    },
                    {
                        "name": "Johnny Lee"
                    },
                    {
                        "name": "Alex Olwal"
                    },
                    {
                        "name": "David Kim"
                    },
                    {
                        "name": "Ram Iyengar"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ruofei Du"
                    }
                ],
                "author_detail": {
                    "name": "Ruofei Du"
                },
                "author": "Ruofei Du",
                "arxiv_comment": "CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12770v2",
                "updated": "2025-03-13T11:34:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    34,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-17T10:33:13Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "title": "A Survey on Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sequential Recommendation"
                },
                "summary": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield."
                },
                "authors": [
                    {
                        "name": "Liwei Pan"
                    },
                    {
                        "name": "Weike Pan"
                    },
                    {
                        "name": "Meiyan Wei"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Zhong Ming"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Ming"
                },
                "author": "Zhong Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10248v1",
                "updated": "2025-03-13T10:47:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:47:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    47,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns"
                },
                "summary": "We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies."
                },
                "authors": [
                    {
                        "name": "Idan Horowitz"
                    },
                    {
                        "name": "Ori Plonsky"
                    }
                ],
                "author_detail": {
                    "name": "Ori Plonsky"
                },
                "author": "Ori Plonsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09158v2",
                "updated": "2025-03-13T10:45:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    45,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T08:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    33,
                    46,
                    2,
                    71,
                    0
                ],
                "title": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with\n  Multimodal Large Language Models"
                },
                "summary": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Linrui Xu"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Danfeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Danfeng Yan"
                },
                "author": "Danfeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v3",
                "updated": "2025-03-13T10:41:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    41,
                    26,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments.Benefiting from our end-to-end methodology\nand efficient network design, our approach surpasses previous methods in model\ninference efficiency and achieves real-time 6-Dof grasp detection on edge\ndevices. Furthermore, real-world experiments validate the effectiveness of our\nmethod, achieving a satisfactory 94% object grasping success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments.Benefiting from our end-to-end methodology\nand efficient network design, our approach surpasses previous methods in model\ninference efficiency and achieves real-time 6-Dof grasp detection on edge\ndevices. Furthermore, real-world experiments validate the effectiveness of our\nmethod, achieving a satisfactory 94% object grasping success rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "arxiv_comment": "Accepted by 2025 IEEE International Symposium on Circuits and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v4",
                "updated": "2025-03-13T10:41:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    41,
                    4,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12455v2",
                "updated": "2025-03-13T10:40:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    40,
                    9,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-18T02:37:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    37,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs"
                },
                "summary": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design."
                },
                "authors": [
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08708v2",
                "updated": "2025-03-13T10:37:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    37,
                    18,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-10T02:55:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    55,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors"
                },
                "summary": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10242v1",
                "updated": "2025-03-13T10:34:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    34,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:34:43Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    34,
                    43,
                    3,
                    72,
                    0
                ],
                "title": "MinorBench: A hand-built benchmark for content-based risks for children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MinorBench: A hand-built benchmark for content-based risks for children"
                },
                "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users."
                },
                "authors": [
                    {
                        "name": "Shaun Khoo"
                    },
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Rachel Shong"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Shong"
                },
                "author": "Rachel Shong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10241v1",
                "updated": "2025-03-13T10:32:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    32,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    32,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning"
                },
                "summary": "Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning."
                },
                "authors": [
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Sabrina Patania"
                    },
                    {
                        "name": "Luca Annese"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Franca Garzotto"
                    },
                    {
                        "name": "Giuseppe Vizzari"
                    },
                    {
                        "name": "Azzurra Ruggeri"
                    },
                    {
                        "name": "Simone Colombani"
                    }
                ],
                "author_detail": {
                    "name": "Simone Colombani"
                },
                "author": "Simone Colombani",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10225v1",
                "updated": "2025-03-13T10:08:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    8,
                    18,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T10:08:18Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    10,
                    8,
                    18,
                    3,
                    72,
                    0
                ],
                "title": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA"
                },
                "summary": "Amodal segmentation aims to infer the complete shape of occluded objects,\neven when the occluded region's appearance is unavailable. However, current\namodal segmentation methods lack the capability to interact with users through\ntext input and struggle to understand or reason about implicit and complex\npurposes. While methods like LISA integrate multi-modal large language models\n(LLMs) with segmentation for reasoning tasks, they are limited to predicting\nonly visible object regions and face challenges in handling complex occlusion\nscenarios. To address these limitations, we propose a novel task named amodal\nreasoning segmentation, aiming to predict the complete amodal shape of occluded\nobjects while providing answers with elaborations based on user text input. We\ndevelop a generalizable dataset generation pipeline and introduce a new dataset\nfocusing on daily life scenarios, encompassing diverse real-world occlusions.\nFurthermore, we present AURA (Amodal Understanding and Reasoning Assistant), a\nnovel model with advanced global and spatial-level designs specifically\ntailored to handle complex occlusions. Extensive experiments validate AURA's\neffectiveness on the proposed dataset. The code, model, and dataset will be\npublicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amodal segmentation aims to infer the complete shape of occluded objects,\neven when the occluded region's appearance is unavailable. However, current\namodal segmentation methods lack the capability to interact with users through\ntext input and struggle to understand or reason about implicit and complex\npurposes. While methods like LISA integrate multi-modal large language models\n(LLMs) with segmentation for reasoning tasks, they are limited to predicting\nonly visible object regions and face challenges in handling complex occlusion\nscenarios. To address these limitations, we propose a novel task named amodal\nreasoning segmentation, aiming to predict the complete amodal shape of occluded\nobjects while providing answers with elaborations based on user text input. We\ndevelop a generalizable dataset generation pipeline and introduce a new dataset\nfocusing on daily life scenarios, encompassing diverse real-world occlusions.\nFurthermore, we present AURA (Amodal Understanding and Reasoning Assistant), a\nnovel model with advanced global and spatial-level designs specifically\ntailored to handle complex occlusions. Extensive experiments validate AURA's\neffectiveness on the proposed dataset. The code, model, and dataset will be\npublicly released."
                },
                "authors": [
                    {
                        "name": "Zhixuan Li"
                    },
                    {
                        "name": "Hyunse Yoon"
                    },
                    {
                        "name": "Sanghoon Lee"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "arxiv_comment": "11 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10217v1",
                "updated": "2025-03-13T09:59:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    16,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:59:16Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    59,
                    16,
                    3,
                    72,
                    0
                ],
                "title": "Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout"
                },
                "summary": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Xianjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xianjun Gao"
                },
                "author": "Xianjun Gao",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10211v1",
                "updated": "2025-03-13T09:54:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    54,
                    35,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:54:35Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    54,
                    35,
                    3,
                    72,
                    0
                ],
                "title": "Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation"
                },
                "summary": "Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning."
                },
                "authors": [
                    {
                        "name": "Henglyu Liu"
                    },
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Yuan Qiu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01143v2",
                "updated": "2025-03-13T09:41:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    41,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-09-02T10:27:47Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    27,
                    47,
                    0,
                    246,
                    0
                ],
                "title": "HexiScale: Accommodating Large Language Model Training over\n  Heterogeneous Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexiScale: Accommodating Large Language Model Training over\n  Heterogeneous Environment"
                },
                "summary": "Training large language model (LLM) is a computationally intensive task,\nwhich is typically conducted in data centers with homogeneous high-performance\nGPUs. We explore an alternative approach by deploying training computations\nacross heterogeneous GPUs to enable better flexibility and efficiency for\nheterogeneous resource utilization. To achieve this goal, we propose a novel\nsystem, HexiScale, that can flexibly support asymmetric partition of training\ncomputations in the scope of data-, pipeline-, and tensor model parallelism. We\nfurther formalize the allocation of asymmetric partitioned training\ncomputations over a set of heterogeneous GPUs as a constrained optimization\nproblem and propose an efficient hierarchical graph partitioning algorithm. Our\napproach effectively allocates training computations across GPUs, fully\nleveraging the available computational power. We conduct empirical studies to\nevaluate the performance of HexiScale with state-of-the-art homogeneous and\nheterogeneous training systems. When training LLMs at different scales (from 7B\nto 30B), HexiScale achieves comparable MFU when running over heterogeneous GPUs\ncompared to state-of-the-art training systems running over homogeneous\nhigh-performance GPUs with the same total peak FLOPS. The percentage gaps in\nMFU between HexiScale and comparable homogeneous settings are as low as\n$0.3\\%$, with an average of $3.5\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language model (LLM) is a computationally intensive task,\nwhich is typically conducted in data centers with homogeneous high-performance\nGPUs. We explore an alternative approach by deploying training computations\nacross heterogeneous GPUs to enable better flexibility and efficiency for\nheterogeneous resource utilization. To achieve this goal, we propose a novel\nsystem, HexiScale, that can flexibly support asymmetric partition of training\ncomputations in the scope of data-, pipeline-, and tensor model parallelism. We\nfurther formalize the allocation of asymmetric partitioned training\ncomputations over a set of heterogeneous GPUs as a constrained optimization\nproblem and propose an efficient hierarchical graph partitioning algorithm. Our\napproach effectively allocates training computations across GPUs, fully\nleveraging the available computational power. We conduct empirical studies to\nevaluate the performance of HexiScale with state-of-the-art homogeneous and\nheterogeneous training systems. When training LLMs at different scales (from 7B\nto 30B), HexiScale achieves comparable MFU when running over heterogeneous GPUs\ncompared to state-of-the-art training systems running over homogeneous\nhigh-performance GPUs with the same total peak FLOPS. The percentage gaps in\nMFU between HexiScale and comparable homogeneous settings are as low as\n$0.3\\%$, with an average of $3.5\\%$."
                },
                "authors": [
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19339v2",
                "updated": "2025-03-13T09:40:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    40,
                    42,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-26T17:32:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets"
                },
                "summary": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types."
                },
                "authors": [
                    {
                        "name": "Tohida Rehman"
                    },
                    {
                        "name": "Soumabha Ghosh"
                    },
                    {
                        "name": "Kuntal Das"
                    },
                    {
                        "name": "Souvik Bhattacharjee"
                    },
                    {
                        "name": "Debarshi Kumar Sanyal"
                    },
                    {
                        "name": "Samiran Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Samiran Chattopadhyay"
                },
                "author": "Samiran Chattopadhyay",
                "arxiv_comment": "5 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14529v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14529v3",
                "updated": "2025-03-13T09:32:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    32,
                    39,
                    3,
                    72,
                    0
                ],
                "published": "2024-05-23T13:15:13Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    15,
                    13,
                    3,
                    144,
                    0
                ],
                "title": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2"
                },
                "summary": "Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nfollows the well-established patch-level deep nearest neighbor paradigm, and\nenables both image-level anomaly prediction and pixel-level anomaly\nsegmentation. The approach is methodologically simple and training-free and,\nthus, does not require any additional data for fine-tuning or meta-learning.\nThe approach is methodologically simple and training-free and, thus, does not\nrequire any additional data for fine-tuning or meta-learning. Despite its\nsimplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot\nanomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an\nAUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding\nfew-shot performance, makes AnomalyDINO a strong candidate for fast deployment,\ne.g., in industrial contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nfollows the well-established patch-level deep nearest neighbor paradigm, and\nenables both image-level anomaly prediction and pixel-level anomaly\nsegmentation. The approach is methodologically simple and training-free and,\nthus, does not require any additional data for fine-tuning or meta-learning.\nThe approach is methodologically simple and training-free and, thus, does not\nrequire any additional data for fine-tuning or meta-learning. Despite its\nsimplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot\nanomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an\nAUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding\nfew-shot performance, makes AnomalyDINO a strong candidate for fast deployment,\ne.g., in industrial contexts."
                },
                "authors": [
                    {
                        "name": "Simon Damm"
                    },
                    {
                        "name": "Mike Laszkiewicz"
                    },
                    {
                        "name": "Johannes Lederer"
                    },
                    {
                        "name": "Asja Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Asja Fischer"
                },
                "author": "Asja Fischer",
                "arxiv_comment": "Accepted at WACV 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14529v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14529v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10195v1",
                "updated": "2025-03-13T09:28:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    28,
                    42,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:28:42Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    28,
                    42,
                    3,
                    72,
                    0
                ],
                "title": "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation"
                },
                "summary": "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications."
                },
                "authors": [
                    {
                        "name": "Hongze Sun"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wuque Cai"
                    },
                    {
                        "name": "Duo Chen"
                    },
                    {
                        "name": "Qianqian Liao"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Yan Cui"
                    },
                    {
                        "name": "Dezhong Yao"
                    },
                    {
                        "name": "Daqing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Daqing Guo"
                },
                "author": "Daqing Guo",
                "arxiv_comment": "12 pages, 5 figures, 5 tables; This work has been submitted for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10185v1",
                "updated": "2025-03-13T09:14:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    14,
                    29,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T09:14:29Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    14,
                    29,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Reward Allocation via Proportional Splitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Reward Allocation via Proportional Splitting"
                },
                "summary": "Following the publication of Bitcoin's arguably most famous attack, selfish\nmining, various works have introduced mechanisms to enhance blockchain systems'\ngame theoretic resilience. Some reward mechanisms, like FruitChains, have been\nshown to be equilibria in theory. However, their guarantees assume\nnon-realistic parameters and their performance degrades significantly in a\npractical deployment setting. In this work we introduce a reward allocation\nmechanism, called Proportional Splitting (PRS), which outperforms existing\nstate of the art. We show that, for large enough parameters, PRS is an\nequilibrium, offering the same theoretical guarantees as the state of the art.\nIn addition, for practical, realistically small, parameters, PRS outperforms\nall existing reward mechanisms across an array of metrics. We implement PRS on\ntop of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more\naccurate estimation of each party's mining power compared to e.g., Bitcoin. We\nthen evaluate PRS both theoretically and in practice. On the theoretical side,\nwe show that our protocol combined with PRS is an equilibrium and guarantees\nfairness, similar to FruitChains. In practice, we compare PRS with an array of\nexisting reward mechanisms and show that, assuming an accurate estimation of\nthe mining power distribution, it outperforms them across various\nwell-established metrics. Finally, we realize this assumption by approximating\nthe power distribution via low-work objects called \"workshares\" and quantify\nthe tradeoff between the approximation's accuracy and storage overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the publication of Bitcoin's arguably most famous attack, selfish\nmining, various works have introduced mechanisms to enhance blockchain systems'\ngame theoretic resilience. Some reward mechanisms, like FruitChains, have been\nshown to be equilibria in theory. However, their guarantees assume\nnon-realistic parameters and their performance degrades significantly in a\npractical deployment setting. In this work we introduce a reward allocation\nmechanism, called Proportional Splitting (PRS), which outperforms existing\nstate of the art. We show that, for large enough parameters, PRS is an\nequilibrium, offering the same theoretical guarantees as the state of the art.\nIn addition, for practical, realistically small, parameters, PRS outperforms\nall existing reward mechanisms across an array of metrics. We implement PRS on\ntop of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more\naccurate estimation of each party's mining power compared to e.g., Bitcoin. We\nthen evaluate PRS both theoretically and in practice. On the theoretical side,\nwe show that our protocol combined with PRS is an equilibrium and guarantees\nfairness, similar to FruitChains. In practice, we compare PRS with an array of\nexisting reward mechanisms and show that, assuming an accurate estimation of\nthe mining power distribution, it outperforms them across various\nwell-established metrics. Finally, we realize this assumption by approximating\nthe power distribution via low-work objects called \"workshares\" and quantify\nthe tradeoff between the approximation's accuracy and storage overhead."
                },
                "authors": [
                    {
                        "name": "Lukas Aumayr"
                    },
                    {
                        "name": "Zeta Avarikioti"
                    },
                    {
                        "name": "Dimitris Karakostas"
                    },
                    {
                        "name": "Karl Kreder"
                    },
                    {
                        "name": "Shreekara Shastry"
                    }
                ],
                "author_detail": {
                    "name": "Shreekara Shastry"
                },
                "author": "Shreekara Shastry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10177v1",
                "updated": "2025-03-13T08:58:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    58,
                    10,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T08:58:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    58,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning"
                },
                "summary": "We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Yanjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Chen"
                },
                "author": "Yanjun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10167v1",
                "updated": "2025-03-13T08:46:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    46,
                    32,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T08:46:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    46,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding"
                },
                "summary": "Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques."
                },
                "authors": [
                    {
                        "name": "Hyunbin Jin"
                    },
                    {
                        "name": "Je Won Yeom"
                    },
                    {
                        "name": "Seunghyun Bae"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08426v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08426v5",
                "updated": "2025-03-13T08:45:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    45,
                    35,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-12T17:13:17Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    13,
                    17,
                    2,
                    164,
                    0
                ],
                "title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL"
                },
                "summary": "Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions."
                },
                "authors": [
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08426v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08426v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10800v2",
                "updated": "2025-03-13T08:43:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    43,
                    27,
                    3,
                    72,
                    0
                ],
                "published": "2025-01-18T15:39:53Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    15,
                    39,
                    53,
                    5,
                    18,
                    0
                ],
                "title": "Jailbreaking Large Language Models in Infinitely Many Ways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Large Language Models in Infinitely Many Ways"
                },
                "summary": "We discuss the ``Infinitely Many Paraphrases'' attacks (IMP), a category of\njailbreaks that leverages the increasing capabilities of a model to handle\nparaphrases and encoded communications to bypass their defensive mechanisms.\nIMPs' viability pairs and grows with a model's capabilities to handle and bind\nthe semantics of simple mappings between tokens and work extremely well in\npractice, posing a concrete threat to the users of the most powerful LLMs in\ncommerce. We show how one can bypass the safeguards of the most powerful open-\nand closed-source LLMs and generate content that explicitly violates their\nsafety policies. One can protect against IMPs by improving the guardrails and\nmaking them scale with the LLMs' capabilities. For two categories of attacks\nthat are straightforward to implement, i.e., bijection and encoding, we discuss\ntwo defensive strategies, one in token and the other in embedding space. We\nconclude with some research questions we believe should be prioritised to\nenhance the defensive mechanisms of LLMs and our understanding of their safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss the ``Infinitely Many Paraphrases'' attacks (IMP), a category of\njailbreaks that leverages the increasing capabilities of a model to handle\nparaphrases and encoded communications to bypass their defensive mechanisms.\nIMPs' viability pairs and grows with a model's capabilities to handle and bind\nthe semantics of simple mappings between tokens and work extremely well in\npractice, posing a concrete threat to the users of the most powerful LLMs in\ncommerce. We show how one can bypass the safeguards of the most powerful open-\nand closed-source LLMs and generate content that explicitly violates their\nsafety policies. One can protect against IMPs by improving the guardrails and\nmaking them scale with the LLMs' capabilities. For two categories of attacks\nthat are straightforward to implement, i.e., bijection and encoding, we discuss\ntwo defensive strategies, one in token and the other in embedding space. We\nconclude with some research questions we believe should be prioritised to\nenhance the defensive mechanisms of LLMs and our understanding of their safety."
                },
                "authors": [
                    {
                        "name": "Oliver Goldstein"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Felix Drinkall"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10150v1",
                "updated": "2025-03-13T08:22:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T08:22:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Hierarchical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Hierarchical Knowledge"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods. The code of our proposed method is available at\n\\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods. The code of our proposed method is available at\n\\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Kaili Ma"
                    },
                    {
                        "name": "Hongzhi Chen"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06828v2",
                "updated": "2025-03-13T08:16:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    16,
                    1,
                    3,
                    72,
                    0
                ],
                "published": "2025-01-12T14:45:27Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    14,
                    45,
                    27,
                    6,
                    12,
                    0
                ],
                "title": "GeoPix: Multi-Modal Large Language Model for Pixel-level Image\n  Understanding in Remote Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoPix: Multi-Modal Large Language Model for Pixel-level Image\n  Understanding in Remote Sensing"
                },
                "summary": "Multi-modal large language models (MLLMs) have achieved remarkable success in\nimage- and region-level remote sensing (RS) image understanding tasks, such as\nimage captioning, visual question answering, and visual grounding. However,\nexisting RS MLLMs lack the pixel-level dialogue capability, which involves\nresponding to user instructions with segmentation masks for specific instances.\nIn this paper, we propose GeoPix, a RS MLLM that extends image understanding\ncapabilities to the pixel level. This is achieved by equipping the MLLM with a\nmask predictor, which transforms visual features from the vision encoder into\nmasks conditioned on the LLM's segmentation token embeddings. To facilitate the\nsegmentation of multi-scale objects in RS imagery, a class-wise learnable\nmemory module is integrated into the mask predictor to capture and store\nclass-wise geo-context at the instance level across the entire dataset. In\naddition, to address the absence of large-scale datasets for training\npixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising\n65,463 images and 140,412 instances, with each instance annotated with text\ndescriptions, bounding boxes, and masks. Furthermore, we develop a two-stage\ntraining strategy to balance the distinct requirements of text generation and\nmasks prediction in multi-modal multi-task optimization. Extensive experiments\nverify the effectiveness and superiority of GeoPix in pixel-level segmentation\ntasks, while also maintaining competitive performance in image- and\nregion-level benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have achieved remarkable success in\nimage- and region-level remote sensing (RS) image understanding tasks, such as\nimage captioning, visual question answering, and visual grounding. However,\nexisting RS MLLMs lack the pixel-level dialogue capability, which involves\nresponding to user instructions with segmentation masks for specific instances.\nIn this paper, we propose GeoPix, a RS MLLM that extends image understanding\ncapabilities to the pixel level. This is achieved by equipping the MLLM with a\nmask predictor, which transforms visual features from the vision encoder into\nmasks conditioned on the LLM's segmentation token embeddings. To facilitate the\nsegmentation of multi-scale objects in RS imagery, a class-wise learnable\nmemory module is integrated into the mask predictor to capture and store\nclass-wise geo-context at the instance level across the entire dataset. In\naddition, to address the absence of large-scale datasets for training\npixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising\n65,463 images and 140,412 instances, with each instance annotated with text\ndescriptions, bounding boxes, and masks. Furthermore, we develop a two-stage\ntraining strategy to balance the distinct requirements of text generation and\nmasks prediction in multi-modal multi-task optimization. Extensive experiments\nverify the effectiveness and superiority of GeoPix in pixel-level segmentation\ntasks, while also maintaining competitive performance in image- and\nregion-level benchmarks."
                },
                "authors": [
                    {
                        "name": "Ruizhe Ou"
                    },
                    {
                        "name": "Yuan Hu"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jiaxin Chen"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10135v1",
                "updated": "2025-03-13T07:55:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    55,
                    38,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T07:55:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    55,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding"
                },
                "summary": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06942v3",
                "updated": "2025-03-13T07:42:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    42,
                    29,
                    3,
                    72,
                    0
                ],
                "published": "2024-07-09T15:19:09Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    15,
                    19,
                    9,
                    1,
                    191,
                    0
                ],
                "title": "An Improved Two-Step Attack on Lattice-Based Cryptography: A Case Study\n  of Kyber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Improved Two-Step Attack on Lattice-Based Cryptography: A Case Study\n  of Kyber"
                },
                "summary": "After three rounds of post-quantum cryptography (PQC) strict evaluations\nconducted by NIST, CRYSTALS-Kyber was successfully selected in July 2022 and\nstandardized in August 2024. It becomes urgent to further evaluate Kyber's\nphysical security for the upcoming deployment phase. In this brief, we present\nan improved two-step attack on Kyber to quickly recover the full secret key, s,\nby using much fewer power traces and less time. In the first step, we use the\ncorrelation power analysis (CPA) to obtain a portion of guess values of s with\na small number of power traces. The CPA is enhanced by utilizing both Pearson\nand Kendall's rank correlation coefficients and modifying the leakage model to\nimprove the accuracy. In the second step, we adopt the lattice attack to\nrecover s based on the results of CPA. The success rate is largely built up by\nconstructing a trial-and-error method. We deploy the reference implementations\nof Kyber-512, -768, and -1024 on an ARM Cortex-M4 target board and successfully\nrecover s in approximately 9~10 minutes with at most 15 power traces, using a\nXeon Gold 6342-equipped machine for the attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After three rounds of post-quantum cryptography (PQC) strict evaluations\nconducted by NIST, CRYSTALS-Kyber was successfully selected in July 2022 and\nstandardized in August 2024. It becomes urgent to further evaluate Kyber's\nphysical security for the upcoming deployment phase. In this brief, we present\nan improved two-step attack on Kyber to quickly recover the full secret key, s,\nby using much fewer power traces and less time. In the first step, we use the\ncorrelation power analysis (CPA) to obtain a portion of guess values of s with\na small number of power traces. The CPA is enhanced by utilizing both Pearson\nand Kendall's rank correlation coefficients and modifying the leakage model to\nimprove the accuracy. In the second step, we adopt the lattice attack to\nrecover s based on the results of CPA. The success rate is largely built up by\nconstructing a trial-and-error method. We deploy the reference implementations\nof Kyber-512, -768, and -1024 on an ARM Cortex-M4 target board and successfully\nrecover s in approximately 9~10 minutes with at most 15 power traces, using a\nXeon Gold 6342-equipped machine for the attack."
                },
                "authors": [
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Dejun Xu"
                    },
                    {
                        "name": "Jing Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tian"
                },
                "author": "Jing Tian",
                "arxiv_doi": "10.1109/TCAD.2025.3550443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCAD.2025.3550443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE TCAD",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04779v2",
                "updated": "2025-03-13T07:41:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    41,
                    37,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-22T13:27:31Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    13,
                    27,
                    31,
                    5,
                    53,
                    0
                ],
                "title": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of\n  LLMs on Formal Specification Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of\n  LLMs on Formal Specification Inference"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%."
                },
                "authors": [
                    {
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Toby Murray"
                    }
                ],
                "author_detail": {
                    "name": "Toby Murray"
                },
                "author": "Toby Murray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10120v1",
                "updated": "2025-03-13T07:28:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    28,
                    33,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T07:28:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    28,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Hybrid Agents for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Agents for Image Restoration"
                },
                "summary": "Existing Image Restoration (IR) studies typically focus on task-specific or\nuniversal modes individually, relying on the mode selection of users and\nlacking the cooperation between multiple task-specific/universal restoration\nmodes. This leads to insufficient interaction for unprofessional users and\nlimits their restoration capability for complicated real-world applications. In\nthis work, we present HybridAgent, intending to incorporate multiple\nrestoration modes into a unified image restoration model and achieve\nintelligent and efficient user interaction through our proposed hybrid agents.\nConcretely, we propose the hybrid rule of fast, slow, and feedback restoration\nagents. Here, the slow restoration agent optimizes the powerful multimodal\nlarge language model (MLLM) with our proposed instruction-tuning dataset to\nidentify degradations within images with ambiguous user prompts and invokes\nproper restoration tools accordingly. The fast restoration agent is designed\nbased on a lightweight large language model (LLM) via in-context learning to\nunderstand the user prompts with simple and clear requirements, which can\nobviate the unnecessary time/resource costs of MLLM. Moreover, we introduce the\nmixed distortion removal mode for our HybridAgents, which is crucial but not\nconcerned in previous agent-based works. It can effectively prevent the error\npropagation of step-by-step image restoration and largely improve the\nefficiency of the agent system. We validate the effectiveness of HybridAgent\nwith both synthetic and real-world IR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Image Restoration (IR) studies typically focus on task-specific or\nuniversal modes individually, relying on the mode selection of users and\nlacking the cooperation between multiple task-specific/universal restoration\nmodes. This leads to insufficient interaction for unprofessional users and\nlimits their restoration capability for complicated real-world applications. In\nthis work, we present HybridAgent, intending to incorporate multiple\nrestoration modes into a unified image restoration model and achieve\nintelligent and efficient user interaction through our proposed hybrid agents.\nConcretely, we propose the hybrid rule of fast, slow, and feedback restoration\nagents. Here, the slow restoration agent optimizes the powerful multimodal\nlarge language model (MLLM) with our proposed instruction-tuning dataset to\nidentify degradations within images with ambiguous user prompts and invokes\nproper restoration tools accordingly. The fast restoration agent is designed\nbased on a lightweight large language model (LLM) via in-context learning to\nunderstand the user prompts with simple and clear requirements, which can\nobviate the unnecessary time/resource costs of MLLM. Moreover, we introduce the\nmixed distortion removal mode for our HybridAgents, which is crucial but not\nconcerned in previous agent-based works. It can effectively prevent the error\npropagation of step-by-step image restoration and largely improve the\nefficiency of the agent system. We validate the effectiveness of HybridAgent\nwith both synthetic and real-world IR tasks."
                },
                "authors": [
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10118v1",
                "updated": "2025-03-13T07:27:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    27,
                    5,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T07:27:05Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    27,
                    5,
                    3,
                    72,
                    0
                ],
                "title": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy\n  Transfer with Differentiable Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy\n  Transfer with Differentiable Simulation"
                },
                "summary": "The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties."
                },
                "authors": [
                    {
                        "name": "Lu Shi"
                    },
                    {
                        "name": "Yuxuan Xu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Wenhao Zhao"
                    },
                    {
                        "name": "Yufei Jia"
                    },
                    {
                        "name": "Zike Yan"
                    },
                    {
                        "name": "Weibin Gu"
                    },
                    {
                        "name": "Guyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guyue Zhou"
                },
                "author": "Guyue Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18883v2",
                "updated": "2025-03-13T07:23:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    23,
                    59,
                    3,
                    72,
                    0
                ],
                "published": "2025-01-31T04:34:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    34,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Predictive Prompt Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Prompt Analysis"
                },
                "summary": "Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers."
                },
                "authors": [
                    {
                        "name": "Jae Yong Lee"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "Accepted by FSE 2025, 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04863v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04863v6",
                "updated": "2025-03-13T07:05:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    5,
                    15,
                    3,
                    72,
                    0
                ],
                "published": "2024-02-07T13:58:26Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    13,
                    58,
                    26,
                    2,
                    38,
                    0
                ],
                "title": "SCLA: Automated Smart Contract Summarization via LLMs and Control Flow\n  Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCLA: Automated Smart Contract Summarization via LLMs and Control Flow\n  Prompt"
                },
                "summary": "Smart contract code summarization is crucial for efficient maintenance and\nvulnerability mitigation. While many studies use Large Language Models (LLMs)\nfor summarization, their performance still falls short compared to fine-tuned\nmodels like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow\nanalysis but fail to fully capture the hierarchy and control structures of the\ncode, leading to information loss and degraded summarization quality. We\npropose SCLA, an LLM-based method that enhances summarization by integrating a\nControl Flow Graph (CFG) and semantic facts from the code's control flow into a\nsemantically enriched prompt. SCLA uses a control flow extraction algorithm to\nderive control flows from semantic nodes in the Abstract Syntax Tree (AST) and\nconstructs the corresponding CFG. Code semantic facts refer to both explicit\nand implicit information within the AST that is relevant to smart contracts.\nThis method enables LLMs to better capture the structural and contextual\ndependencies of the code. We validate the effectiveness of SCLA through\ncomprehensive experiments on a dataset of 40,000 real-world smart contracts.\nThe experiment shows that SCLA significantly improves summarization quality,\noutperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and\n14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract code summarization is crucial for efficient maintenance and\nvulnerability mitigation. While many studies use Large Language Models (LLMs)\nfor summarization, their performance still falls short compared to fine-tuned\nmodels like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow\nanalysis but fail to fully capture the hierarchy and control structures of the\ncode, leading to information loss and degraded summarization quality. We\npropose SCLA, an LLM-based method that enhances summarization by integrating a\nControl Flow Graph (CFG) and semantic facts from the code's control flow into a\nsemantically enriched prompt. SCLA uses a control flow extraction algorithm to\nderive control flows from semantic nodes in the Abstract Syntax Tree (AST) and\nconstructs the corresponding CFG. Code semantic facts refer to both explicit\nand implicit information within the AST that is relevant to smart contracts.\nThis method enables LLMs to better capture the structural and contextual\ndependencies of the code. We validate the effectiveness of SCLA through\ncomprehensive experiments on a dataset of 40,000 real-world smart contracts.\nThe experiment shows that SCLA significantly improves summarization quality,\noutperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and\n14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaoqi Li"
                    },
                    {
                        "name": "Yingjie Mao"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Zongwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Zongwei Li"
                },
                "author": "Zongwei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04863v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04863v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10105v1",
                "updated": "2025-03-13T07:02:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    2,
                    53,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T07:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    2,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error"
                },
                "summary": "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https://github.com/SHU-XUN/StepMathAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https://github.com/SHU-XUN/StepMathAgent."
                },
                "authors": [
                    {
                        "name": "Shu-Xun Yang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06669v2",
                "updated": "2025-03-13T06:59:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    59,
                    16,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-09T15:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    15,
                    40,
                    29,
                    6,
                    68,
                    0
                ],
                "title": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems"
                },
                "summary": "We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence."
                },
                "authors": [
                    {
                        "name": "AgiBot-World-Contributors"
                    },
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Xiuqi Cui"
                    },
                    {
                        "name": "Yan Ding"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Xindong He"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Cheng Jing"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Chiming Liu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yuxiang Lu"
                    },
                    {
                        "name": "Jianlan Luo"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Yuehan Niu"
                    },
                    {
                        "name": "Yixuan Pan"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Cheng Ruan"
                    },
                    {
                        "name": "Jiaqi Shan"
                    },
                    {
                        "name": "Yongjian Shen"
                    },
                    {
                        "name": "Chengshi Shi"
                    },
                    {
                        "name": "Mingkang Shi"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Chonghao Sima"
                    },
                    {
                        "name": "Jianheng Song"
                    },
                    {
                        "name": "Huijie Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dafeng Wei"
                    },
                    {
                        "name": "Chengen Xie"
                    },
                    {
                        "name": "Guo Xu"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Cunbiao Yang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Shukai Yang"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Chengyue Zhao"
                    },
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Jianchao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianchao Zhu"
                },
                "author": "Jianchao Zhu",
                "arxiv_comment": "Project website: https://agibot-world.com/. Github repo:\n  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered\n  alphabetically by surname, with detailed contributions provided in the\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10095v1",
                "updated": "2025-03-13T06:42:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    42,
                    37,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T06:42:37Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    42,
                    37,
                    3,
                    72,
                    0
                ],
                "title": "Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Avinash Patil"
                    },
                    {
                        "name": "Amardeep Kour Gedhu"
                    }
                ],
                "author_detail": {
                    "name": "Amardeep Kour Gedhu"
                },
                "author": "Amardeep Kour Gedhu",
                "arxiv_comment": "8 pages, 4 Figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12854v2",
                "updated": "2025-03-13T06:40:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    40,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-10T22:22:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    22,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees"
                },
                "summary": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10093v1",
                "updated": "2025-03-13T06:40:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    40,
                    34,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T06:40:34Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    40,
                    34,
                    3,
                    72,
                    0
                ],
                "title": "Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model"
                },
                "summary": "Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads."
                },
                "authors": [
                    {
                        "name": "Qiyuan Deng"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Liqiang Nie"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07589v2",
                "updated": "2025-03-13T06:23:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    23,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    24,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation"
                },
                "summary": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/."
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "[CVPR 2025] The project page is\n  https://jianzongwu.github.io/projects/diffsensei/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04954v2",
                "updated": "2025-03-13T06:11:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    11,
                    16,
                    3,
                    72,
                    0
                ],
                "published": "2024-11-07T18:31:08Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    31,
                    8,
                    3,
                    312,
                    0
                ],
                "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM"
                },
                "summary": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/"
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Shenghua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Gao"
                },
                "author": "Shenghua Gao",
                "arxiv_comment": "Project page: https://cad-mllm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10084v1",
                "updated": "2025-03-13T06:11:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    11,
                    10,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T06:11:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    11,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt\n  Space Complexity, its Interaction with Answer Space During CoT Reasoning with\n  LLMs: A Recurrent Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt\n  Space Complexity, its Interaction with Answer Space During CoT Reasoning with\n  LLMs: A Recurrent Perspective"
                },
                "summary": "Despite the remarkable successes of Large Language Models (LLMs), their\nfundamental Transformer architecture possesses inherent theoretical limitations\nthat restrict their capability to handle reasoning tasks with increasing\ncomputational complexity. Chain-of-Thought (CoT) prompting has emerged as a\npractical solution, supported by several theoretical studies. However, current\nCoT-based methods (including ToT, GoT, etc.) generally adopt a\n\"one-prompt-fits-all\" strategy, using fixed templates (e.g., \"think step by\nstep\") across diverse reasoning tasks. This method forces models to navigate an\nextremely complex prompt space to identify effective reasoning paths. The\ncurrent prompt designing research are also heavily relying on trial-and-error\nrather than theoretically informed guidance. In this paper, we provide a\nrigorous theoretical analysis of the complexity and interplay between two\ncrucial spaces: the prompt space (the space of potential prompt structures) and\nthe answer space (the space of reasoning solutions generated by LLMs) in CoT\nreasoning. We demonstrate how reliance on a single universal prompt (e.g. think\nstep by step) can negatively impact the theoretical computability of LLMs,\nillustrating that prompt complexity directly influences the structure and\neffectiveness of the navigation in answer space. Our analysis highlights that\nsometimes human supervision is critical for efficiently navigating the prompt\nspace. We theoretically and empirically show that task-specific prompting\nsignificantly outperforms unsupervised prompt generation, emphasizing the\nnecessity of thoughtful human guidance in CoT prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable successes of Large Language Models (LLMs), their\nfundamental Transformer architecture possesses inherent theoretical limitations\nthat restrict their capability to handle reasoning tasks with increasing\ncomputational complexity. Chain-of-Thought (CoT) prompting has emerged as a\npractical solution, supported by several theoretical studies. However, current\nCoT-based methods (including ToT, GoT, etc.) generally adopt a\n\"one-prompt-fits-all\" strategy, using fixed templates (e.g., \"think step by\nstep\") across diverse reasoning tasks. This method forces models to navigate an\nextremely complex prompt space to identify effective reasoning paths. The\ncurrent prompt designing research are also heavily relying on trial-and-error\nrather than theoretically informed guidance. In this paper, we provide a\nrigorous theoretical analysis of the complexity and interplay between two\ncrucial spaces: the prompt space (the space of potential prompt structures) and\nthe answer space (the space of reasoning solutions generated by LLMs) in CoT\nreasoning. We demonstrate how reliance on a single universal prompt (e.g. think\nstep by step) can negatively impact the theoretical computability of LLMs,\nillustrating that prompt complexity directly influences the structure and\neffectiveness of the navigation in answer space. Our analysis highlights that\nsometimes human supervision is critical for efficiently navigating the prompt\nspace. We theoretically and empirically show that task-specific prompting\nsignificantly outperforms unsupervised prompt generation, emphasizing the\nnecessity of thoughtful human guidance in CoT prompting."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Juntai Cao"
                    },
                    {
                        "name": "Jiaqi Wei"
                    },
                    {
                        "name": "Chenyu You"
                    },
                    {
                        "name": "Dujian Ding"
                    }
                ],
                "author_detail": {
                    "name": "Dujian Ding"
                },
                "author": "Dujian Ding",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2410.14198",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08202v3",
                "updated": "2025-03-13T06:09:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    9,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-10T17:59:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training"
                },
                "summary": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://github.com/OpenGVLab/Mono-InternVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://github.com/OpenGVLab/Mono-InternVL."
                },
                "authors": [
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xizhou Zhu"
                },
                "author": "Xizhou Zhu",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09022v2",
                "updated": "2025-03-13T05:55:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    55,
                    55,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T03:20:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    20,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tingsong Xiao"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear at IEEE Symposium on Security and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10076v1",
                "updated": "2025-03-13T05:54:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    54,
                    42,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:54:42Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    54,
                    42,
                    3,
                    72,
                    0
                ],
                "title": "VMBench: A Benchmark for Perception-Aligned Video Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMBench: A Benchmark for Perception-Aligned Video Motion Generation"
                },
                "summary": "Video generation has advanced rapidly, improving evaluation methods, yet\nassessing video's motion remains a major challenge. Specifically, there are two\nkey issues: 1) current motion metrics do not fully align with human\nperceptions; 2) the existing motion prompts are limited. Based on these\nfindings, we introduce VMBench--a comprehensive Video Motion Benchmark that has\nperception-aligned motion metrics and features the most diverse types of\nmotion. VMBench has several appealing properties: 1) Perception-Driven Motion\nEvaluation Metrics, we identify five dimensions based on human perception in\nmotion video assessment and develop fine-grained evaluation metrics, providing\ndeeper insights into models' strengths and weaknesses in motion quality. 2)\nMeta-Guided Motion Prompt Generation, a structured method that extracts\nmeta-information, generates diverse motion prompts with LLMs, and refines them\nthrough human-AI validation, resulting in a multi-level prompt library covering\nsix key dynamic scene dimensions. 3) Human-Aligned Validation Mechanism, we\nprovide human preference annotations to validate our benchmarks, with our\nmetrics achieving an average 35.3% improvement in Spearman's correlation over\nbaseline methods. This is the first time that the quality of motion in videos\nhas been evaluated from the perspective of human perception alignment.\nAdditionally, we will soon release VMBench at\nhttps://github.com/GD-AIGC/VMBench, setting a new standard for evaluating and\nadvancing motion generation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has advanced rapidly, improving evaluation methods, yet\nassessing video's motion remains a major challenge. Specifically, there are two\nkey issues: 1) current motion metrics do not fully align with human\nperceptions; 2) the existing motion prompts are limited. Based on these\nfindings, we introduce VMBench--a comprehensive Video Motion Benchmark that has\nperception-aligned motion metrics and features the most diverse types of\nmotion. VMBench has several appealing properties: 1) Perception-Driven Motion\nEvaluation Metrics, we identify five dimensions based on human perception in\nmotion video assessment and develop fine-grained evaluation metrics, providing\ndeeper insights into models' strengths and weaknesses in motion quality. 2)\nMeta-Guided Motion Prompt Generation, a structured method that extracts\nmeta-information, generates diverse motion prompts with LLMs, and refines them\nthrough human-AI validation, resulting in a multi-level prompt library covering\nsix key dynamic scene dimensions. 3) Human-Aligned Validation Mechanism, we\nprovide human preference annotations to validate our benchmarks, with our\nmetrics achieving an average 35.3% improvement in Spearman's correlation over\nbaseline methods. This is the first time that the quality of motion in videos\nhas been evaluated from the perspective of human perception alignment.\nAdditionally, we will soon release VMBench at\nhttps://github.com/GD-AIGC/VMBench, setting a new standard for evaluating and\nadvancing motion generation models."
                },
                "authors": [
                    {
                        "name": "Xinrang Ling"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Hangyu Li"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Cundian Yang"
                    },
                    {
                        "name": "Aiming Hao"
                    },
                    {
                        "name": "Jiashu Zhu"
                    },
                    {
                        "name": "Jiahong Wu"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Chu"
                },
                "author": "Xiangxiang Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09533v2",
                "updated": "2025-03-13T05:54:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    54,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    49,
                    56,
                    2,
                    71,
                    0
                ],
                "title": "Large Language Models for Multi-Facility Location Mechanism Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multi-Facility Location Mechanism Design"
                },
                "summary": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents."
                },
                "authors": [
                    {
                        "name": "Nguyen Thach"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Houyu Zhou"
                    },
                    {
                        "name": "Hau Chan"
                    }
                ],
                "author_detail": {
                    "name": "Hau Chan"
                },
                "author": "Hau Chan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10071v1",
                "updated": "2025-03-13T05:39:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    39,
                    0,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:39:00Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    39,
                    0,
                    3,
                    72,
                    0
                ],
                "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM"
                },
                "summary": "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving."
                },
                "authors": [
                    {
                        "name": "Mohd Ariful Haque"
                    },
                    {
                        "name": "Justin Williams"
                    },
                    {
                        "name": "Sunzida Siddique"
                    },
                    {
                        "name": "Md. Hujaifa Islam"
                    },
                    {
                        "name": "Hasmot Ali"
                    },
                    {
                        "name": "Kishor Datta Gupta"
                    },
                    {
                        "name": "Roy George"
                    }
                ],
                "author_detail": {
                    "name": "Roy George"
                },
                "author": "Roy George",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15960v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15960v4",
                "updated": "2025-03-13T05:36:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    36,
                    12,
                    3,
                    72,
                    0
                ],
                "published": "2023-12-26T08:49:57Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    8,
                    49,
                    57,
                    1,
                    360,
                    0
                ],
                "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks"
                },
                "summary": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Pengguang Chen"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:\n  https://github.com/dvlab-research/MoTCoder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15960v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15960v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10061v2",
                "updated": "2025-03-14T01:39:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    1,
                    39,
                    39,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T05:21:22Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    21,
                    22,
                    3,
                    72,
                    0
                ],
                "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning"
                },
                "summary": "Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: scaling laws are skill-dependent.\nNext, to understand whether skill-dependent scaling is an artefact of the\npretraining datamix, we conduct an extensive ablation of different datamixes\nand find that, also when correcting for datamix differences, knowledge and code\nexhibit fundamental differences in scaling behaviour. We conclude with an\nanalysis of how our findings relate to standard compute-optimal scaling using a\nvalidation set, and find that a misspecified validation set can impact\ncompute-optimal parameter count by nearly 50%, depending on its skill\ncomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: scaling laws are skill-dependent.\nNext, to understand whether skill-dependent scaling is an artefact of the\npretraining datamix, we conduct an extensive ablation of different datamixes\nand find that, also when correcting for datamix differences, knowledge and code\nexhibit fundamental differences in scaling behaviour. We conclude with an\nanalysis of how our findings relate to standard compute-optimal scaling using a\nvalidation set, and find that a misspecified validation set can impact\ncompute-optimal parameter count by nearly 50%, depending on its skill\ncomposition."
                },
                "authors": [
                    {
                        "name": "Nicholas Roberts"
                    },
                    {
                        "name": "Niladri Chatterji"
                    },
                    {
                        "name": "Sharan Narang"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10049v1",
                "updated": "2025-03-13T05:02:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    2,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:02:49Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    2,
                    49,
                    3,
                    72,
                    0
                ],
                "title": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based\n  Planner and Graph-based Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based\n  Planner and Graph-based Policy"
                },
                "summary": "Multi-agent systems (MAS) have shown great potential in executing complex\ntasks, but coordination and safety remain significant challenges. Multi-Agent\nReinforcement Learning (MARL) offers a promising framework for agent\ncollaboration, but it faces difficulties in handling complex tasks and\ndesigning reward functions. The introduction of Large Language Models (LLMs)\nhas brought stronger reasoning and cognitive abilities to MAS, but existing\nLLM-based systems struggle to respond quickly and accurately in dynamic\nenvironments. To address these challenges, we propose LLM-based Graph\nCollaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and\nMARL. This framework decomposes complex tasks into executable subtasks and\nachieves efficient collaboration among multiple agents through graph-based\ncoordination. Specifically, LGC-MARL consists of two main components: an LLM\nplanner and a graph-based collaboration meta policy. The LLM planner transforms\ncomplex task instructions into a series of executable subtasks, evaluates the\nrationality of these subtasks using a critic model, and generates an action\ndependency graph. The graph-based collaboration meta policy facilitates\ncommunication and collaboration among agents based on the action dependency\ngraph, and adapts to new task environments through meta-learning. Experimental\nresults on the AI2-THOR simulation platform demonstrate the superior\nperformance and scalability of LGC-MARL in completing various complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) have shown great potential in executing complex\ntasks, but coordination and safety remain significant challenges. Multi-Agent\nReinforcement Learning (MARL) offers a promising framework for agent\ncollaboration, but it faces difficulties in handling complex tasks and\ndesigning reward functions. The introduction of Large Language Models (LLMs)\nhas brought stronger reasoning and cognitive abilities to MAS, but existing\nLLM-based systems struggle to respond quickly and accurately in dynamic\nenvironments. To address these challenges, we propose LLM-based Graph\nCollaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and\nMARL. This framework decomposes complex tasks into executable subtasks and\nachieves efficient collaboration among multiple agents through graph-based\ncoordination. Specifically, LGC-MARL consists of two main components: an LLM\nplanner and a graph-based collaboration meta policy. The LLM planner transforms\ncomplex task instructions into a series of executable subtasks, evaluates the\nrationality of these subtasks using a critic model, and generates an action\ndependency graph. The graph-based collaboration meta policy facilitates\ncommunication and collaboration among agents based on the action dependency\ngraph, and adapts to new task environments through meta-learning. Experimental\nresults on the AI2-THOR simulation platform demonstrate the superior\nperformance and scalability of LGC-MARL in completing various complex tasks."
                },
                "authors": [
                    {
                        "name": "Ziqi Jia"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10041v1",
                "updated": "2025-03-13T04:46:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    46,
                    53,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T04:46:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    46,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "NumScout: Unveiling Numerical Defects in Smart Contracts using\n  LLM-Pruning Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NumScout: Unveiling Numerical Defects in Smart Contracts using\n  LLM-Pruning Symbolic Execution"
                },
                "summary": "In recent years, the Ethereum platform has witnessed a proliferation of smart\ncontracts, accompanied by exponential growth in total value locked (TVL).\nHigh-TVL smart contracts often require complex numerical computations,\nparticularly in mathematical financial models used by many decentralized\napplications (DApps). Improper calculations can introduce numerical defects,\nposing potential security risks. Existing research primarily focuses on\ntraditional numerical defects like integer overflow, and there is currently a\nlack of systematic research and effective detection methods targeting new types\nof numerical defects. In this paper, we identify five new types of numerical\ndefects through the analysis of 1,199 audit reports by utilizing the open card\nmethod. Each defect is defined and illustrated with a code example to highlight\nits features and potential consequences. We also propose NumScout, a symbolic\nexecution-based tool designed to detect these five defects. Specifically, the\ntool combines information from source code and bytecode, analyzing key\noperations such as comparisons and transfers, to effectively locate defects and\nreport them based on predefined detection patterns. Furthermore, NumScout uses\na large language model (LLM) to prune functions which are unrelated to\nnumerical operations. This step allows symbolic execution to quickly enter the\ntarget function and improve runtime speed by 28.4%. We run NumScout on 6,617\nreal-world contracts and evaluated its performance based on manually labeled\nresults. We find that 1,774 contracts contained at least one of the five\ndefects, and the tool achieved an overall precision of 89.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Ethereum platform has witnessed a proliferation of smart\ncontracts, accompanied by exponential growth in total value locked (TVL).\nHigh-TVL smart contracts often require complex numerical computations,\nparticularly in mathematical financial models used by many decentralized\napplications (DApps). Improper calculations can introduce numerical defects,\nposing potential security risks. Existing research primarily focuses on\ntraditional numerical defects like integer overflow, and there is currently a\nlack of systematic research and effective detection methods targeting new types\nof numerical defects. In this paper, we identify five new types of numerical\ndefects through the analysis of 1,199 audit reports by utilizing the open card\nmethod. Each defect is defined and illustrated with a code example to highlight\nits features and potential consequences. We also propose NumScout, a symbolic\nexecution-based tool designed to detect these five defects. Specifically, the\ntool combines information from source code and bytecode, analyzing key\noperations such as comparisons and transfers, to effectively locate defects and\nreport them based on predefined detection patterns. Furthermore, NumScout uses\na large language model (LLM) to prune functions which are unrelated to\nnumerical operations. This step allows symbolic execution to quickly enter the\ntarget function and improve runtime speed by 28.4%. We run NumScout on 6,617\nreal-world contracts and evaluated its performance based on manually labeled\nresults. We find that 1,774 contracts contained at least one of the five\ndefects, and the tool achieved an overall precision of 89.7%."
                },
                "authors": [
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Zhenzhe Shao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yiming Shen"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Zhenyu Shan"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04759v2",
                "updated": "2025-03-13T04:00:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    0,
                    16,
                    3,
                    72,
                    0
                ],
                "published": "2024-10-07T05:27:22Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    27,
                    22,
                    0,
                    281,
                    0
                ],
                "title": "Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM"
                },
                "summary": "This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease."
                },
                "authors": [
                    {
                        "name": "Tianhui Cai"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Zewei Zhou"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Seth Z. Zhao"
                    },
                    {
                        "name": "Zhiwen Wu"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09010v2",
                "updated": "2025-03-13T03:42:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    42,
                    53,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T02:59:21Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    59,
                    21,
                    2,
                    71,
                    0
                ],
                "title": "HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception\n  for Humanoid Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception\n  for Humanoid Robots"
                },
                "summary": "The perceptual system design for humanoid robots poses unique challenges due\nto inherent structural constraints that cause severe self-occlusion and limited\nfield-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal\nperception framework that synergistically integrates panoramic vision and LiDAR\nsensing to overcome these limitations. Unlike conventional robot perception\nsystems that rely on monocular cameras or standard multi-sensor configurations,\nour method establishes geometrically-aware modality alignment through a\nspherical vision transformer, enabling seamless fusion of 360 visual context\nwith LiDAR's precise depth measurements. First, Spherical Geometry-aware\nConstraints (SGC) leverage panoramic camera ray properties to guide\ndistortion-regularized sampling offsets for geometric alignment. Second,\nSpatial Deformable Attention (SDA) aggregates hierarchical 3D features via\nspherical offsets, enabling efficient 360{\\deg}-to-BEV fusion with\ngeometrically complete object representations. Third, Panoramic Augmentation\n(AUG) combines cross-view transformations and semantic alignment to enhance\nBEV-panoramic feature consistency during data augmentation. Extensive\nevaluations demonstrate state-of-the-art performance on the 360BEV-Matterport\nbenchmark. Real-world deployment on humanoid platforms validates the system's\ncapability to generate accurate BEV segmentation maps through panoramic-LiDAR\nco-perception, directly enabling downstream navigation tasks in complex\nenvironments. Our work establishes a new paradigm for embodied perception in\nhumanoid robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perceptual system design for humanoid robots poses unique challenges due\nto inherent structural constraints that cause severe self-occlusion and limited\nfield-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal\nperception framework that synergistically integrates panoramic vision and LiDAR\nsensing to overcome these limitations. Unlike conventional robot perception\nsystems that rely on monocular cameras or standard multi-sensor configurations,\nour method establishes geometrically-aware modality alignment through a\nspherical vision transformer, enabling seamless fusion of 360 visual context\nwith LiDAR's precise depth measurements. First, Spherical Geometry-aware\nConstraints (SGC) leverage panoramic camera ray properties to guide\ndistortion-regularized sampling offsets for geometric alignment. Second,\nSpatial Deformable Attention (SDA) aggregates hierarchical 3D features via\nspherical offsets, enabling efficient 360{\\deg}-to-BEV fusion with\ngeometrically complete object representations. Third, Panoramic Augmentation\n(AUG) combines cross-view transformations and semantic alignment to enhance\nBEV-panoramic feature consistency during data augmentation. Extensive\nevaluations demonstrate state-of-the-art performance on the 360BEV-Matterport\nbenchmark. Real-world deployment on humanoid platforms validates the system's\ncapability to generate accurate BEV segmentation maps through panoramic-LiDAR\nco-perception, directly enabling downstream navigation tasks in complex\nenvironments. Our work establishes a new paradigm for embodied perception in\nhumanoid robotics."
                },
                "authors": [
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Wei Cui"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Jiaxu Wang"
                    },
                    {
                        "name": "Chenghao Sun"
                    },
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Yujie Chen"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10009v1",
                "updated": "2025-03-13T03:40:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    40,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:40:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    40,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problem with Reasoning Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problem with Reasoning Large Language Model"
                },
                "summary": "Operations Research (OR) has been widely applied in various fields such as\nresource allocation, production planning, and supply chain management. However,\naddressing real-world OR problems requires OR experts to perform mathematical\nmodeling and programmers to develop solution algorithms. This traditional\nmethod, heavily reliant on experts, is costly and has long development cycles,\nseverely limiting the widespread adoption of OR techniques. Few have considered\nusing Artificial Intelligence (AI) to replace professionals to achieve fully\nautomated solutions for OR problems. We propose OR-LLM-Agent, the first AI\nagent that enables end-to-end automation for solving real-world OR problems.\nOR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of\nLarge Language Models (LLMs) to translate natural language problem descriptions\ninto formal mathematical models and automatically generate Gurobi solver code.\nIn OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair\nwithin a sandbox environment, facilitating the derivation of the final\nsolution. Due to the lack of dedicated benchmark datasets for evaluating the\nautomated solving of OR problems, we construct a benchmark dataset comprising\n83 real-world OR problems described in natural language. We conduct comparative\nexperiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the\nhighest pass rate of 100% and the highest solution accuracy of 85%,\ndemonstrating the feasibility of automated OR problem-solving. Data and code\nhave been publicly available at https://github.com/bwz96sco/or_llm_agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations Research (OR) has been widely applied in various fields such as\nresource allocation, production planning, and supply chain management. However,\naddressing real-world OR problems requires OR experts to perform mathematical\nmodeling and programmers to develop solution algorithms. This traditional\nmethod, heavily reliant on experts, is costly and has long development cycles,\nseverely limiting the widespread adoption of OR techniques. Few have considered\nusing Artificial Intelligence (AI) to replace professionals to achieve fully\nautomated solutions for OR problems. We propose OR-LLM-Agent, the first AI\nagent that enables end-to-end automation for solving real-world OR problems.\nOR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of\nLarge Language Models (LLMs) to translate natural language problem descriptions\ninto formal mathematical models and automatically generate Gurobi solver code.\nIn OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair\nwithin a sandbox environment, facilitating the derivation of the final\nsolution. Due to the lack of dedicated benchmark datasets for evaluating the\nautomated solving of OR problems, we construct a benchmark dataset comprising\n83 real-world OR problems described in natural language. We conduct comparative\nexperiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the\nhighest pass rate of 100% and the highest solution accuracy of 85%,\ndemonstrating the feasibility of automated OR problem-solving. Data and code\nhave been publicly available at https://github.com/bwz96sco/or_llm_agent."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Pengcheng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Luo"
                },
                "author": "Pengcheng Luo",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02191v2",
                "updated": "2025-03-13T03:25:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    25,
                    44,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-04T02:01:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    2,
                    1,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Predicting Derailment in Toxic Conversations on GitHub"
                },
                "summary": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches."
                },
                "authors": [
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Robert Zita"
                    },
                    {
                        "name": "Rebekah Copeland"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    },
                    {
                        "name": "Rahat Rizvi Rahman"
                    },
                    {
                        "name": "Kostadin Damevski"
                    }
                ],
                "author_detail": {
                    "name": "Kostadin Damevski"
                },
                "author": "Kostadin Damevski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]