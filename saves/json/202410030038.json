[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Lukáš Ondráček"
                    },
                    {
                        "name": "Ondřej Mička"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Mička"
                },
                "author": "Ondřej Mička",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.12618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12618v2",
                "updated": "2024-10-01T17:50:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    50,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-19T09:44:17Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    9,
                    44,
                    17,
                    3,
                    263,
                    0
                ],
                "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning"
                },
                "summary": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention."
                },
                "authors": [
                    {
                        "name": "Santosh Kumar Radha"
                    },
                    {
                        "name": "Yasamin Nouri Jelyani"
                    },
                    {
                        "name": "Ara Ghukasyan"
                    },
                    {
                        "name": "Oktay Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Oktay Goktas"
                },
                "author": "Oktay Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18924v2",
                "updated": "2024-10-01T17:49:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    49,
                    0,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T17:17:15Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow"
                },
                "summary": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration."
                },
                "authors": [
                    {
                        "name": "Huizi Yu"
                    },
                    {
                        "name": "Jiayan Zhou"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Anye Shi"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Guang Chen"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Trisha Gupte"
                    },
                    {
                        "name": "Ming-Li Chen"
                    },
                    {
                        "name": "Zahra Azizi"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Themistocles L. Assimes"
                    },
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Lizhou Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lizhou Fan"
                },
                "author": "Lizhou Fan",
                "arxiv_comment": "42 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01247v2",
                "updated": "2024-10-01T17:21:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    21,
                    28,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-02T13:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    29,
                    44,
                    0,
                    246,
                    0
                ],
                "title": "Conversational Complexity for Assessing Risk in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Complexity for Assessing Risk in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm."
                },
                "authors": [
                    {
                        "name": "John Burden"
                    },
                    {
                        "name": "Manuel Cebrian"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jose Hernandez-Orallo"
                },
                "author": "Jose Hernandez-Orallo",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12857v2",
                "updated": "2024-10-01T17:13:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    13,
                    38,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-09T15:06:14Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    15,
                    6,
                    14,
                    1,
                    191,
                    0
                ],
                "title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis"
                },
                "summary": "In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers."
                },
                "authors": [
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Jiaqi Tan"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Zhenmin Weng"
                    },
                    {
                        "name": "Chenghua Gong"
                    },
                    {
                        "name": "Long Zeng"
                    },
                    {
                        "name": "Renjing Cui"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09111v2",
                "updated": "2024-10-01T17:10:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    10,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-12T09:24:34Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    9,
                    24,
                    34,
                    4,
                    194,
                    0
                ],
                "title": "Inference Optimization of Foundation Models on AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimization of Foundation Models on AI Accelerators"
                },
                "summary": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context."
                },
                "authors": [
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Kailash Budhathoki"
                    },
                    {
                        "name": "Liangfu Chen"
                    },
                    {
                        "name": "Jonas Kübler"
                    },
                    {
                        "name": "Jiaji Huang"
                    },
                    {
                        "name": "Matthäus Kleindessner"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "George Karypis"
                    }
                ],
                "author_detail": {
                    "name": "George Karypis"
                },
                "author": "George Karypis",
                "arxiv_comment": "[v2] Tutorial website added [v1] Tutorial published at KDD 2024.\n  Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17328v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17328v3",
                "updated": "2024-10-01T16:45:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    45,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-25T07:25:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    7,
                    25,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "Dual-Space Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Space Knowledge Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Zengkui Sun"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "The camera-ready version for EMNLP 2024 main conference. 17 pages, 11\n  figures, code available at: https://github.com/songmzhang/DSKD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17328v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17328v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12376v2",
                "updated": "2024-10-01T16:38:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    38,
                    29,
                    1,
                    275,
                    0
                ],
                "published": "2024-02-19T18:59:07Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    18,
                    59,
                    7,
                    0,
                    50,
                    0
                ],
                "title": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion\n  Model"
                },
                "summary": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo address this limitation, we conceptualize images as sequences of tokens with\ndynamic sizes, rather than traditional methods that perceive images as\nfixed-resolution grids. This perspective enables a flexible training strategy\nthat seamlessly accommodates various aspect ratios during both training and\ninference, thus promoting resolution generalization and eliminating biases\nintroduced by image cropping. On this basis, we present the Flexible Vision\nTransformer (FiT), a transformer architecture specifically designed for\ngenerating images with unrestricted resolutions and aspect ratios. We further\nupgrade the FiT to FiTv2 with several innovative designs, includingthe\nQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flow\nscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted\nnetwork structure, FiTv2 exhibits 2x convergence speed of FiT. When\nincorporating advanced training-free extrapolation techniques, FiTv2\ndemonstrates remarkable adaptability in both resolution extrapolation and\ndiverse resolution generation. Additionally, our exploration of the scalability\nof the FiTv2 model reveals that larger models exhibit better computational\nefficiency. Furthermore, we introduce an efficient post-training strategy to\nadapt a pre-trained model for the high-resolution generation. Comprehensive\nexperiments demonstrate the exceptional performance of FiTv2 across a broad\nrange of resolutions. We have released all the codes and models at\nhttps://github.com/whlzy/FiT to promote the exploration of diffusion\ntransformer models for arbitrary-resolution image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo address this limitation, we conceptualize images as sequences of tokens with\ndynamic sizes, rather than traditional methods that perceive images as\nfixed-resolution grids. This perspective enables a flexible training strategy\nthat seamlessly accommodates various aspect ratios during both training and\ninference, thus promoting resolution generalization and eliminating biases\nintroduced by image cropping. On this basis, we present the Flexible Vision\nTransformer (FiT), a transformer architecture specifically designed for\ngenerating images with unrestricted resolutions and aspect ratios. We further\nupgrade the FiT to FiTv2 with several innovative designs, includingthe\nQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flow\nscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted\nnetwork structure, FiTv2 exhibits 2x convergence speed of FiT. When\nincorporating advanced training-free extrapolation techniques, FiTv2\ndemonstrates remarkable adaptability in both resolution extrapolation and\ndiverse resolution generation. Additionally, our exploration of the scalability\nof the FiTv2 model reveals that larger models exhibit better computational\nefficiency. Furthermore, we introduce an efficient post-training strategy to\nadapt a pre-trained model for the high-resolution generation. Comprehensive\nexperiments demonstrate the exceptional performance of FiTv2 across a broad\nrange of resolutions. We have released all the codes and models at\nhttps://github.com/whlzy/FiT to promote the exploration of diffusion\ntransformer models for arbitrary-resolution image generation."
                },
                "authors": [
                    {
                        "name": "Zidong Wang"
                    },
                    {
                        "name": "Zeyu Lu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20252v2",
                "updated": "2024-10-01T16:34:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    34,
                    13,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T12:42:25Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "title": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?"
                },
                "summary": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Morgan Fouesneau"
                    },
                    {
                        "name": "Ivelina G. Momcheva"
                    },
                    {
                        "name": "Urmila Chadayammuri"
                    },
                    {
                        "name": "Mariia Demianenko"
                    },
                    {
                        "name": "Antoine Dumont"
                    },
                    {
                        "name": "Raphael E. Hviding"
                    },
                    {
                        "name": "K. Angelique Kahle"
                    },
                    {
                        "name": "Nadiia Pulatova"
                    },
                    {
                        "name": "Bhavesh Rajpoot"
                    },
                    {
                        "name": "Marten B. Scheuck"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Dmitry Semenov"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    }
                ],
                "author_detail": {
                    "name": "Jaime I. Villaseñor"
                },
                "author": "Jaime I. Villaseñor",
                "arxiv_comment": "Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06917v2",
                "updated": "2024-10-01T15:50:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    50,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-09T14:52:52Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    52,
                    52,
                    1,
                    191,
                    0
                ],
                "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13214v2",
                "updated": "2024-10-01T15:48:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    48,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2023-05-22T16:45:50Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    16,
                    45,
                    50,
                    0,
                    142,
                    0
                ],
                "title": "Atomic Inference for NLI with Generated Facts as Atoms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic Inference for NLI with Generated Facts as Atoms"
                },
                "summary": "With recent advances, neural models can achieve human-level performance on\nvarious natural language tasks. However, there are no guarantees that any\nexplanations from these models are faithful, i.e. that they reflect the inner\nworkings of the model. Atomic inference overcomes this issue, providing\ninterpretable and faithful model decisions. This approach involves making\npredictions for different components (or atoms) of an instance, before using\ninterpretable and deterministic rules to derive the overall prediction based on\nthe individual atom-level predictions. We investigate the effectiveness of\nusing LLM-generated facts as atoms, decomposing Natural Language Inference\npremises into lists of facts. While directly using generated facts in atomic\ninference systems can result in worse performance, with 1) a multi-stage fact\ngeneration process, and 2) a training regime that incorporates the facts, our\nfact-based method outperforms other approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent advances, neural models can achieve human-level performance on\nvarious natural language tasks. However, there are no guarantees that any\nexplanations from these models are faithful, i.e. that they reflect the inner\nworkings of the model. Atomic inference overcomes this issue, providing\ninterpretable and faithful model decisions. This approach involves making\npredictions for different components (or atoms) of an instance, before using\ninterpretable and deterministic rules to derive the overall prediction based on\nthe individual atom-level predictions. We investigate the effectiveness of\nusing LLM-generated facts as atoms, decomposing Natural Language Inference\npremises into lists of facts. While directly using generated facts in atomic\ninference systems can result in worse performance, with 1) a multi-stage fact\ngeneration process, and 2) a training regime that incorporates the facts, our\nfact-based method outperforms other approaches."
                },
                "authors": [
                    {
                        "name": "Joe Stacey"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Haim Dubossarsky"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.11301v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.11301v4",
                "updated": "2024-10-01T15:42:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    42,
                    19,
                    1,
                    275,
                    0
                ],
                "published": "2023-01-26T18:39:19Z",
                "published_parsed": [
                    2023,
                    1,
                    26,
                    18,
                    39,
                    19,
                    3,
                    26,
                    0
                ],
                "title": "A Complete Inference System for Skip-free Guarded Kleene Algebra with\n  Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Inference System for Skip-free Guarded Kleene Algebra with\n  Tests"
                },
                "summary": "Guarded Kleene Algebra with Tests (GKAT) is a fragment of Kleene Algebra with\nTests (KAT) that was recently introduced to reason efficiently about imperative\nprograms. In contrast to KAT, GKAT does not have an algebraic axiomatization,\nbut relies on an analogue of Salomaa's axiomatization of Kleene Algebra. In\nthis paper, we present an algebraic axiomatization and prove two completeness\nresults for a large fragment of GKAT consisting of skip-free programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarded Kleene Algebra with Tests (GKAT) is a fragment of Kleene Algebra with\nTests (KAT) that was recently introduced to reason efficiently about imperative\nprograms. In contrast to KAT, GKAT does not have an algebraic axiomatization,\nbut relies on an analogue of Salomaa's axiomatization of Kleene Algebra. In\nthis paper, we present an algebraic axiomatization and prove two completeness\nresults for a large fragment of GKAT consisting of skip-free programs."
                },
                "authors": [
                    {
                        "name": "Tobias Kappé"
                    },
                    {
                        "name": "Todd Schmid"
                    },
                    {
                        "name": "Alexandra Silva"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Silva"
                },
                "author": "Alexandra Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.11301v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.11301v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03354v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03354v3",
                "updated": "2024-10-01T15:41:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    41,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-06T09:15:25Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    9,
                    15,
                    25,
                    1,
                    219,
                    0
                ],
                "title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums"
                },
                "summary": "Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence."
                },
                "authors": [
                    {
                        "name": "Vanessa Clairoux-Trepanier"
                    },
                    {
                        "name": "Isa-May Beauchamp"
                    },
                    {
                        "name": "Estelle Ruellan"
                    },
                    {
                        "name": "Masarah Paquet-Clouston"
                    },
                    {
                        "name": "Serge-Olivier Paquette"
                    },
                    {
                        "name": "Eric Clay"
                    }
                ],
                "author_detail": {
                    "name": "Eric Clay"
                },
                "author": "Eric Clay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03354v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03354v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10774v2",
                "updated": "2024-10-01T15:39:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    39,
                    48,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-16T17:59:10Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    59,
                    10,
                    1,
                    107,
                    0
                ],
                "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents"
                },
                "summary": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models."
                },
                "authors": [
                    {
                        "name": "Liyan Tang"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18597v2",
                "updated": "2024-10-01T15:33:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    33,
                    30,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-28T21:19:15Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    21,
                    19,
                    15,
                    1,
                    149,
                    0
                ],
                "title": "Nonparametric causal inference for optogenetics: sequential excursion\n  effects for dynamic regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric causal inference for optogenetics: sequential excursion\n  effects for dynamic regimes"
                },
                "summary": "Optogenetics is a powerful neuroscience technique for studying how neural\ncircuit manipulation affects behavior. Standard analysis conventions discard\ninformation and severely limit the scope of the causal questions that can be\nprobed. To address this gap, we 1) draw connections to the causal inference\nliterature on sequentially randomized experiments, 2) propose a non-parametric\nframework for analyzing \"open-loop\" (static regime) optogenetics behavioral\nexperiments, 3) derive extensions of history-restricted marginal structural\nmodels for dynamic treatment regimes with positivity violations for\n\"closed-loop\" designs, and 4) propose a taxonomy of identifiable causal effects\nthat encompass a far richer collection of scientific questions compared to\nstandard methods. From another view, our work extends \"excursion effect\"\nmethods, popularized recently in the mobile health literature, to enable\nestimation of causal contrasts for treatment sequences in the presence of\npositivity violations. We describe sufficient conditions for identifiability of\nthe proposed causal estimands, and provide asymptotic statistical guarantees\nfor a proposed inverse probability-weighted estimator, a multiply-robust\nestimator (for two intervention timepoints), a framework for hypothesis\ntesting, and a computationally scalable implementation. Finally, we apply our\nframework to data from a recent neuroscience study and show how it provides\ninsight into causal effects of optogenetics on behavior that are obscured by\nstandard analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optogenetics is a powerful neuroscience technique for studying how neural\ncircuit manipulation affects behavior. Standard analysis conventions discard\ninformation and severely limit the scope of the causal questions that can be\nprobed. To address this gap, we 1) draw connections to the causal inference\nliterature on sequentially randomized experiments, 2) propose a non-parametric\nframework for analyzing \"open-loop\" (static regime) optogenetics behavioral\nexperiments, 3) derive extensions of history-restricted marginal structural\nmodels for dynamic treatment regimes with positivity violations for\n\"closed-loop\" designs, and 4) propose a taxonomy of identifiable causal effects\nthat encompass a far richer collection of scientific questions compared to\nstandard methods. From another view, our work extends \"excursion effect\"\nmethods, popularized recently in the mobile health literature, to enable\nestimation of causal contrasts for treatment sequences in the presence of\npositivity violations. We describe sufficient conditions for identifiability of\nthe proposed causal estimands, and provide asymptotic statistical guarantees\nfor a proposed inverse probability-weighted estimator, a multiply-robust\nestimator (for two intervention timepoints), a framework for hypothesis\ntesting, and a computationally scalable implementation. Finally, we apply our\nframework to data from a recent neuroscience study and show how it provides\ninsight into causal effects of optogenetics on behavior that are obscured by\nstandard analyses."
                },
                "authors": [
                    {
                        "name": "Gabriel Loewinger"
                    },
                    {
                        "name": "Alexander W. Levis"
                    },
                    {
                        "name": "Francisco Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Pereira"
                },
                "author": "Francisco Pereira",
                "arxiv_comment": "52 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14891v2",
                "updated": "2024-10-01T15:31:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    31,
                    23,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-23T10:38:20Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    10,
                    38,
                    20,
                    0,
                    267,
                    0
                ],
                "title": "Observe Then Act: Asynchronous Active Vision-Action Model for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observe Then Act: Asynchronous Active Vision-Action Model for Robotic\n  Manipulation"
                },
                "summary": "In real-world scenarios, many robotic manipulation tasks are hindered by\nocclusions and limited fields of view, posing significant challenges for\npassive observation-based models that rely on fixed or wrist-mounted cameras.\nIn this paper, we investigate the problem of robotic manipulation under limited\nvisual observation and propose a task-driven asynchronous active vision-action\nmodel.Our model serially connects a camera Next-Best-View (NBV) policy with a\ngripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor\ncoordination framework using few-shot reinforcement learning. This approach\nallows the agent to adjust a third-person camera to actively observe the\nenvironment based on the task goal, and subsequently infer the appropriate\nmanipulation actions.We trained and evaluated our model on 8\nviewpoint-constrained tasks in RLBench. The results demonstrate that our model\nconsistently outperforms baseline algorithms, showcasing its effectiveness in\nhandling visual constraints in manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, many robotic manipulation tasks are hindered by\nocclusions and limited fields of view, posing significant challenges for\npassive observation-based models that rely on fixed or wrist-mounted cameras.\nIn this paper, we investigate the problem of robotic manipulation under limited\nvisual observation and propose a task-driven asynchronous active vision-action\nmodel.Our model serially connects a camera Next-Best-View (NBV) policy with a\ngripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor\ncoordination framework using few-shot reinforcement learning. This approach\nallows the agent to adjust a third-person camera to actively observe the\nenvironment based on the task goal, and subsequently infer the appropriate\nmanipulation actions.We trained and evaluated our model on 8\nviewpoint-constrained tasks in RLBench. The results demonstrate that our model\nconsistently outperforms baseline algorithms, showcasing its effectiveness in\nhandling visual constraints in manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Guokang Wang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Yanhong Liu"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19346v3",
                "updated": "2024-10-01T15:28:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    28,
                    16,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-28T12:04:28Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    12,
                    4,
                    28,
                    3,
                    88,
                    0
                ],
                "title": "Large Language Models Are Unconscious of Unreasonability in Math\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Unconscious of Unreasonability in Math\n  Problems"
                },
                "summary": "Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12701v2",
                "updated": "2024-10-01T15:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    3,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-21T11:50:16Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    11,
                    50,
                    16,
                    1,
                    142,
                    0
                ],
                "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering"
                },
                "summary": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available."
                },
                "authors": [
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14945v2",
                "updated": "2024-10-01T14:29:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    14,
                    29,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-23T18:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    18,
                    0,
                    4,
                    3,
                    144,
                    0
                ],
                "title": "Residual eccentricity as a systematic uncertainty on the formation\n  channels of binary black holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual eccentricity as a systematic uncertainty on the formation\n  channels of binary black holes"
                },
                "summary": "Resolving the formation channel(s) of merging binary black holes is a key\ngoal in gravitational-wave astronomy. The orbital eccentricity is believed to\nbe a precious tracer of the underlying formation pathway, but is largely\ndissipated during the usually long inspiral between black hole formation and\nmerger. Most gravitational-wave sources are thus expected to enter the\nsensitivity windows of current detectors on configurations that are compatible\nwith quasi-circular orbits. In this paper, we investigate the impact of\n\"negligible\" residual eccentricity -- lower than currently detectable by\nLIGO/Virgo -- on our ability to infer the formation history of binary black\nholes, focusing in particular on their spin orientations. We trace the\nevolution of both observed and synthetic gravitational-wave events backward in\ntime, while resampling their residual eccentricities to values that are below\nthe detectability threshold. Eccentricities in-band as low as $\\sim 10^{-4}$\ncan lead to significant biases when reconstructing the spin directions,\nespecially in the case of loud, highly precessing systems. Residual\neccentricity thus act like a systematic uncertainty for our astrophysical\ninference. As a mitigation strategy, one can marginalize the posterior\ndistribution over the residual eccentricity using astrophysical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving the formation channel(s) of merging binary black holes is a key\ngoal in gravitational-wave astronomy. The orbital eccentricity is believed to\nbe a precious tracer of the underlying formation pathway, but is largely\ndissipated during the usually long inspiral between black hole formation and\nmerger. Most gravitational-wave sources are thus expected to enter the\nsensitivity windows of current detectors on configurations that are compatible\nwith quasi-circular orbits. In this paper, we investigate the impact of\n\"negligible\" residual eccentricity -- lower than currently detectable by\nLIGO/Virgo -- on our ability to infer the formation history of binary black\nholes, focusing in particular on their spin orientations. We trace the\nevolution of both observed and synthetic gravitational-wave events backward in\ntime, while resampling their residual eccentricities to values that are below\nthe detectability threshold. Eccentricities in-band as low as $\\sim 10^{-4}$\ncan lead to significant biases when reconstructing the spin directions,\nespecially in the case of loud, highly precessing systems. Residual\neccentricity thus act like a systematic uncertainty for our astrophysical\ninference. As a mitigation strategy, one can marginalize the posterior\ndistribution over the residual eccentricity using astrophysical predictions."
                },
                "authors": [
                    {
                        "name": "Giulia Fumagalli"
                    },
                    {
                        "name": "Isobel Romero-Shaw"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Viola De Renzis"
                    },
                    {
                        "name": "Konstantinos Kritos"
                    },
                    {
                        "name": "Aleksandra Olejak"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Olejak"
                },
                "author": "Aleksandra Olejak",
                "arxiv_doi": "10.1103/PhysRevD.110.063012",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.063012",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.14945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures, 2 tables",
                "arxiv_journal_ref": "Phys. Rev. D 110, 063012 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08040v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08040v4",
                "updated": "2024-10-01T14:08:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    14,
                    8,
                    10,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-12T19:23:13Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    19,
                    23,
                    13,
                    1,
                    72,
                    0
                ],
                "title": "Low-Energy On-Device Personalization for MCUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Energy On-Device Personalization for MCUs"
                },
                "summary": "Microcontroller Units (MCUs) are ideal platforms for edge applications due to\ntheir low cost and energy consumption, and are widely used in various\napplications, including personalized machine learning tasks, where customized\nmodels can enhance the task adaptation. However, existing approaches for local\non-device personalization mostly support simple ML architectures or require\ncomplex local pre-training/training, leading to high energy consumption and\nnegating the low-energy advantage of MCUs. In this paper, we introduce\n$MicroT$, an efficient and low-energy MCU personalization approach. $MicroT$\nincludes a robust, general, but tiny feature extractor, developed through\nself-supervised knowledge distillation, which trains a task-specific head to\nenable independent on-device personalization with minimal energy and\ncomputational requirements. MicroT implements an MCU-optimized early-exit\ninference mechanism called stage-decision to further reduce energy costs. This\nmechanism allows for user-configurable exit criteria (stage-decision ratio) to\nadaptively balance energy cost with model performance. We evaluated MicroT\nusing two models, three datasets, and two MCU boards. $MicroT$ outperforms\ntraditional transfer learning (TTL) and two SOTA approaches by 2.12 - 11.60%\nacross two models and three datasets. Targeting widely used energy-aware edge\ndevices, MicroT's on-device training requires no additional complex operations,\nhalving the energy cost compared to SOTA approaches by up to 2.28X while\nkeeping SRAM usage below 1MB. During local inference, MicroT reduces energy\ncost by 14.17% compared to TTL across two boards and two datasets, highlighting\nits suitability for long-term use on energy-aware resource-constrained MCUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microcontroller Units (MCUs) are ideal platforms for edge applications due to\ntheir low cost and energy consumption, and are widely used in various\napplications, including personalized machine learning tasks, where customized\nmodels can enhance the task adaptation. However, existing approaches for local\non-device personalization mostly support simple ML architectures or require\ncomplex local pre-training/training, leading to high energy consumption and\nnegating the low-energy advantage of MCUs. In this paper, we introduce\n$MicroT$, an efficient and low-energy MCU personalization approach. $MicroT$\nincludes a robust, general, but tiny feature extractor, developed through\nself-supervised knowledge distillation, which trains a task-specific head to\nenable independent on-device personalization with minimal energy and\ncomputational requirements. MicroT implements an MCU-optimized early-exit\ninference mechanism called stage-decision to further reduce energy costs. This\nmechanism allows for user-configurable exit criteria (stage-decision ratio) to\nadaptively balance energy cost with model performance. We evaluated MicroT\nusing two models, three datasets, and two MCU boards. $MicroT$ outperforms\ntraditional transfer learning (TTL) and two SOTA approaches by 2.12 - 11.60%\nacross two models and three datasets. Targeting widely used energy-aware edge\ndevices, MicroT's on-device training requires no additional complex operations,\nhalving the energy cost compared to SOTA approaches by up to 2.28X while\nkeeping SRAM usage below 1MB. During local inference, MicroT reduces energy\ncost by 14.17% compared to TTL across two boards and two datasets, highlighting\nits suitability for long-term use on energy-aware resource-constrained MCUs."
                },
                "authors": [
                    {
                        "name": "Yushan Huang"
                    },
                    {
                        "name": "Ranya Aloufi"
                    },
                    {
                        "name": "Xavier Cadet"
                    },
                    {
                        "name": "Yuchen Zhao"
                    },
                    {
                        "name": "Payam Barnaghi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "Accepted to The 9th ACM/IEEE Symposium on Edge Computing (SEC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08040v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08040v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06875v3",
                "updated": "2024-10-01T13:56:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    56,
                    17,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-09T14:04:08Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    4,
                    8,
                    1,
                    191,
                    0
                ],
                "title": "Extending the blended generalized extreme value distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the blended generalized extreme value distribution"
                },
                "summary": "The generalized extreme value (GEV) distribution is commonly employed to help\nestimate the likelihood of extreme events in many geophysical and other\napplication areas. The recently proposed blended generalized extreme value\n(bGEV) distribution modifies the GEV with positive shape parameter to avoid a\nhard lower bound that complicates fitting and inference. Here, the bGEV is\nextended to the GEV with negative shape parameter, avoiding a hard upper bound\nthat is unrealistic in many applications. This extended bGEV is shown to\nimprove on the GEV for forecasting heat and sea level extremes based on past\ndata. Software implementing this bGEV and applying it to the example\ntemperature and sea level data is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalized extreme value (GEV) distribution is commonly employed to help\nestimate the likelihood of extreme events in many geophysical and other\napplication areas. The recently proposed blended generalized extreme value\n(bGEV) distribution modifies the GEV with positive shape parameter to avoid a\nhard lower bound that complicates fitting and inference. Here, the bGEV is\nextended to the GEV with negative shape parameter, avoiding a hard upper bound\nthat is unrealistic in many applications. This extended bGEV is shown to\nimprove on the GEV for forecasting heat and sea level extremes based on past\ndata. Software implementing this bGEV and applying it to the example\ntemperature and sea level data is provided."
                },
                "authors": [
                    {
                        "name": "Nir Y. Krakauer"
                    }
                ],
                "author_detail": {
                    "name": "Nir Y. Krakauer"
                },
                "author": "Nir Y. Krakauer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01453v3",
                "updated": "2024-10-01T13:46:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    46,
                    4,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-02T16:36:26Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    36,
                    26,
                    3,
                    123,
                    0
                ],
                "title": "Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?"
                },
                "summary": "We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs"
                },
                "authors": [
                    {
                        "name": "Lakshmi Nair"
                    },
                    {
                        "name": "Evana Gizzi"
                    },
                    {
                        "name": "Jivko Sinapov"
                    }
                ],
                "author_detail": {
                    "name": "Jivko Sinapov"
                },
                "author": "Jivko Sinapov",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08208v2",
                "updated": "2024-10-01T13:19:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    19,
                    34,
                    1,
                    275,
                    0
                ],
                "published": "2023-10-12T10:52:45Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    10,
                    52,
                    45,
                    3,
                    285,
                    0
                ],
                "title": "DsubCox: A Fast Subsampling Algorithm for Cox Model with Distributed and\n  Massive Survival Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DsubCox: A Fast Subsampling Algorithm for Cox Model with Distributed and\n  Massive Survival Data"
                },
                "summary": "To ensure privacy protection and alleviate computational burden, we propose a\nfast subsmaling procedure for the Cox model with massive survival datasets from\nmulti-centered, decentralized sources. The proposed estimator is computed based\non optimal subsampling probabilities that we derived and enables transmission\nof subsample-based summary level statistics between different storage sites\nwith only one round of communication. For inference, the asymptotic properties\nof the proposed estimator were rigorously established. An extensive simulation\nstudy demonstrated that the proposed approach is effective. The methodology was\napplied to analyze a large dataset from the U.S. airlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure privacy protection and alleviate computational burden, we propose a\nfast subsmaling procedure for the Cox model with massive survival datasets from\nmulti-centered, decentralized sources. The proposed estimator is computed based\non optimal subsampling probabilities that we derived and enables transmission\nof subsample-based summary level statistics between different storage sites\nwith only one round of communication. For inference, the asymptotic properties\nof the proposed estimator were rigorously established. An extensive simulation\nstudy demonstrated that the proposed approach is effective. The methodology was\napplied to analyze a large dataset from the U.S. airlines."
                },
                "authors": [
                    {
                        "name": "Haixiang Zhang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "HaiYing Wang"
                    }
                ],
                "author_detail": {
                    "name": "HaiYing Wang"
                },
                "author": "HaiYing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.04223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.04223v3",
                "updated": "2024-10-01T13:18:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    18,
                    41,
                    1,
                    275,
                    0
                ],
                "published": "2022-12-08T12:05:50Z",
                "published_parsed": [
                    2022,
                    12,
                    8,
                    12,
                    5,
                    50,
                    3,
                    342,
                    0
                ],
                "title": "Vicious Classifiers: Assessing Inference-time Data Reconstruction Risk\n  in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vicious Classifiers: Assessing Inference-time Data Reconstruction Risk\n  in Edge Computing"
                },
                "summary": "Privacy-preserving inference in edge computing paradigms encourages the users\nof machine-learning services to locally run a model on their private input and\nonly share the models outputs for a target task with the server. We study how a\nvicious server can reconstruct the input data by observing only the models\noutputs while keeping the target accuracy very close to that of a honest server\nby jointly training a target model (to run at users' side) and an attack model\nfor data reconstruction (to secretly use at servers' side). We present a new\nmeasure to assess the inference-time reconstruction risk. Evaluations on six\nbenchmark datasets show the model's input can be approximately reconstructed\nfrom the outputs of a single inference. We propose a primary defense mechanism\nto distinguish vicious versus honest classifiers at inference time. By studying\nsuch a risk associated with emerging ML services our work has implications for\nenhancing privacy in edge computing. We discuss open challenges and directions\nfor future studies and release our code as a benchmark for the community at\nhttps://github.com/mmalekzadeh/vicious-classifiers .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving inference in edge computing paradigms encourages the users\nof machine-learning services to locally run a model on their private input and\nonly share the models outputs for a target task with the server. We study how a\nvicious server can reconstruct the input data by observing only the models\noutputs while keeping the target accuracy very close to that of a honest server\nby jointly training a target model (to run at users' side) and an attack model\nfor data reconstruction (to secretly use at servers' side). We present a new\nmeasure to assess the inference-time reconstruction risk. Evaluations on six\nbenchmark datasets show the model's input can be approximately reconstructed\nfrom the outputs of a single inference. We propose a primary defense mechanism\nto distinguish vicious versus honest classifiers at inference time. By studying\nsuch a risk associated with emerging ML services our work has implications for\nenhancing privacy in edge computing. We discuss open challenges and directions\nfor future studies and release our code as a benchmark for the community at\nhttps://github.com/mmalekzadeh/vicious-classifiers ."
                },
                "authors": [
                    {
                        "name": "Mohammad Malekzadeh"
                    },
                    {
                        "name": "Deniz Gunduz"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Gunduz"
                },
                "author": "Deniz Gunduz",
                "arxiv_comment": "Published at BMVC 2024 workshop on Privacy, Fairness, Accountability\n  and Transparency in Computer Vision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.04223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.04223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16167v2",
                "updated": "2024-10-01T13:16:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    16,
                    45,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T15:08:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging."
                },
                "authors": [
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.07635v3",
                "updated": "2024-10-01T13:15:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    15,
                    53,
                    1,
                    275,
                    0
                ],
                "published": "2023-07-14T21:13:04Z",
                "published_parsed": [
                    2023,
                    7,
                    14,
                    21,
                    13,
                    4,
                    4,
                    195,
                    0
                ],
                "title": "CoTracker: It is Better to Track Together",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTracker: It is Better to Track Together"
                },
                "summary": "We introduce CoTracker, a transformer-based model that tracks a large number\nof 2D points in long video sequences. Differently from most existing approaches\nthat track points independently, CoTracker tracks them jointly, accounting for\ntheir dependencies. We show that joint tracking significantly improves tracking\naccuracy and robustness, and allows CoTracker to track occluded points and\npoints outside of the camera view. We also introduce several innovations for\nthis class of trackers, including using token proxies that significantly\nimprove memory efficiency and allow CoTracker to track 70k points jointly and\nsimultaneously at inference on a single GPU. CoTracker is an online algorithm\nthat operates causally on short windows. However, it is trained utilizing\nunrolled windows as a recurrent network, maintaining tracks for long periods of\ntime even when points are occluded or leave the field of view. Quantitatively,\nCoTracker substantially outperforms prior trackers on standard point-tracking\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CoTracker, a transformer-based model that tracks a large number\nof 2D points in long video sequences. Differently from most existing approaches\nthat track points independently, CoTracker tracks them jointly, accounting for\ntheir dependencies. We show that joint tracking significantly improves tracking\naccuracy and robustness, and allows CoTracker to track occluded points and\npoints outside of the camera view. We also introduce several innovations for\nthis class of trackers, including using token proxies that significantly\nimprove memory efficiency and allow CoTracker to track 70k points jointly and\nsimultaneously at inference on a single GPU. CoTracker is an online algorithm\nthat operates causally on short windows. However, it is trained utilizing\nunrolled windows as a recurrent network, maintaining tracks for long periods of\ntime even when points are occluded or leave the field of view. Quantitatively,\nCoTracker substantially outperforms prior trackers on standard point-tracking\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Nikita Karaev"
                    },
                    {
                        "name": "Ignacio Rocco"
                    },
                    {
                        "name": "Benjamin Graham"
                    },
                    {
                        "name": "Natalia Neverova"
                    },
                    {
                        "name": "Andrea Vedaldi"
                    },
                    {
                        "name": "Christian Rupprecht"
                    }
                ],
                "author_detail": {
                    "name": "Christian Rupprecht"
                },
                "author": "Christian Rupprecht",
                "arxiv_comment": "Code and model weights are available at:\n  https://co-tracker.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09864v2",
                "updated": "2024-10-01T13:07:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    7,
                    2,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-14T09:22:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    9,
                    22,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data"
                },
                "summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications."
                },
                "authors": [
                    {
                        "name": "Grigor Bezirganyan"
                    },
                    {
                        "name": "Sana Sellami"
                    },
                    {
                        "name": "Laure Berti-Équille"
                    },
                    {
                        "name": "Sébastien Fournier"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Fournier"
                },
                "author": "Sébastien Fournier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19979v2",
                "updated": "2024-10-01T13:04:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    4,
                    55,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T06:07:12Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model"
                },
                "summary": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimi Suzuki"
                },
                "author": "Yoshimi Suzuki",
                "arxiv_comment": "Long paper accepted to EMNLP 2024 Main. 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17946v2",
                "updated": "2024-10-01T13:01:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    1,
                    40,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-26T15:20:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation"
                },
                "summary": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05236v3",
                "updated": "2024-10-01T12:58:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    58,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-08T11:54:40Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    11,
                    54,
                    40,
                    4,
                    68,
                    0
                ],
                "title": "Modeling Fault Recovery and Transient Stability of Grid-Forming\n  Converters Equipped With Current Reference Limitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Fault Recovery and Transient Stability of Grid-Forming\n  Converters Equipped With Current Reference Limitation"
                },
                "summary": "When grid-forming (GFM) inverter-based resources (IBRs) face severe grid\ndisturbances (e.g., short-circuit faults), the current limitation mechanism may\nbe triggered. Consequently, the GFM IBRs enter the current-saturation mode,\ninducing nonlinear dynamical behaviors and posing great challenges to the\npost-disturbance transient angle stability. This paper presents a systematic\nstudy to reveal the fault recovery behaviors of a GFM IBR and identify the risk\nof instability. A closed-form expression for the necessary condition that a GFM\nIBR returns from the current-saturation mode to the normal operation mode is\npresented. Based on these analyses, it is inferred that the angle of the\nmagnitude-saturated current significantly affects the post-fault recovery and\ntransient stability; with different angle selection, the system may follow\nmultiple post-fault trajectories depending on those conditions: 1) Convergence\nto a normal stable equilibrium point (SEP), 2) convergence to a saturated\nstable equilibrium point (satSEP), or 3) divergence (instability). In this\npaper, the circumstances under which a GFM IBR cannot escape from the\ncurrent-saturation mode are thoroughly investigated. The theoretical analyses\nare verified by dynamic simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When grid-forming (GFM) inverter-based resources (IBRs) face severe grid\ndisturbances (e.g., short-circuit faults), the current limitation mechanism may\nbe triggered. Consequently, the GFM IBRs enter the current-saturation mode,\ninducing nonlinear dynamical behaviors and posing great challenges to the\npost-disturbance transient angle stability. This paper presents a systematic\nstudy to reveal the fault recovery behaviors of a GFM IBR and identify the risk\nof instability. A closed-form expression for the necessary condition that a GFM\nIBR returns from the current-saturation mode to the normal operation mode is\npresented. Based on these analyses, it is inferred that the angle of the\nmagnitude-saturated current significantly affects the post-fault recovery and\ntransient stability; with different angle selection, the system may follow\nmultiple post-fault trajectories depending on those conditions: 1) Convergence\nto a normal stable equilibrium point (SEP), 2) convergence to a saturated\nstable equilibrium point (satSEP), or 3) divergence (instability). In this\npaper, the circumstances under which a GFM IBR cannot escape from the\ncurrent-saturation mode are thoroughly investigated. The theoretical analyses\nare verified by dynamic simulations."
                },
                "authors": [
                    {
                        "name": "Ali Arjomandi-Nezhad"
                    },
                    {
                        "name": "Yifei Guo"
                    },
                    {
                        "name": "Bikash C. Pal"
                    },
                    {
                        "name": "Guangya Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangya Yang"
                },
                "author": "Guangya Yang",
                "arxiv_comment": "13 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17373v2",
                "updated": "2024-10-01T12:43:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    43,
                    55,
                    1,
                    275,
                    0
                ],
                "published": "2023-10-26T13:10:59Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    13,
                    10,
                    59,
                    3,
                    299,
                    0
                ],
                "title": "Causality-Inspired Fair Representation Learning for Multimodal\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality-Inspired Fair Representation Learning for Multimodal\n  Recommendation"
                },
                "summary": "Recently, multimodal recommendations (MMR) have gained increasing attention\nfor alleviating the data sparsity problem of traditional recommender systems by\nincorporating modality-based representations. Although MMR exhibit notable\nimprovement in recommendation accuracy, we empirically validate that an\nincrease in the quantity or variety of modalities leads to a higher degree of\nusers' sensitive information leakage due to entangled causal relationships,\nrisking fair representation learning. On the other hand, existing fair\nrepresentation learning approaches are mostly based on the assumption that\nsensitive information is solely leaked from users' interaction data and do not\nexplicitly model the causal relationships introduced by multimodal data, which\nlimits their applicability in multimodal scenarios. Particularly, we\ndisentangle biased and filtered modal embeddings inspired by causal inference\ntechniques, enabling the mining of modality-based unfair and fair user-user\nrelations, thereby enhancing the fairness and informativeness of user\nrepresentations. By addressing the causal effects of sensitive attributes on\nuser preferences, our approach aims to achieve counterfactual fairness in\nmultimodal recommendations. Experiments on two public datasets demonstrate the\nsuperiority of our FMMRec relative to the state-of-the-art baselines. Our\nsource code is available at https://github.com/WeixinChen98/FMMRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal recommendations (MMR) have gained increasing attention\nfor alleviating the data sparsity problem of traditional recommender systems by\nincorporating modality-based representations. Although MMR exhibit notable\nimprovement in recommendation accuracy, we empirically validate that an\nincrease in the quantity or variety of modalities leads to a higher degree of\nusers' sensitive information leakage due to entangled causal relationships,\nrisking fair representation learning. On the other hand, existing fair\nrepresentation learning approaches are mostly based on the assumption that\nsensitive information is solely leaked from users' interaction data and do not\nexplicitly model the causal relationships introduced by multimodal data, which\nlimits their applicability in multimodal scenarios. Particularly, we\ndisentangle biased and filtered modal embeddings inspired by causal inference\ntechniques, enabling the mining of modality-based unfair and fair user-user\nrelations, thereby enhancing the fairness and informativeness of user\nrepresentations. By addressing the causal effects of sensitive attributes on\nuser preferences, our approach aims to achieve counterfactual fairness in\nmultimodal recommendations. Experiments on two public datasets demonstrate the\nsuperiority of our FMMRec relative to the state-of-the-art baselines. Our\nsource code is available at https://github.com/WeixinChen98/FMMRec."
                },
                "authors": [
                    {
                        "name": "Weixin Chen"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yongxin Ni"
                    },
                    {
                        "name": "Yuhan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Zhao"
                },
                "author": "Yuhan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19924v2",
                "updated": "2024-10-01T12:43:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    43,
                    9,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T03:58:43Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    58,
                    43,
                    0,
                    274,
                    0
                ],
                "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning."
                },
                "authors": [
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yihan Xi"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Ufuk Topcu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "Updated link to code repository",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05687v2",
                "updated": "2024-10-01T12:17:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    17,
                    42,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-09T08:17:13Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    8,
                    17,
                    13,
                    6,
                    161,
                    0
                ],
                "title": "FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation"
                },
                "summary": "Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics.\nAdditionally, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance. We hope\nthis benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. The source code and\ndocumentation is available at \\url{https://github.com/thu-uav/FlightBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics.\nAdditionally, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance. We hope\nthis benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. The source code and\ndocumentation is available at \\url{https://github.com/thu-uav/FlightBench}."
                },
                "authors": [
                    {
                        "name": "Shu-Ang Yu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first three authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05904v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05904v3",
                "updated": "2024-10-01T12:08:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    8,
                    23,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-09T17:00:22Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    0,
                    22,
                    3,
                    130,
                    0
                ],
                "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"
                },
                "summary": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Jonathan Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Herzig"
                },
                "author": "Jonathan Herzig",
                "arxiv_comment": "Accepted as a long paper at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05904v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05904v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10122v3",
                "updated": "2024-10-01T12:07:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    7,
                    31,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-16T10:59:44Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    10,
                    59,
                    44,
                    3,
                    320,
                    0
                ],
                "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LLaVA: Learning United Visual Representation by Alignment Before\n  Projection"
                },
                "summary": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM. Code address:\n\\href{https://github.com/PKU-YuanGroup/Video-LLaVA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM. Code address:\n\\href{https://github.com/PKU-YuanGroup/Video-LLaVA}"
                },
                "authors": [
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jiaxi Cui"
                    },
                    {
                        "name": "Munan Ning"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12269v2",
                "updated": "2024-10-01T11:26:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    26,
                    11,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-18T04:55:09Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    55,
                    9,
                    1,
                    170,
                    0
                ],
                "title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization"
                },
                "summary": "Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework."
                },
                "authors": [
                    {
                        "name": "Kwangwook Seo"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06841v2",
                "updated": "2024-10-01T11:14:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    14,
                    40,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-10T23:23:36Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    23,
                    23,
                    36,
                    0,
                    162,
                    0
                ],
                "title": "CompassDock: Comprehensive Accurate Assessment Approach for Deep\n  Learning-Based Molecular Docking in Inference and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompassDock: Comprehensive Accurate Assessment Approach for Deep\n  Learning-Based Molecular Docking in Inference and Fine-Tuning"
                },
                "summary": "Datasets used for molecular docking, such as PDBBind, contain technical\nvariability - they are noisy. Although the origins of the noise have been\ndiscussed, a comprehensive analysis of the physical, chemical, and bioactivity\ncharacteristics of the datasets is still lacking. To address this gap, we\nintroduce the Comprehensive Accurate Assessment (Compass). Compass integrates\ntwo key components: PoseCheck, which examines ligand strain energy,\nprotein-ligand steric clashes, and interactions, and AA-Score, a new empirical\nscoring function for calculating binding affinity energy. Together, these form\na unified workflow that assesses both the physical/chemical properties and\nbioactivity favorability of ligands and protein-ligand interactions. Our\nanalysis of the PDBBind dataset using Compass reveals substantial noise in the\nground truth data. Additionally, we propose CompassDock, which incorporates the\nCompass module with DiffDock, the state-of-the-art deep learning-based\nmolecular docking method, to enable accurate assessment of docked ligands\nduring inference. Finally, we present a new paradigm for enhancing molecular\ndocking model performance by fine-tuning with Compass Scores, which encompass\nbinding affinity energy, strain energy, and the number of steric clashes\nidentified by Compass. Our results show that, while fine-tuning without Compass\nimproves the percentage of docked poses with RMSD < 2{\\AA}, it leads to a\ndecrease in physical/chemical and bioactivity favorability. In contrast,\nfine-tuning with Compass shows a limited improvement in RMSD < 2{\\AA} but\nenhances the physical/chemical and bioactivity favorability of the ligand\nconformation. The source code is available publicly at\nhttps://github.com/BIMSBbioinfo/CompassDock.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datasets used for molecular docking, such as PDBBind, contain technical\nvariability - they are noisy. Although the origins of the noise have been\ndiscussed, a comprehensive analysis of the physical, chemical, and bioactivity\ncharacteristics of the datasets is still lacking. To address this gap, we\nintroduce the Comprehensive Accurate Assessment (Compass). Compass integrates\ntwo key components: PoseCheck, which examines ligand strain energy,\nprotein-ligand steric clashes, and interactions, and AA-Score, a new empirical\nscoring function for calculating binding affinity energy. Together, these form\na unified workflow that assesses both the physical/chemical properties and\nbioactivity favorability of ligands and protein-ligand interactions. Our\nanalysis of the PDBBind dataset using Compass reveals substantial noise in the\nground truth data. Additionally, we propose CompassDock, which incorporates the\nCompass module with DiffDock, the state-of-the-art deep learning-based\nmolecular docking method, to enable accurate assessment of docked ligands\nduring inference. Finally, we present a new paradigm for enhancing molecular\ndocking model performance by fine-tuning with Compass Scores, which encompass\nbinding affinity energy, strain energy, and the number of steric clashes\nidentified by Compass. Our results show that, while fine-tuning without Compass\nimproves the percentage of docked poses with RMSD < 2{\\AA}, it leads to a\ndecrease in physical/chemical and bioactivity favorability. In contrast,\nfine-tuning with Compass shows a limited improvement in RMSD < 2{\\AA} but\nenhances the physical/chemical and bioactivity favorability of the ligand\nconformation. The source code is available publicly at\nhttps://github.com/BIMSBbioinfo/CompassDock."
                },
                "authors": [
                    {
                        "name": "Ahmet Sarigun"
                    },
                    {
                        "name": "Vedran Franke"
                    },
                    {
                        "name": "Bora Uyar"
                    },
                    {
                        "name": "Altuna Akalin"
                    }
                ],
                "author_detail": {
                    "name": "Altuna Akalin"
                },
                "author": "Altuna Akalin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12968v2",
                "updated": "2024-10-01T11:01:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    1,
                    37,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-19T15:54:15Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    15,
                    54,
                    15,
                    4,
                    110,
                    0
                ],
                "title": "Scalable Data Assimilation with Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Data Assimilation with Message Passing"
                },
                "summary": "Data assimilation is a core component of numerical weather prediction\nsystems. The large quantity of data processed during assimilation requires the\ncomputation to be distributed across increasingly many compute nodes, yet\nexisting approaches suffer from synchronisation overhead in this setting. In\nthis paper, we exploit the formulation of data assimilation as a Bayesian\ninference problem and apply a message-passing algorithm to solve the spatial\ninference problem. Since message passing is inherently based on local\ncomputations, this approach lends itself to parallel and distributed\ncomputation. In combination with a GPU-accelerated implementation, we can scale\nthe algorithm to very large grid sizes while retaining good accuracy and\ncompute and memory requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data assimilation is a core component of numerical weather prediction\nsystems. The large quantity of data processed during assimilation requires the\ncomputation to be distributed across increasingly many compute nodes, yet\nexisting approaches suffer from synchronisation overhead in this setting. In\nthis paper, we exploit the formulation of data assimilation as a Bayesian\ninference problem and apply a message-passing algorithm to solve the spatial\ninference problem. Since message passing is inherently based on local\ncomputations, this approach lends itself to parallel and distributed\ncomputation. In combination with a GPU-accelerated implementation, we can scale\nthe algorithm to very large grid sizes while retaining good accuracy and\ncompute and memory requirements."
                },
                "authors": [
                    {
                        "name": "Oscar Key"
                    },
                    {
                        "name": "So Takao"
                    },
                    {
                        "name": "Daniel Giles"
                    },
                    {
                        "name": "Marc Peter Deisenroth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Peter Deisenroth"
                },
                "author": "Marc Peter Deisenroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09697v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09697v5",
                "updated": "2024-10-01T10:43:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    43,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2023-04-19T14:41:14Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    14,
                    41,
                    14,
                    2,
                    109,
                    0
                ],
                "title": "A Calculus for Scoped Effects & Handlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Calculus for Scoped Effects & Handlers"
                },
                "summary": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples."
                },
                "authors": [
                    {
                        "name": "Roger Bosman"
                    },
                    {
                        "name": "Birthe van den Berg"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Tom Schrijvers"
                    }
                ],
                "author_detail": {
                    "name": "Tom Schrijvers"
                },
                "author": "Tom Schrijvers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09697v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09697v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19676v2",
                "updated": "2024-10-01T10:42:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    42,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-29T12:08:20Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    8,
                    20,
                    6,
                    273,
                    0
                ],
                "title": "See Detail Say Clear: Towards Brain CT Report Generation via\n  Pathological Clue-driven Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See Detail Say Clear: Towards Brain CT Report Generation via\n  Pathological Clue-driven Representation Learning"
                },
                "summary": "Brain CT report generation is significant to aid physicians in diagnosing\ncranial diseases. Recent studies concentrate on handling the consistency\nbetween visual and textual pathological features to improve the coherence of\nreport. However, there exist some challenges: 1) Redundant visual representing:\nMassive irrelevant areas in 3D scans distract models from representing salient\nvisual contexts. 2) Shifted semantic representing: Limited medical corpus\ncauses difficulties for models to transfer the learned textual representations\nto generative layers. This study introduces a Pathological Clue-driven\nRepresentation Learning (PCRL) model to build cross-modal representations based\non pathological clues and naturally adapt them for accurate report generation.\nSpecifically, we construct pathological clues from perspectives of segmented\nregions, pathological entities, and report themes, to fully grasp visual\npathological patterns and learn cross-modal feature representations. To adapt\nthe representations for the text generation task, we bridge the gap between\nrepresentation learning and report generation by using a unified large language\nmodel (LLM) with task-tailored instructions. These crafted instructions enable\nthe LLM to be flexibly fine-tuned across tasks and smoothly transfer the\nsemantic representation for report generation. Experiments demonstrate that our\nmethod outperforms previous methods and achieves SoTA performance. Our code is\navailable at \"https://github.com/Chauncey-Jheng/PCRL-MRG\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain CT report generation is significant to aid physicians in diagnosing\ncranial diseases. Recent studies concentrate on handling the consistency\nbetween visual and textual pathological features to improve the coherence of\nreport. However, there exist some challenges: 1) Redundant visual representing:\nMassive irrelevant areas in 3D scans distract models from representing salient\nvisual contexts. 2) Shifted semantic representing: Limited medical corpus\ncauses difficulties for models to transfer the learned textual representations\nto generative layers. This study introduces a Pathological Clue-driven\nRepresentation Learning (PCRL) model to build cross-modal representations based\non pathological clues and naturally adapt them for accurate report generation.\nSpecifically, we construct pathological clues from perspectives of segmented\nregions, pathological entities, and report themes, to fully grasp visual\npathological patterns and learn cross-modal feature representations. To adapt\nthe representations for the text generation task, we bridge the gap between\nrepresentation learning and report generation by using a unified large language\nmodel (LLM) with task-tailored instructions. These crafted instructions enable\nthe LLM to be flexibly fine-tuned across tasks and smoothly transfer the\nsemantic representation for report generation. Experiments demonstrate that our\nmethod outperforms previous methods and achieves SoTA performance. Our code is\navailable at \"https://github.com/Chauncey-Jheng/PCRL-MRG\"."
                },
                "authors": [
                    {
                        "name": "Chengxin Zheng"
                    },
                    {
                        "name": "Junzhong Ji"
                    },
                    {
                        "name": "Yanzhao Shi"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "arxiv_comment": "Our work has been accepted by EMNLP2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17011v2",
                "updated": "2024-10-01T10:38:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    38,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-02-26T20:35:34Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    20,
                    35,
                    34,
                    0,
                    57,
                    0
                ],
                "title": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion"
                },
                "summary": "Inferring contextually-relevant and diverse commonsense to understand\nnarratives remains challenging for knowledge models. In this work, we develop a\nseries of knowledge models, DiffuCOMET, that leverage diffusion to learn to\nreconstruct the implicit semantic connections between narrative contexts and\nrelevant commonsense knowledge. Across multiple diffusion steps, our method\nprogressively refines a representation of commonsense facts that is anchored to\na narrative, producing contextually-relevant and diverse commonsense inferences\nfor an input context. To evaluate DiffuCOMET, we introduce new metrics for\ncommonsense inference that more closely measure knowledge diversity and\ncontextual relevance. Our results on two different benchmarks, ComFact and\nWebNLG+, show that knowledge generated by DiffuCOMET achieves a better\ntrade-off between commonsense diversity, contextual relevance and alignment to\nknown gold references, compared to baseline knowledge models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring contextually-relevant and diverse commonsense to understand\nnarratives remains challenging for knowledge models. In this work, we develop a\nseries of knowledge models, DiffuCOMET, that leverage diffusion to learn to\nreconstruct the implicit semantic connections between narrative contexts and\nrelevant commonsense knowledge. Across multiple diffusion steps, our method\nprogressively refines a representation of commonsense facts that is anchored to\na narrative, producing contextually-relevant and diverse commonsense inferences\nfor an input context. To evaluate DiffuCOMET, we introduce new metrics for\ncommonsense inference that more closely measure knowledge diversity and\ncontextual relevance. Our results on two different benchmarks, ComFact and\nWebNLG+, show that knowledge generated by DiffuCOMET achieves a better\ntrade-off between commonsense diversity, contextual relevance and alignment to\nknown gold references, compared to baseline knowledge models."
                },
                "authors": [
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.04109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.04109v2",
                "updated": "2024-10-01T10:30:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    30,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2023-09-08T04:10:01Z",
                "published_parsed": [
                    2023,
                    9,
                    8,
                    4,
                    10,
                    1,
                    4,
                    251,
                    0
                ],
                "title": "From Text to Mask: Localizing Entities Using the Attention of\n  Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Mask: Localizing Entities Using the Attention of\n  Text-to-Image Diffusion Models"
                },
                "summary": "Diffusion models have revolted the field of text-to-image generation\nrecently. The unique way of fusing text and image information contributes to\ntheir remarkable capability of generating highly text-related images. From\nanother perspective, these generative models imply clues about the precise\ncorrelation between words and pixels. In this work, a simple but effective\nmethod is proposed to utilize the attention mechanism in the denoising network\nof text-to-image diffusion models. Without re-training nor inference-time\noptimization, the semantic grounding of phrases can be attained directly. We\nevaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under\nweakly-supervised semantic segmentation setting and our method achieves\nsuperior performance to prior methods. In addition, the acquired word-pixel\ncorrelation is found to be generalizable for the learned text embedding of\ncustomized generation methods, requiring only a few modifications. To validate\nour discovery, we introduce a new practical task called \"personalized referring\nimage segmentation\" with a new dataset. Experiments in various situations\ndemonstrate the advantages of our method compared to strong baselines on this\ntask. In summary, our work reveals a novel way to extract the rich multi-modal\nknowledge hidden in diffusion models for segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolted the field of text-to-image generation\nrecently. The unique way of fusing text and image information contributes to\ntheir remarkable capability of generating highly text-related images. From\nanother perspective, these generative models imply clues about the precise\ncorrelation between words and pixels. In this work, a simple but effective\nmethod is proposed to utilize the attention mechanism in the denoising network\nof text-to-image diffusion models. Without re-training nor inference-time\noptimization, the semantic grounding of phrases can be attained directly. We\nevaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under\nweakly-supervised semantic segmentation setting and our method achieves\nsuperior performance to prior methods. In addition, the acquired word-pixel\ncorrelation is found to be generalizable for the learned text embedding of\ncustomized generation methods, requiring only a few modifications. To validate\nour discovery, we introduce a new practical task called \"personalized referring\nimage segmentation\" with a new dataset. Experiments in various situations\ndemonstrate the advantages of our method compared to strong baselines on this\ntask. In summary, our work reveals a novel way to extract the rich multi-modal\nknowledge hidden in diffusion models for segmentation."
                },
                "authors": [
                    {
                        "name": "Changming Xiao"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Changshui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changshui Zhang"
                },
                "author": "Changshui Zhang",
                "arxiv_doi": "10.1016/j.neucom.2024.128437",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2024.128437",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.04109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.04109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A revised version of this paper will be published in Neurocomputing,\n  see https://doi.org/10.1016/j.neucom.2024.128437",
                "arxiv_journal_ref": "Neurocomputing, Volume 610, 2024, 128437",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19382v2",
                "updated": "2024-10-01T10:28:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    28,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-28T15:13:04Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    13,
                    4,
                    5,
                    272,
                    0
                ],
                "title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with\n  Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly impacted\nthe domain of multi-hop question answering (MHQA), where systems are required\nto aggregate information and infer answers from disparate pieces of text.\nHowever, the autoregressive nature of LLMs inherently poses a challenge as\nerrors may accumulate if mistakes are made in the intermediate reasoning steps.\nThis paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question\nAnswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to\nidentify optimal reasoning paths in MHQA tasks, mitigating the error\npropagation from sequential reasoning processes. Unlike previous works, we\npropose a zero-shot prompting method, which relies solely on instructions\nwithout the support of hand-crafted few-shot examples that typically require\ndomain expertise. We also introduce a behavioral cloning approach (MZQA-BC)\ntrained on self-generated MCTS inference trajectories, achieving an over\n10-fold increase in reasoning speed with bare compromise in performance. The\nefficacy of our method is validated on standard benchmarks such as HotpotQA,\n2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly impacted\nthe domain of multi-hop question answering (MHQA), where systems are required\nto aggregate information and infer answers from disparate pieces of text.\nHowever, the autoregressive nature of LLMs inherently poses a challenge as\nerrors may accumulate if mistakes are made in the intermediate reasoning steps.\nThis paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question\nAnswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to\nidentify optimal reasoning paths in MHQA tasks, mitigating the error\npropagation from sequential reasoning processes. Unlike previous works, we\npropose a zero-shot prompting method, which relies solely on instructions\nwithout the support of hand-crafted few-shot examples that typically require\ndomain expertise. We also introduce a behavioral cloning approach (MZQA-BC)\ntrained on self-generated MCTS inference trajectories, achieving an over\n10-fold increase in reasoning speed with bare compromise in performance. The\nefficacy of our method is validated on standard benchmarks such as HotpotQA,\n2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing\nframeworks."
                },
                "authors": [
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Jaewook Shin"
                    },
                    {
                        "name": "Youngjin Ahn"
                    },
                    {
                        "name": "Seokin Seo"
                    },
                    {
                        "name": "Ohjoon Kwon"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kee-Eung Kim"
                },
                "author": "Kee-Eung Kim",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08367v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08367v4",
                "updated": "2024-10-01T10:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    11,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2023-12-13T18:58:15Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    18,
                    58,
                    15,
                    2,
                    347,
                    0
                ],
                "title": "ViLA: Efficient Video-Language Alignment for Video Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLA: Efficient Video-Language Alignment for Video Question Answering"
                },
                "summary": "In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose an efficient Video-Language Alignment (ViLA)\nnetwork. Our ViLA model addresses both efficient frame sampling and effective\ncross-modal alignment in a unified way. In our ViLA network, we design a new\nlearnable text-guided Frame-Prompter together with a new cross-modal\ndistillation (QFormer-Distiller) module. Pre-trained large image-language\nmodels have shown promising results on problems such as visual question\nanswering (VQA). However, how to efficiently and effectively sample video\nframes when adapting pre-trained large image-language model to video-language\nalignment is still the major challenge. Compared with prior work, our ViLA\nmodel demonstrates the capability of selecting key frames with critical\ncontents, thus improving the video-language alignment accuracy while reducing\nthe inference latency +3.3% on NExT-QA Temporal with 3.0X speed up). Overall,\nour ViLA network outperforms the state-of-the-art methods on the video\nquestion-answering benchmarks: +4.6% on STAR Interaction, +2.2% on STAR average\nwith 3.0X speed up, ours 2-frames out-perform SeViLA 4-frames on the VLEP\ndataset with 4.2X speed-up. The code will be available at\nhttps://github.com/xijun-cs/ViLA."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Junbang Liang"
                    },
                    {
                        "name": "Chun-Kai Wang"
                    },
                    {
                        "name": "Kenan Deng"
                    },
                    {
                        "name": "Yu Lou"
                    },
                    {
                        "name": "Ming Lin"
                    },
                    {
                        "name": "Shan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shan Yang"
                },
                "author": "Shan Yang",
                "arxiv_comment": "ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08367v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08367v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08626v3",
                "updated": "2024-10-01T10:07:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    7,
                    27,
                    1,
                    275,
                    0
                ],
                "published": "2023-12-07T20:17:09Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    20,
                    17,
                    9,
                    3,
                    341,
                    0
                ],
                "title": "Validation and Comparison of Non-Stationary Cognitive Models: A\n  Diffusion Model Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validation and Comparison of Non-Stationary Cognitive Models: A\n  Diffusion Model Application"
                },
                "summary": "Cognitive processes undergo various fluctuations and transient states across\ndifferent temporal scales. Superstatistics are emerging as a flexible framework\nfor incorporating such non-stationary dynamics into existing cognitive model\nclasses. In this work, we provide the first experimental validation of\nsuperstatistics and formal comparison of four non-stationary diffusion decision\nmodels in a specifically designed perceptual decision-making task. Task\ndifficulty and speed-accuracy trade-off were systematically manipulated to\ninduce expected changes in model parameters. To validate our models, we assess\nwhether the inferred parameter trajectories align with the patterns and\nsequences of the experimental manipulations. To address computational\nchallenges, we present novel deep learning techniques for amortized Bayesian\nestimation and comparison of models with time-varying parameters. Our findings\nindicate that transition models incorporating both gradual and abrupt parameter\nshifts provide the best fit to the empirical data. Moreover, we find that the\ninferred parameter trajectories closely mirror the sequence of experimental\nmanipulations. Posterior re-simulations further underscore the ability of the\nmodels to faithfully reproduce critical data patterns. Accordingly, our results\nsuggest that the inferred non-stationary dynamics may reflect actual changes in\nthe targeted psychological constructs. We argue that our initial experimental\nvalidation paves the way for the widespread application of superstatistics in\ncognitive modeling and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive processes undergo various fluctuations and transient states across\ndifferent temporal scales. Superstatistics are emerging as a flexible framework\nfor incorporating such non-stationary dynamics into existing cognitive model\nclasses. In this work, we provide the first experimental validation of\nsuperstatistics and formal comparison of four non-stationary diffusion decision\nmodels in a specifically designed perceptual decision-making task. Task\ndifficulty and speed-accuracy trade-off were systematically manipulated to\ninduce expected changes in model parameters. To validate our models, we assess\nwhether the inferred parameter trajectories align with the patterns and\nsequences of the experimental manipulations. To address computational\nchallenges, we present novel deep learning techniques for amortized Bayesian\nestimation and comparison of models with time-varying parameters. Our findings\nindicate that transition models incorporating both gradual and abrupt parameter\nshifts provide the best fit to the empirical data. Moreover, we find that the\ninferred parameter trajectories closely mirror the sequence of experimental\nmanipulations. Posterior re-simulations further underscore the ability of the\nmodels to faithfully reproduce critical data patterns. Accordingly, our results\nsuggest that the inferred non-stationary dynamics may reflect actual changes in\nthe targeted psychological constructs. We argue that our initial experimental\nvalidation paves the way for the widespread application of superstatistics in\ncognitive modeling and beyond."
                },
                "authors": [
                    {
                        "name": "Lukas Schumacher"
                    },
                    {
                        "name": "Martin Schnuerch"
                    },
                    {
                        "name": "Andreas Voss"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v3",
                "updated": "2024-10-02T07:38:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    38,
                    2,
                    2,
                    276,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03032v2",
                "updated": "2024-10-01T09:53:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    53,
                    44,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-05T15:11:05Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    15,
                    11,
                    5,
                    1,
                    65,
                    0
                ],
                "title": "Logic Programming with Multiplicative Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Programming with Multiplicative Structures"
                },
                "summary": "In the logic programming paradigm, a program is defined by a set of methods,\neach of which can be executed when specific conditions are met during the\ncurrent state of an execution. The semantics of these programs can be elegantly\nrepresented using sequent calculi, in which each method is linked to an\ninference rule. In this context, proof search mirrors the program's execution.\nPrevious works introduced a framework in which the process of constructing\nproof nets is employed to model executions, as opposed to the traditional\napproach of proof search in sequent calculus.\n  This paper further extends this investigation by focussing on the pure\nmultiplicative fragment of this framework. We demonstrate, providing practical\nexamples, the capability to define logic programming methods with\ncontext-sensitive behaviors solely through specific resource-preserving and\ncontext-free operations, corresponding to certain generalized multiplicative\nconnectives explored in existing literature. We show how some of these methods,\nalthough still multiplicative, escape the purely multiplicative fragment of\nLinear Logic (MLL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the logic programming paradigm, a program is defined by a set of methods,\neach of which can be executed when specific conditions are met during the\ncurrent state of an execution. The semantics of these programs can be elegantly\nrepresented using sequent calculi, in which each method is linked to an\ninference rule. In this context, proof search mirrors the program's execution.\nPrevious works introduced a framework in which the process of constructing\nproof nets is employed to model executions, as opposed to the traditional\napproach of proof search in sequent calculus.\n  This paper further extends this investigation by focussing on the pure\nmultiplicative fragment of this framework. We demonstrate, providing practical\nexamples, the capability to define logic programming methods with\ncontext-sensitive behaviors solely through specific resource-preserving and\ncontext-free operations, corresponding to certain generalized multiplicative\nconnectives explored in existing literature. We show how some of these methods,\nalthough still multiplicative, escape the purely multiplicative fragment of\nLinear Logic (MLL)."
                },
                "authors": [
                    {
                        "name": "Matteo Acclavio"
                    },
                    {
                        "name": "Roberto Maieli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Maieli"
                },
                "arxiv_affiliation": "Università Roma Tre",
                "author": "Roberto Maieli",
                "arxiv_doi": "10.4204/EPTCS.408.3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.408.3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.03032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings DCM 2023, arXiv:2409.19298",
                "arxiv_journal_ref": "EPTCS 408, 2024, pp. 42-61",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07663v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07663v3",
                "updated": "2024-10-01T09:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    23,
                    0,
                    1,
                    275,
                    0
                ],
                "published": "2023-08-15T09:26:55Z",
                "published_parsed": [
                    2023,
                    8,
                    15,
                    9,
                    26,
                    55,
                    1,
                    227,
                    0
                ],
                "title": "Coherent set identification via direct low rank maximum likelihood\n  estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent set identification via direct low rank maximum likelihood\n  estimation"
                },
                "summary": "We analyze connections between two low rank modeling approaches from the last\ndecade for treating dynamical data. The first one is the coherence problem (or\ncoherent set approach), where groups of states are sought that evolve under the\naction of a stochastic transition matrix in a way maximally distinguishable\nfrom other groups. The second one is a low rank factorization approach for\nstochastic matrices, called Direct Bayesian Model Reduction (DBMR), which\nestimates the low rank factors directly from observed data. We show that DBMR\nresults in a low rank model that is a projection of the full model, and exploit\nthis insight to infer bounds on a quantitative measure of coherence within the\nreduced model. Both approaches can be formulated as optimization problems, and\nwe also prove a bound between their respective objectives. On a broader scope,\nthis work relates the two classical loss functions of nonnegative matrix\nfactorization, namely the Frobenius norm and the generalized Kullback--Leibler\ndivergence, and suggests new links between likelihood-based and\nprojection-based estimation of probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze connections between two low rank modeling approaches from the last\ndecade for treating dynamical data. The first one is the coherence problem (or\ncoherent set approach), where groups of states are sought that evolve under the\naction of a stochastic transition matrix in a way maximally distinguishable\nfrom other groups. The second one is a low rank factorization approach for\nstochastic matrices, called Direct Bayesian Model Reduction (DBMR), which\nestimates the low rank factors directly from observed data. We show that DBMR\nresults in a low rank model that is a projection of the full model, and exploit\nthis insight to infer bounds on a quantitative measure of coherence within the\nreduced model. Both approaches can be formulated as optimization problems, and\nwe also prove a bound between their respective objectives. On a broader scope,\nthis work relates the two classical loss functions of nonnegative matrix\nfactorization, namely the Frobenius norm and the generalized Kullback--Leibler\ndivergence, and suggests new links between likelihood-based and\nprojection-based estimation of probabilistic models."
                },
                "authors": [
                    {
                        "name": "Robert Polzin"
                    },
                    {
                        "name": "Ilja Klebanov"
                    },
                    {
                        "name": "Nikolas Nüsken"
                    },
                    {
                        "name": "Péter Koltai"
                    }
                ],
                "author_detail": {
                    "name": "Péter Koltai"
                },
                "author": "Péter Koltai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07663v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07663v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 62M05, 37M10, 15A23, 60J22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18943v2",
                "updated": "2024-10-01T09:20:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    20,
                    58,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T17:44:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models"
                },
                "summary": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler."
                },
                "authors": [
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "yuelin bai"
                    },
                    {
                        "name": "Run Luo"
                    },
                    {
                        "name": "Longze Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16636v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16636v3",
                "updated": "2024-10-01T09:15:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    15,
                    17,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-23T16:52:42Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    16,
                    52,
                    42,
                    1,
                    205,
                    0
                ],
                "title": "Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles"
                },
                "summary": "Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes."
                },
                "authors": [
                    {
                        "name": "Seamie Hayes"
                    },
                    {
                        "name": "Sushil Sharma"
                    },
                    {
                        "name": "Ciarán Eising"
                    }
                ],
                "author_detail": {
                    "name": "Ciarán Eising"
                },
                "author": "Ciarán Eising",
                "arxiv_comment": "This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library",
                "arxiv_journal_ref": "Proceedings of the Irish Machine Vision and Image Processing\n  Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16636v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15301v2",
                "updated": "2024-10-01T09:05:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    5,
                    45,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-27T15:03:01Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    3,
                    1,
                    1,
                    240,
                    0
                ],
                "title": "The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization"
                },
                "summary": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Minghai Qin"
                    }
                ],
                "author_detail": {
                    "name": "Minghai Qin"
                },
                "author": "Minghai Qin",
                "arxiv_comment": "27 pages, 41 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v3",
                "updated": "2024-10-01T08:50:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    50,
                    1,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10691v2",
                "updated": "2024-10-01T08:48:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    48,
                    34,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-20T09:42:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches"
                },
                "summary": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge."
                },
                "authors": [
                    {
                        "name": "Yanjie Dong"
                    },
                    {
                        "name": "Haijun Zhang"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10312v2",
                "updated": "2024-10-01T08:40:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    40,
                    59,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-16T17:59:00Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    17,
                    59,
                    0,
                    3,
                    137,
                    0
                ],
                "title": "KiDS-1000 and DES-Y1 combined: Cosmology from peak count statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KiDS-1000 and DES-Y1 combined: Cosmology from peak count statistics"
                },
                "summary": "We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and\nextract cosmological parameter constraints based on the cosmic shear peak count\nstatistics. Peaks are identified in aperture mass maps in which the filter is\nmaximally sensitive to angular scales in the range 2-4arcmin, probing deep into\nthe non-linear regime of structure formation. We interpret our results with a\nsimulation-based inference pipeline, sampling over a broad $w$CDM prior volume\nand marginalising over uncertainties on shape calibration, photometric redshift\ndistribution, intrinsic alignment and baryonic feedback. Our measurements\nconstrain the structure growth parameter and the amplitude of the non-linear\nintrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm\nm}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}=\n0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000\nresults based on two-point shear statistics. These results are robust against\nmodelling of the non-linear physics, different scale cuts and selections of\ntomographic bins. The posterior is also consistent with that from the Dark\nEnergy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al\n(2021), and hence we jointly analyse both surveys. We obtain $\\Sigma_8^{\\rm\njoint} \\equiv \\sigma_8\\left[\\Omega_{\\rm\nm}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck\n$w$CDM results. The shear-CMB tension on this parameter increases to\n$3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with\n$S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the\ntightest constraints to date on this quantity. (abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and\nextract cosmological parameter constraints based on the cosmic shear peak count\nstatistics. Peaks are identified in aperture mass maps in which the filter is\nmaximally sensitive to angular scales in the range 2-4arcmin, probing deep into\nthe non-linear regime of structure formation. We interpret our results with a\nsimulation-based inference pipeline, sampling over a broad $w$CDM prior volume\nand marginalising over uncertainties on shape calibration, photometric redshift\ndistribution, intrinsic alignment and baryonic feedback. Our measurements\nconstrain the structure growth parameter and the amplitude of the non-linear\nintrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm\nm}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}=\n0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000\nresults based on two-point shear statistics. These results are robust against\nmodelling of the non-linear physics, different scale cuts and selections of\ntomographic bins. The posterior is also consistent with that from the Dark\nEnergy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al\n(2021), and hence we jointly analyse both surveys. We obtain $\\Sigma_8^{\\rm\njoint} \\equiv \\sigma_8\\left[\\Omega_{\\rm\nm}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck\n$w$CDM results. The shear-CMB tension on this parameter increases to\n$3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with\n$S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the\ntightest constraints to date on this quantity. (abridged)"
                },
                "authors": [
                    {
                        "name": "Joachim Harnois-Deraps"
                    },
                    {
                        "name": "Sven Heydenreich"
                    },
                    {
                        "name": "Benjamin Giblin"
                    },
                    {
                        "name": "Nicolas Martinet"
                    },
                    {
                        "name": "Tilman Troester"
                    },
                    {
                        "name": "Marika Asgari"
                    },
                    {
                        "name": "Pierre Burger"
                    },
                    {
                        "name": "Tiago Castro"
                    },
                    {
                        "name": "Klaus Dolag"
                    },
                    {
                        "name": "Catherine Heymans"
                    },
                    {
                        "name": "Hendrik Hildebrandt"
                    },
                    {
                        "name": "Benjamin Joachimi"
                    },
                    {
                        "name": "Angus H. Wright"
                    }
                ],
                "author_detail": {
                    "name": "Angus H. Wright"
                },
                "author": "Angus H. Wright",
                "arxiv_comment": "26 pages, 20 figures. MNRAS Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17518v2",
                "updated": "2024-10-01T08:08:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    8,
                    42,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-26T04:01:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    1,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "Multi-Designated Detector Watermarking for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Designated Detector Watermarking for Language Models"
                },
                "summary": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics."
                },
                "authors": [
                    {
                        "name": "Zhengan Huang"
                    },
                    {
                        "name": "Gongxian Zeng"
                    },
                    {
                        "name": "Xin Mu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19872v2",
                "updated": "2024-10-01T07:34:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    34,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:13:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    13,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration"
                },
                "summary": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}."
                },
                "authors": [
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Zhaoyu Fan"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Qianru Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianru Sun"
                },
                "author": "Qianru Sun",
                "arxiv_comment": "Accepted by NeurIPS 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18618v2",
                "updated": "2024-10-01T07:29:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    29,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T10:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback"
                },
                "summary": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback."
                },
                "authors": [
                    {
                        "name": "Jaepill Choi"
                    },
                    {
                        "name": "Kyubyung Chae"
                    },
                    {
                        "name": "Jiwoo Song"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19898v2",
                "updated": "2024-10-01T07:11:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    11,
                    44,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:56:35Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    56,
                    35,
                    0,
                    274,
                    0
                ],
                "title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs"
                },
                "summary": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0."
                },
                "authors": [
                    {
                        "name": "Yuho Lee"
                    },
                    {
                        "name": "Taewon Yun"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Accepted at EMNLP-Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19663v2",
                "updated": "2024-10-01T06:35:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    35,
                    24,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-29T11:29:57Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    11,
                    29,
                    57,
                    6,
                    273,
                    0
                ],
                "title": "Identifying Knowledge Editing Types in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Knowledge Editing Types in Large Language Models"
                },
                "summary": "Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Huijun Liu"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yu"
                },
                "author": "Jie Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18384v2",
                "updated": "2024-10-01T06:17:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    17,
                    52,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-29T02:43:23Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    43,
                    23,
                    0,
                    120,
                    0
                ],
                "title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions"
                },
                "summary": "Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions."
                },
                "authors": [
                    {
                        "name": "Jordan Meadows"
                    },
                    {
                        "name": "Tamsin James"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19389v2",
                "updated": "2024-10-01T06:07:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    7,
                    24,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-27T17:59:01Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    59,
                    1,
                    3,
                    179,
                    0
                ],
                "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding"
                },
                "summary": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "NeurIPS-2024. Project page:\n  https://lxtgh.github.io/project/omg_llava/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20441v2",
                "updated": "2024-10-01T06:03:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    3,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T16:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-adaptive Zero-shot Chain-of-Thought Prompting"
                },
                "summary": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Renchu Guan"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15811v2",
                "updated": "2024-10-01T05:56:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    56,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-28T14:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    11,
                    59,
                    2,
                    241,
                    0
                ],
                "title": "Identifying Influential and Vulnerable Nodes in Interaction Networks\n  through Estimation of Transfer Entropy Between Univariate and Multivariate\n  Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Influential and Vulnerable Nodes in Interaction Networks\n  through Estimation of Transfer Entropy Between Univariate and Multivariate\n  Time Series"
                },
                "summary": "Transfer entropy (TE) is a powerful tool for measuring causal relationships\nwithin interaction networks. Traditionally, TE and its conditional variants are\napplied pairwise between dynamic variables to infer these causal relationships.\nHowever, identifying the most influential or vulnerable node in a system\nrequires measuring the causal influence of each component on the entire system\nand vice versa. In this paper, I propose using outgoing and incoming transfer\nentropy-where outgoing TE quantifies the influence of a node on the rest of the\nsystem, and incoming TE measures the influence of the rest of the system on the\nnode. The node with the highest outgoing TE is identified as the most\ninfluential, or \"hub\", while the node with the highest incoming TE is the most\nvulnerable, or \"anti-hub\". Since these measures involve transfer entropy\nbetween univariate and multivariate time series, naive estimation methods can\nresult in significant errors, particularly when the number of variables is\ncomparable to or exceeds the number of samples. To address this, I introduce a\nnovel estimation scheme that computes outgoing and incoming TE only between\nsignificantly interacting partners. The feasibility of this approach is\ndemonstrated by using synthetic data, and by applying it to real data of oral\nmicrobiota. The method successfully identifies the bacterial species known to\nbe key players in the bacterial community, demonstrating the power of the new\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer entropy (TE) is a powerful tool for measuring causal relationships\nwithin interaction networks. Traditionally, TE and its conditional variants are\napplied pairwise between dynamic variables to infer these causal relationships.\nHowever, identifying the most influential or vulnerable node in a system\nrequires measuring the causal influence of each component on the entire system\nand vice versa. In this paper, I propose using outgoing and incoming transfer\nentropy-where outgoing TE quantifies the influence of a node on the rest of the\nsystem, and incoming TE measures the influence of the rest of the system on the\nnode. The node with the highest outgoing TE is identified as the most\ninfluential, or \"hub\", while the node with the highest incoming TE is the most\nvulnerable, or \"anti-hub\". Since these measures involve transfer entropy\nbetween univariate and multivariate time series, naive estimation methods can\nresult in significant errors, particularly when the number of variables is\ncomparable to or exceeds the number of samples. To address this, I introduce a\nnovel estimation scheme that computes outgoing and incoming TE only between\nsignificantly interacting partners. The feasibility of this approach is\ndemonstrated by using synthetic data, and by applying it to real data of oral\nmicrobiota. The method successfully identifies the bacterial species known to\nbe key players in the bacterial community, demonstrating the power of the new\nmethod."
                },
                "authors": [
                    {
                        "name": "Julian Lee"
                    }
                ],
                "author_detail": {
                    "name": "Julian Lee"
                },
                "author": "Julian Lee",
                "arxiv_comment": "36 pages, 18 figures, Explanations on the behavior of OutTE(R) are\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v2",
                "updated": "2024-10-01T05:55:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    55,
                    33,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07198v2",
                "updated": "2024-10-01T05:52:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    52,
                    11,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-10T17:56:07Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    17,
                    56,
                    7,
                    2,
                    101,
                    0
                ],
                "title": "A Foundation Model for Zero-shot Logical Query Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundation Model for Zero-shot Logical Query Reasoning"
                },
                "summary": "Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond\nsimple KG completion and aims at answering compositional queries comprised of\nmultiple projections and logical operations. Existing CLQA methods that learn\nparameters bound to certain entity or relation vocabularies can only be applied\nto the graph they are trained on which requires substantial training time\nbefore being deployed on a new graph. Here we present UltraQuery, the first\nfoundation model for inductive reasoning that can zero-shot answer logical\nqueries on any KG. The core idea of UltraQuery is to derive both projections\nand logical operations as vocabulary-independent functions which generalize to\nnew entities and relations in any KG. With the projection operation initialized\nfrom a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on\nany KG after finetuning on a single dataset. Experimenting on 23 datasets,\nUltraQuery in the zero-shot inference mode shows competitive or better query\nanswering performance than best available baselines and sets a new state of the\nart on 15 of them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond\nsimple KG completion and aims at answering compositional queries comprised of\nmultiple projections and logical operations. Existing CLQA methods that learn\nparameters bound to certain entity or relation vocabularies can only be applied\nto the graph they are trained on which requires substantial training time\nbefore being deployed on a new graph. Here we present UltraQuery, the first\nfoundation model for inductive reasoning that can zero-shot answer logical\nqueries on any KG. The core idea of UltraQuery is to derive both projections\nand logical operations as vocabulary-independent functions which generalize to\nnew entities and relations in any KG. With the projection operation initialized\nfrom a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on\nany KG after finetuning on a single dataset. Experimenting on 23 datasets,\nUltraQuery in the zero-shot inference mode shows competitive or better query\nanswering performance than best available baselines and sets a new state of the\nart on 15 of them."
                },
                "authors": [
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Bruno Ribeiro"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Zhaocheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaocheng Zhu"
                },
                "author": "Zhaocheng Zhu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08642v2",
                "updated": "2024-10-01T05:42:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    42,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-13T08:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "title": "CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks"
                },
                "summary": "Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%)."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Junzhe Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Jing Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jing Bai"
                },
                "author": "Jing Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v2",
                "updated": "2024-10-01T05:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    37,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13647v2",
                "updated": "2024-10-01T05:28:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    28,
                    54,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-18T16:25:17Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    25,
                    17,
                    3,
                    200,
                    0
                ],
                "title": "Weak-to-Strong Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Reasoning"
                },
                "summary": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}."
                },
                "authors": [
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yan Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v2",
                "updated": "2024-10-01T05:19:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    19,
                    17,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Under Review in the Special Issue of ACM Transactions on Software\n  Engineering and Methodology (TOSEM): 2030 Software Engineering Roadmap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16674v3",
                "updated": "2024-10-01T04:45:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    45,
                    4,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-25T07:06:14Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "title": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models"
                },
                "summary": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    }
                ],
                "author_detail": {
                    "name": "Toyotaro Suzumura"
                },
                "author": "Toyotaro Suzumura",
                "arxiv_comment": "Risks: The 1st International Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommendation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06341v2",
                "updated": "2024-10-01T04:44:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    44,
                    29,
                    1,
                    275,
                    0
                ],
                "published": "2023-10-10T06:22:06Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    6,
                    22,
                    6,
                    1,
                    283,
                    0
                ],
                "title": "Federated Learning with Reduced Information Leakage and Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning with Reduced Information Leakage and Computation"
                },
                "summary": "Federated learning (FL) is a distributed learning paradigm that allows\nmultiple decentralized clients to collaboratively learn a common model without\nsharing local data. Although local data is not exposed directly, privacy\nconcerns nonetheless exist as clients' sensitive information can be inferred\nfrom intermediate computations. Moreover, such information leakage accumulates\nsubstantially over time as the same data is repeatedly used during the\niterative learning process. As a result, it can be particularly difficult to\nbalance the privacy-accuracy trade-off when designing privacy-preserving FL\nalgorithms. This paper introduces Upcycled-FL, a simple yet effective strategy\nthat applies first-order approximation at every even round of model update.\nUnder this strategy, half of the FL updates incur no information leakage and\nrequire much less computational and transmission costs. We first conduct the\ntheoretical analysis on the convergence (rate) of Upcycled-FL and then apply\ntwo perturbation mechanisms to preserve privacy. Extensive experiments on both\nsynthetic and real-world data show that the Upcycled-FL strategy can be adapted\nto many existing FL frameworks and consistently improve the privacy-accuracy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a distributed learning paradigm that allows\nmultiple decentralized clients to collaboratively learn a common model without\nsharing local data. Although local data is not exposed directly, privacy\nconcerns nonetheless exist as clients' sensitive information can be inferred\nfrom intermediate computations. Moreover, such information leakage accumulates\nsubstantially over time as the same data is repeatedly used during the\niterative learning process. As a result, it can be particularly difficult to\nbalance the privacy-accuracy trade-off when designing privacy-preserving FL\nalgorithms. This paper introduces Upcycled-FL, a simple yet effective strategy\nthat applies first-order approximation at every even round of model update.\nUnder this strategy, half of the FL updates incur no information leakage and\nrequire much less computational and transmission costs. We first conduct the\ntheoretical analysis on the convergence (rate) of Upcycled-FL and then apply\ntwo perturbation mechanisms to preserve privacy. Extensive experiments on both\nsynthetic and real-world data show that the Upcycled-FL strategy can be adapted\nto many existing FL frameworks and consistently improve the privacy-accuracy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Tongxin Yin"
                    },
                    {
                        "name": "Xuwei Tan"
                    },
                    {
                        "name": "Xueru Zhang"
                    },
                    {
                        "name": "Mohammad Mahdi Khalili"
                    },
                    {
                        "name": "Mingyan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Liu"
                },
                "author": "Mingyan Liu",
                "arxiv_comment": "Accepted by Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12566v3",
                "updated": "2024-10-01T04:42:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    42,
                    48,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-18T12:52:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    52,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users."
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Xin Yu"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04732v3",
                "updated": "2024-10-01T04:41:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    41,
                    53,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-07T18:35:54Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    18,
                    35,
                    54,
                    3,
                    67,
                    0
                ],
                "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We from Intelligent Visual Deductive Reasoning?"
                },
                "summary": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "He Bai"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Shuangfei Zhai"
                    },
                    {
                        "name": "Josh Susskind"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "arxiv_comment": "COLM 2024. https://github.com/apple/ml-rpm-bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19894v2",
                "updated": "2024-10-01T04:35:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    35,
                    5,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:53:03Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    3,
                    0,
                    274,
                    0
                ],
                "title": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation"
                },
                "summary": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02262v2",
                "updated": "2024-10-01T04:10:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    10,
                    34,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-03T22:56:43Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    22,
                    56,
                    43,
                    4,
                    307,
                    0
                ],
                "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs"
                },
                "summary": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA ."
                },
                "authors": [
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Chandan Singh"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Bin Yu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "arxiv_comment": "The 12th International Conference on Learning Representations (ICLR\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04187v2",
                "updated": "2024-10-01T03:26:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    26,
                    15,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-06T11:05:12Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    5,
                    12,
                    4,
                    250,
                    0
                ],
                "title": "LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID\n  Feature Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID\n  Feature Integration"
                },
                "summary": "The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is\nintroduced as a novel multi-object tracking (MOT) approach. It enhances\nReID-based trackers by eliminating inference, pre-processing, post-processing,\nand ReID model training costs. LITE uses real-time appearance features without\ncompromising speed. By integrating appearance feature extraction directly into\nthe tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE\ndemonstrates significant performance improvements. The simplest implementation\nof LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS\non the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four\ntimes faster on the more crowded MOT20 dataset, while maintaining similar\naccuracy. Additionally, a new evaluation framework for tracking-by-detection\napproaches reveals that conventional trackers like DeepSORT remain competitive\nwith modern state-of-the-art trackers when evaluated under fair conditions. The\ncode will be available post-publication at https://github.com/Jumabek/LITE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is\nintroduced as a novel multi-object tracking (MOT) approach. It enhances\nReID-based trackers by eliminating inference, pre-processing, post-processing,\nand ReID model training costs. LITE uses real-time appearance features without\ncompromising speed. By integrating appearance feature extraction directly into\nthe tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE\ndemonstrates significant performance improvements. The simplest implementation\nof LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS\non the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four\ntimes faster on the more crowded MOT20 dataset, while maintaining similar\naccuracy. Additionally, a new evaluation framework for tracking-by-detection\napproaches reveals that conventional trackers like DeepSORT remain competitive\nwith modern state-of-the-art trackers when evaluated under fair conditions. The\ncode will be available post-publication at https://github.com/Jumabek/LITE."
                },
                "authors": [
                    {
                        "name": "Jumabek Alikhanov"
                    },
                    {
                        "name": "Dilshod Obidov"
                    },
                    {
                        "name": "Hakil Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hakil Kim"
                },
                "author": "Hakil Kim",
                "arxiv_comment": "15 pages, 6 figures, to be published in ICONIP-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v3",
                "updated": "2024-10-01T03:12:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    12,
                    35,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19868v2",
                "updated": "2024-10-01T02:57:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    57,
                    50,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T01:59:38Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    1,
                    59,
                    38,
                    0,
                    274,
                    0
                ],
                "title": "The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for\n  Digital Library Document Recommendation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for\n  Digital Library Document Recommendation Tasks"
                },
                "summary": "This paper investigates the performance of several representative large\nmodels in the field of literature recommendation and explores potential biases.\nThe results indicate that while some large models' recommendations can be\nsomewhat satisfactory after simple manual screening, overall, the accuracy of\nthese models in specific literature recommendation tasks is generally moderate.\nAdditionally, the models tend to recommend literature that is timely,\ncollaborative, and expands or deepens the field. In scholar recommendation\ntasks. There is no evidence to suggest that LLMs exacerbate inequalities\nrelated to gender, race, or the level of development of countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of several representative large\nmodels in the field of literature recommendation and explores potential biases.\nThe results indicate that while some large models' recommendations can be\nsomewhat satisfactory after simple manual screening, overall, the accuracy of\nthese models in specific literature recommendation tasks is generally moderate.\nAdditionally, the models tend to recommend literature that is timely,\ncollaborative, and expands or deepens the field. In scholar recommendation\ntasks. There is no evidence to suggest that LLMs exacerbate inequalities\nrelated to gender, race, or the level of development of countries."
                },
                "authors": [
                    {
                        "name": "Yifan Tian"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "There are some aspects in the original research that need to be\n  supplemented with robustness checks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01314v2",
                "updated": "2024-10-01T02:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    56,
                    30,
                    1,
                    275,
                    0
                ],
                "published": "2023-12-03T08:09:45Z",
                "published_parsed": [
                    2023,
                    12,
                    3,
                    8,
                    9,
                    45,
                    6,
                    337,
                    0
                ],
                "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian"
                },
                "summary": "Norwegian, spoken by only 5 million population, is under-representative\nwithin the most impressive breakthroughs in NLP tasks. To the best of our\nknowledge, there has not yet been a comprehensive evaluation of the existing\nlanguage models (LMs) on Norwegian generation tasks during the article writing\nprocess. To fill this gap, we 1) compiled the existing Norwegian dataset and\npre-trained 4 Norwegian Open Language Models varied from parameter scales and\narchitectures, collectively called NorGLM; 2) introduced a comprehensive\nbenchmark, NLEBench, for evaluating natural language generation capabilities in\nNorwegian, encompassing translation and human annotation. Based on the\ninvestigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5\nhas limited capability in understanding the Norwegian context; 2) the increase\nin model parameter scales demonstrates limited impact on the performance of\ndownstream tasks when the pre-training dataset is constrained in size; 3)\nsmaller models also demonstrate the reasoning capability through\nChain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be\nused to verify the generalizability of LLMs on natural language understanding\nand, meanwhile, test the interconnectedness of these NLP tasks. We share our\nresources and code for reproducibility under a CC BY-NC 4.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Norwegian, spoken by only 5 million population, is under-representative\nwithin the most impressive breakthroughs in NLP tasks. To the best of our\nknowledge, there has not yet been a comprehensive evaluation of the existing\nlanguage models (LMs) on Norwegian generation tasks during the article writing\nprocess. To fill this gap, we 1) compiled the existing Norwegian dataset and\npre-trained 4 Norwegian Open Language Models varied from parameter scales and\narchitectures, collectively called NorGLM; 2) introduced a comprehensive\nbenchmark, NLEBench, for evaluating natural language generation capabilities in\nNorwegian, encompassing translation and human annotation. Based on the\ninvestigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5\nhas limited capability in understanding the Norwegian context; 2) the increase\nin model parameter scales demonstrates limited impact on the performance of\ndownstream tasks when the pre-training dataset is constrained in size; 3)\nsmaller models also demonstrate the reasoning capability through\nChain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be\nused to verify the generalizability of LLMs on natural language understanding\nand, meanwhile, test the interconnectedness of these NLP tasks. We share our\nresources and code for reproducibility under a CC BY-NC 4.0 license."
                },
                "authors": [
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Terje Farup"
                    },
                    {
                        "name": "Even W. Lauvrak"
                    },
                    {
                        "name": "Jon Espen Ingvaldsen"
                    },
                    {
                        "name": "Simen Eide"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Zhirong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhirong Yang"
                },
                "author": "Zhirong Yang",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference. Code available at\n  https://github.com/Smartmedia-AI/NorGLM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15963v3",
                "updated": "2024-10-01T02:00:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    0,
                    50,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T10:48:13Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    48,
                    13,
                    1,
                    268,
                    0
                ],
                "title": "Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning"
                },
                "summary": "To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with guaranteed efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with guaranteed efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments."
                },
                "authors": [
                    {
                        "name": "Bo Yue"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Guiliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guiliang Liu"
                },
                "author": "Guiliang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03247v3",
                "updated": "2024-10-01T01:48:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    48,
                    58,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-06T15:07:08Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    15,
                    7,
                    8,
                    1,
                    219,
                    0
                ],
                "title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons"
                },
                "summary": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon."
                },
                "authors": [
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Wanting Wen"
                    },
                    {
                        "name": "Yu Sheng"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Daniel Dajun Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Dajun Zeng"
                },
                "author": "Daniel Dajun Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08978v2",
                "updated": "2024-10-01T01:40:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    40,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-16T19:01:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    19,
                    1,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses"
                },
                "summary": "The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation."
                },
                "authors": [
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Yinghao Yang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08369v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08369v4",
                "updated": "2024-10-01T01:24:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    24,
                    5,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-14T18:32:52Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    18,
                    32,
                    52,
                    1,
                    318,
                    0
                ],
                "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection"
                },
                "summary": "To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance."
                },
                "authors": [
                    {
                        "name": "Ryuto Koike"
                    },
                    {
                        "name": "Masahiro Kaneko"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "EMNLP 2024 Findings camera ready. Dataset available at\n  https://github.com/ryuryukke/HowYouPromptMatters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08369v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08369v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19162v2",
                "updated": "2024-10-01T01:14:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    14,
                    59,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-28T06:13:54Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    6,
                    13,
                    54,
                    3,
                    88,
                    0
                ],
                "title": "Fast and faithful interpolation of numerical relativity surrogate\n  waveforms using meshfree approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and faithful interpolation of numerical relativity surrogate\n  waveforms using meshfree approximation"
                },
                "summary": "Several theoretical waveform models have been developed over the years to\ncapture the gravitational wave emission from the dynamical evolution of compact\nbinary systems of neutron stars and black holes. As ground-based detectors\nimprove their sensitivity at low frequencies, the real-time computation of\nthese waveforms can become computationally expensive, exacerbating the steep\ncost of rapidly reconstructing source parameters using Bayesian methods. This\npaper describes an efficient numerical algorithm for generating high-fidelity\ninterpolated compact binary waveforms at an arbitrary point in the signal\nmanifold by leveraging computational linear algebra techniques such as singular\nvalue decomposition and meshfree approximation. The results are presented for\nthe time-domain \\texttt{NRHybSur3dq8} inspiral-merger-ringdown (IMR) waveform\nmodel that is fine tuned to numerical relativity simulations and parameterized\nby the two component-masses and two aligned spins. For demonstration, we target\na specific region of the intrinsic parameter space inspired by the previously\ninferred parameters of the \\texttt{GW200311\\_115853} event -- a binary black\nhole system whose merger was recorded by the network of advanced-LIGO and Virgo\ndetectors during the third observation run. We show that the meshfree\ninterpolated waveforms can be evaluated in $\\sim 2.3$ ms, which is about\n$\\times 38$ faster than its brute-force (frequency-domain tapered)\nimplementation in the \\textsc{PyCBC} software package at a median accuracy of\n$\\sim \\mathcal{O}(10^{-5})$. The algorithm is computationally efficient and\nscales favourably with an increasing number of dimensions of the parameter\nspace. This technique may find use in rapid parameter estimation and source\nreconstruction studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several theoretical waveform models have been developed over the years to\ncapture the gravitational wave emission from the dynamical evolution of compact\nbinary systems of neutron stars and black holes. As ground-based detectors\nimprove their sensitivity at low frequencies, the real-time computation of\nthese waveforms can become computationally expensive, exacerbating the steep\ncost of rapidly reconstructing source parameters using Bayesian methods. This\npaper describes an efficient numerical algorithm for generating high-fidelity\ninterpolated compact binary waveforms at an arbitrary point in the signal\nmanifold by leveraging computational linear algebra techniques such as singular\nvalue decomposition and meshfree approximation. The results are presented for\nthe time-domain \\texttt{NRHybSur3dq8} inspiral-merger-ringdown (IMR) waveform\nmodel that is fine tuned to numerical relativity simulations and parameterized\nby the two component-masses and two aligned spins. For demonstration, we target\na specific region of the intrinsic parameter space inspired by the previously\ninferred parameters of the \\texttt{GW200311\\_115853} event -- a binary black\nhole system whose merger was recorded by the network of advanced-LIGO and Virgo\ndetectors during the third observation run. We show that the meshfree\ninterpolated waveforms can be evaluated in $\\sim 2.3$ ms, which is about\n$\\times 38$ faster than its brute-force (frequency-domain tapered)\nimplementation in the \\textsc{PyCBC} software package at a median accuracy of\n$\\sim \\mathcal{O}(10^{-5})$. The algorithm is computationally efficient and\nscales favourably with an increasing number of dimensions of the parameter\nspace. This technique may find use in rapid parameter estimation and source\nreconstruction studies."
                },
                "authors": [
                    {
                        "name": "Lalit Pathak"
                    },
                    {
                        "name": "Amit Reza"
                    },
                    {
                        "name": "Anand S. Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Anand S. Sengupta"
                },
                "author": "Anand S. Sengupta",
                "arxiv_doi": "10.1103/PhysRevD.110.064022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.064022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.19162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures",
                "arxiv_journal_ref": "Phys. Rev. D 110, 064022 (2024), Vol. 110, Iss. 6",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01908v2",
                "updated": "2024-10-01T00:30:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    30,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-02-02T21:21:06Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    21,
                    21,
                    6,
                    4,
                    33,
                    0
                ],
                "title": "Large language models should not replace human participants because they\n  can misportray and flatten identity groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models should not replace human participants because they\n  can misportray and flatten identity groups"
                },
                "summary": "Large language models (LLMs) are increasing in capability and popularity,\npropelling their application in new domains -- including as replacements for\nhuman participants in computational social science, user testing, annotation\ntasks, and more. In many settings, researchers seek to distribute their surveys\nto a sample of participants that are representative of the underlying human\npopulation of interest. This means in order to be a suitable replacement, LLMs\nwill need to be able to capture the influence of positionality (i.e., relevance\nof social identities like gender and race). However, we show that there are two\ninherent limitations in the way current LLMs are trained that prevent this. We\nargue analytically for why LLMs are likely to both misportray and flatten the\nrepresentations of demographic groups, then empirically show this on 4 LLMs\nthrough a series of human studies with 3200 participants across 16 demographic\nidentities. We also discuss a third limitation about how identity prompts can\nessentialize identities. Throughout, we connect each limitation to a pernicious\nhistory that explains why it is harmful for marginalized demographic groups.\nOverall, we urge caution in use cases where LLMs are intended to replace human\nparticipants whose identities are relevant to the task at hand. At the same\ntime, in cases where the goal is to supplement rather than replace (e.g., pilot\nstudies), we provide inference-time techniques that we empirically demonstrate\ndo reduce, but do not remove, these harms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasing in capability and popularity,\npropelling their application in new domains -- including as replacements for\nhuman participants in computational social science, user testing, annotation\ntasks, and more. In many settings, researchers seek to distribute their surveys\nto a sample of participants that are representative of the underlying human\npopulation of interest. This means in order to be a suitable replacement, LLMs\nwill need to be able to capture the influence of positionality (i.e., relevance\nof social identities like gender and race). However, we show that there are two\ninherent limitations in the way current LLMs are trained that prevent this. We\nargue analytically for why LLMs are likely to both misportray and flatten the\nrepresentations of demographic groups, then empirically show this on 4 LLMs\nthrough a series of human studies with 3200 participants across 16 demographic\nidentities. We also discuss a third limitation about how identity prompts can\nessentialize identities. Throughout, we connect each limitation to a pernicious\nhistory that explains why it is harmful for marginalized demographic groups.\nOverall, we urge caution in use cases where LLMs are intended to replace human\nparticipants whose identities are relevant to the task at hand. At the same\ntime, in cases where the goal is to supplement rather than replace (e.g., pilot\nstudies), we provide inference-time techniques that we empirically demonstrate\ndo reduce, but do not remove, these harms."
                },
                "authors": [
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Jamie Morgenstern"
                    },
                    {
                        "name": "John P. Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P. Dickerson"
                },
                "author": "John P. Dickerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17633v2",
                "updated": "2024-10-01T00:17:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    17,
                    41,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-27T20:00:38Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    20,
                    0,
                    38,
                    0,
                    148,
                    0
                ],
                "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal\n  Stories with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal\n  Stories with LLMs"
                },
                "summary": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights."
                },
                "authors": [
                    {
                        "name": "Jocelyn Shen"
                    },
                    {
                        "name": "Joel Mire"
                    },
                    {
                        "name": "Hae Won Park"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17146v2",
                "updated": "2024-10-01T00:09:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    9,
                    49,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-25T19:44:06Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    19,
                    44,
                    6,
                    0,
                    85,
                    0
                ],
                "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-Constrained Large Language Models for Countering Hate Speech"
                },
                "summary": "Automatic counterspeech generation methods have been developed to assist\nefforts in combating hate speech. Existing research focuses on generating\ncounterspeech with linguistic attributes such as being polite, informative, and\nintent-driven. However, the real impact of counterspeech in online environments\nis seldom considered. This study aims to develop methods for generating\ncounterspeech constrained by conversation outcomes and evaluate their\neffectiveness. We experiment with large language models (LLMs) to incorporate\ninto the text generation process two desired conversation outcomes: low\nconversation incivility and non-hateful hater reentry. Specifically, we\nexperiment with instruction prompts, LLM finetuning, and LLM reinforcement\nlearning (RL). Evaluation results show that our methods effectively steer the\ngeneration of counterspeech toward the desired outcomes. Our analyses, however,\nshow that there are differences in the quality and style depending on the\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic counterspeech generation methods have been developed to assist\nefforts in combating hate speech. Existing research focuses on generating\ncounterspeech with linguistic attributes such as being polite, informative, and\nintent-driven. However, the real impact of counterspeech in online environments\nis seldom considered. This study aims to develop methods for generating\ncounterspeech constrained by conversation outcomes and evaluate their\neffectiveness. We experiment with large language models (LLMs) to incorporate\ninto the text generation process two desired conversation outcomes: low\nconversation incivility and non-hateful hater reentry. Specifically, we\nexperiment with instruction prompts, LLM finetuning, and LLM reinforcement\nlearning (RL). Evaluation results show that our methods effectively steer the\ngeneration of counterspeech toward the desired outcomes. Our analyses, however,\nshow that there are differences in the quality and style depending on the\nmodel."
                },
                "authors": [
                    {
                        "name": "Lingzi Hong"
                    },
                    {
                        "name": "Pengcheng Luo"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Xiaoying Song"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoying Song"
                },
                "author": "Xiaoying Song",
                "arxiv_comment": "Accepted for presentation at the EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17544v2",
                "updated": "2024-09-30T23:31:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    23,
                    31,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-26T05:22:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    5,
                    22,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings"
                },
                "summary": "Theoretical and empirical evidence suggests that joint graph embedding\nalgorithms induce correlation across the networks in the embedding space. In\nthe Omnibus joint graph embedding framework, previous results explicitly\ndelineated the dual effects of the algorithm-induced and model-inherent\ncorrelations on the correlation across the embedded networks. Accounting for\nand mitigating the algorithm-induced correlation is key to subsequent\ninference, as sub-optimal Omnibus matrix constructions have been demonstrated\nto lead to loss in inference fidelity. This work presents the first efforts to\nautomate the Omnibus construction in order to address two key questions in this\njoint embedding framework: the correlation-to-OMNI problem and the flat\ncorrelation problem. In the flat correlation problem, we seek to understand the\nminimum algorithm-induced flat correlation (i.e., the same across all graph\npairs) produced by a generalized Omnibus embedding. Working in a subspace of\nthe fully general Omnibus matrices, we prove both a lower bound for this flat\ncorrelation and that the classical Omnibus construction induces the maximal\nflat correlation. In the correlation-to-OMNI problem, we present an algorithm\n-- named corr2Omni -- that, from a given matrix of estimated pairwise graph\ncorrelations, estimates the matrix of generalized Omnibus weights that induces\noptimal correlation in the embedding space. Moreover, in both simulated and\nreal data settings, we demonstrate the increased effectiveness of our corr2Omni\nalgorithm versus the classical Omnibus construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical and empirical evidence suggests that joint graph embedding\nalgorithms induce correlation across the networks in the embedding space. In\nthe Omnibus joint graph embedding framework, previous results explicitly\ndelineated the dual effects of the algorithm-induced and model-inherent\ncorrelations on the correlation across the embedded networks. Accounting for\nand mitigating the algorithm-induced correlation is key to subsequent\ninference, as sub-optimal Omnibus matrix constructions have been demonstrated\nto lead to loss in inference fidelity. This work presents the first efforts to\nautomate the Omnibus construction in order to address two key questions in this\njoint embedding framework: the correlation-to-OMNI problem and the flat\ncorrelation problem. In the flat correlation problem, we seek to understand the\nminimum algorithm-induced flat correlation (i.e., the same across all graph\npairs) produced by a generalized Omnibus embedding. Working in a subspace of\nthe fully general Omnibus matrices, we prove both a lower bound for this flat\ncorrelation and that the classical Omnibus construction induces the maximal\nflat correlation. In the correlation-to-OMNI problem, we present an algorithm\n-- named corr2Omni -- that, from a given matrix of estimated pairwise graph\ncorrelations, estimates the matrix of generalized Omnibus weights that induces\noptimal correlation in the embedding space. Moreover, in both simulated and\nreal data settings, we demonstrate the increased effectiveness of our corr2Omni\nalgorithm versus the classical Omnibus construction."
                },
                "authors": [
                    {
                        "name": "Konstantinos Pantazis"
                    },
                    {
                        "name": "Michael Trosset"
                    },
                    {
                        "name": "William N. Frost"
                    },
                    {
                        "name": "Carey E. Priebe"
                    },
                    {
                        "name": "Vince Lyzinski"
                    }
                ],
                "author_detail": {
                    "name": "Vince Lyzinski"
                },
                "author": "Vince Lyzinski",
                "arxiv_comment": "34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17648v2",
                "updated": "2024-09-30T22:52:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    52,
                    18,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-26T08:55:21Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "title": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited."
                },
                "authors": [
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Phat Vo"
                    },
                    {
                        "name": "Arman Kizilkale"
                    },
                    {
                        "name": "Aaron Reite"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reite"
                },
                "author": "Aaron Reite",
                "arxiv_comment": "6 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05282v3",
                "updated": "2024-09-30T21:55:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    55,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-07T22:27:29Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    22,
                    27,
                    29,
                    4,
                    159,
                    0
                ],
                "title": "Look-Up Table based Neural Network Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look-Up Table based Neural Network Hardware"
                },
                "summary": "Traditional digital implementations of neural accelerators are limited by\nhigh power and area overheads, while analog and non-CMOS implementations suffer\nfrom noise, device mismatch, and reliability issues. This paper introduces a\nCMOS Look-Up Table (LUT)-based Neural Accelerator (LUT-NA) framework that\nreduces the power, latency, and area consumption of traditional digital\naccelerators through pre-computed, faster look-ups while avoiding noise and\nmismatch of analog circuits. To solve the scalability issues of conventional\nLUT-based computation, we split the high-precision multiply and accumulate\n(MAC) operations into lower-precision MACs using a divide-and-conquer-based\napproach. We show that LUT-NA achieves up to $29.54\\times$ lower area with\n$3.34\\times$ lower energy per inference task than traditional LUT-based\ntechniques and up to $1.23\\times$ lower area with $1.80\\times$ lower energy per\ninference task than conventional digital MAC-based techniques (Wallace\nTree/Array Multipliers) without retraining and without affecting accuracy, even\non lottery ticket pruned (LTP) models that already reduce the number of\nrequired MAC operations by up to 98%. Finally, we introduce mixed precision\nanalysis in LUT-NA framework for various LTP models (VGG11, VGG19, Resnet18,\nResnet34, GoogleNet) that achieved up to $32.22\\times$-$50.95\\times$ lower area\nacross models with $3.68\\times$-$6.25\\times$ lower energy per inference than\ntraditional LUT-based techniques, and up to $1.35\\times$-$2.14\\times$ lower\narea requirement with $1.99\\times$-$3.38\\times$ lower energy per inference\nacross models as compared to conventional digital MAC-based techniques with\n$\\sim$1% accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional digital implementations of neural accelerators are limited by\nhigh power and area overheads, while analog and non-CMOS implementations suffer\nfrom noise, device mismatch, and reliability issues. This paper introduces a\nCMOS Look-Up Table (LUT)-based Neural Accelerator (LUT-NA) framework that\nreduces the power, latency, and area consumption of traditional digital\naccelerators through pre-computed, faster look-ups while avoiding noise and\nmismatch of analog circuits. To solve the scalability issues of conventional\nLUT-based computation, we split the high-precision multiply and accumulate\n(MAC) operations into lower-precision MACs using a divide-and-conquer-based\napproach. We show that LUT-NA achieves up to $29.54\\times$ lower area with\n$3.34\\times$ lower energy per inference task than traditional LUT-based\ntechniques and up to $1.23\\times$ lower area with $1.80\\times$ lower energy per\ninference task than conventional digital MAC-based techniques (Wallace\nTree/Array Multipliers) without retraining and without affecting accuracy, even\non lottery ticket pruned (LTP) models that already reduce the number of\nrequired MAC operations by up to 98%. Finally, we introduce mixed precision\nanalysis in LUT-NA framework for various LTP models (VGG11, VGG19, Resnet18,\nResnet34, GoogleNet) that achieved up to $32.22\\times$-$50.95\\times$ lower area\nacross models with $3.68\\times$-$6.25\\times$ lower energy per inference than\ntraditional LUT-based techniques, and up to $1.35\\times$-$2.14\\times$ lower\narea requirement with $1.99\\times$-$3.38\\times$ lower energy per inference\nacross models as compared to conventional digital MAC-based techniques with\n$\\sim$1% accuracy loss."
                },
                "authors": [
                    {
                        "name": "Ovishake Sen"
                    },
                    {
                        "name": "Chukwufumnanya Ogbogu"
                    },
                    {
                        "name": "Peyman Dehghanzadeh"
                    },
                    {
                        "name": "Janardhan Rao Doppa"
                    },
                    {
                        "name": "Swarup Bhunia"
                    },
                    {
                        "name": "Partha Pratim Pande"
                    },
                    {
                        "name": "Baibhab Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Baibhab Chatterjee"
                },
                "author": "Baibhab Chatterjee",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03616v3",
                "updated": "2024-09-30T21:26:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    26,
                    56,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-04T03:59:52Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    3,
                    59,
                    52,
                    3,
                    186,
                    0
                ],
                "title": "When can weak latent factors be statistically inferred?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When can weak latent factors be statistically inferred?"
                },
                "summary": "This article establishes a new and comprehensive estimation and inference\ntheory for principal component analysis (PCA) under the weak factor model that\nallow for cross-sectional dependent idiosyncratic components under the nearly\nminimal factor strength relative to the noise level or signal-to-noise ratio.\nOur theory is applicable regardless of the relative growth rate between the\ncross-sectional dimension $N$ and temporal dimension $T$. This more realistic\nassumption and noticeable result require completely new technical device, as\nthe commonly-used leave-one-out trick is no longer applicable to the case with\ncross-sectional dependence. Another notable advancement of our theory is on PCA\ninference $ - $ for example, under the regime where $N\\asymp T$, we show that\nthe asymptotic normality for the PCA-based estimator holds as long as the\nsignal-to-noise ratio (SNR) grows faster than a polynomial rate of $\\log N$.\nThis finding significantly surpasses prior work that required a polynomial rate\nof $N$. Our theory is entirely non-asymptotic, offering finite-sample\ncharacterizations for both the estimation error and the uncertainty level of\nstatistical inference. A notable technical innovation is our closed-form\nfirst-order approximation of PCA-based estimator, which paves the way for\nvarious statistical tests. Furthermore, we apply our theories to design\neasy-to-implement statistics for validating whether given factors fall in the\nlinear spans of unknown latent factors, testing structural breaks in the factor\nloadings for an individual unit, checking whether two units have the same risk\nexposures, and constructing confidence intervals for systematic risks. Our\nempirical studies uncover insightful correlations between our test results and\neconomic cycles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article establishes a new and comprehensive estimation and inference\ntheory for principal component analysis (PCA) under the weak factor model that\nallow for cross-sectional dependent idiosyncratic components under the nearly\nminimal factor strength relative to the noise level or signal-to-noise ratio.\nOur theory is applicable regardless of the relative growth rate between the\ncross-sectional dimension $N$ and temporal dimension $T$. This more realistic\nassumption and noticeable result require completely new technical device, as\nthe commonly-used leave-one-out trick is no longer applicable to the case with\ncross-sectional dependence. Another notable advancement of our theory is on PCA\ninference $ - $ for example, under the regime where $N\\asymp T$, we show that\nthe asymptotic normality for the PCA-based estimator holds as long as the\nsignal-to-noise ratio (SNR) grows faster than a polynomial rate of $\\log N$.\nThis finding significantly surpasses prior work that required a polynomial rate\nof $N$. Our theory is entirely non-asymptotic, offering finite-sample\ncharacterizations for both the estimation error and the uncertainty level of\nstatistical inference. A notable technical innovation is our closed-form\nfirst-order approximation of PCA-based estimator, which paves the way for\nvarious statistical tests. Furthermore, we apply our theories to design\neasy-to-implement statistics for validating whether given factors fall in the\nlinear spans of unknown latent factors, testing structural breaks in the factor\nloadings for an individual unit, checking whether two units have the same risk\nexposures, and constructing confidence intervals for systematic risks. Our\nempirical studies uncover insightful correlations between our test results and\neconomic cycles."
                },
                "authors": [
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Yuling Yan"
                    },
                    {
                        "name": "Yuheng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Zheng"
                },
                "author": "Yuheng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17336v2",
                "updated": "2024-09-30T21:25:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    25,
                    23,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-26T02:47:42Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    2,
                    47,
                    42,
                    1,
                    86,
                    0
                ],
                "title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models"
                },
                "summary": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Shunning Liang"
                    },
                    {
                        "name": "Zach Cameron"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Ning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Zhang"
                },
                "author": "Ning Zhang",
                "arxiv_comment": "Accepted by USENIX Security 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14732v2",
                "updated": "2024-09-30T21:25:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    25,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-20T20:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    20,
                    55,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text\n  Question Answering with Reasoning and Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text\n  Question Answering with Reasoning and Summarization"
                },
                "summary": "Question answering (QA) over tables and text has gained much popularity over\nthe years. Multi-hop table-text QA requires multiple hops between the table and\ntext, making it a challenging QA task. Although several works have attempted to\nsolve the table-text QA task, most involve training the models and requiring\nlabeled data. In this paper, we have proposed a Retrieval Augmented Generation\n(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop\nTable-Text Question Answering with Reasoning and Summarization. Our model uses\nan enhanced retriever for table-text information retrieval and uses augmented\nknowledge, including table-text summary with decomposed sub-questions with\nanswers for a reasoning-based table-text QA. Using open-source language models,\nour model outperformed all existing prompting methods for table-text QA tasks\non existing table-text QA datasets, such as HybridQA and OTT-QA's development\nset. Our experiments demonstrate the potential of prompt-based approaches using\nopen-source LLMs. Additionally, by using LLaMA3-70B, our model achieved\nstate-of-the-art performance for prompting-based methods on multi-hop\ntable-text QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) over tables and text has gained much popularity over\nthe years. Multi-hop table-text QA requires multiple hops between the table and\ntext, making it a challenging QA task. Although several works have attempted to\nsolve the table-text QA task, most involve training the models and requiring\nlabeled data. In this paper, we have proposed a Retrieval Augmented Generation\n(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop\nTable-Text Question Answering with Reasoning and Summarization. Our model uses\nan enhanced retriever for table-text information retrieval and uses augmented\nknowledge, including table-text summary with decomposed sub-questions with\nanswers for a reasoning-based table-text QA. Using open-source language models,\nour model outperformed all existing prompting methods for table-text QA tasks\non existing table-text QA datasets, such as HybridQA and OTT-QA's development\nset. Our experiments demonstrate the potential of prompt-based approaches using\nopen-source LLMs. Additionally, by using LLaMA3-70B, our model achieved\nstate-of-the-art performance for prompting-based methods on multi-hop\ntable-text QA."
                },
                "authors": [
                    {
                        "name": "Jayetri Bardhan"
                    },
                    {
                        "name": "Bushi Xiao"
                    },
                    {
                        "name": "Daisy Zhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Daisy Zhe Wang"
                },
                "author": "Daisy Zhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v4",
                "updated": "2024-09-30T20:57:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    57,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate twelve LLMs from six model\nfamilies, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$, followed by\nLLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental\nresults are publicly available via https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate twelve LLMs from six model\nfamilies, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$, followed by\nLLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental\nresults are publicly available via https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "11 pages of main text; 19 pages of appendices. Included models:\n  GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7,\n  70, 405}B, Mixtral-8x{7, 22}B, Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v4",
                "updated": "2024-09-30T20:42:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    42,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.01397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.01397v5",
                "updated": "2024-09-30T20:29:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    29,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2023-04-03T22:16:52Z",
                "published_parsed": [
                    2023,
                    4,
                    3,
                    22,
                    16,
                    52,
                    0,
                    93,
                    0
                ],
                "title": "LTM: Scalable and Black-box Similarity-based Test Suite Minimization\n  based on Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTM: Scalable and Black-box Similarity-based Test Suite Minimization\n  based on Language Models"
                },
                "summary": "Test suites tend to grow when software evolves, making it often infeasible to\nexecute all test cases with the allocated testing budgets, especially for large\nsoftware systems. Test suite minimization (TSM) is employed to improve the\nefficiency of software testing by removing redundant test cases, thus reducing\ntesting time and resources, while maintaining the fault detection capability of\nthe test suite. Most existing TSM approaches rely on code coverage (white-box)\nor model-based features, which are not always available to test engineers.\nRecent TSM approaches that rely only on test code (black-box) have been\nproposed, such as ATM and FAST-R. To address the scalability, we propose LTM\n(Language model-based Test suite Minimization), a novel, scalable, and\nblack-box similarity-based TSM approach based on large language models (LLMs),\nwhich is the first application of LLMs in the context of TSM. To support\nsimilarity measurement for test code embeddings, we investigate five\npre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder,\nand CodeLlama, on which we compute two similarity measures: Cosine Similarity\nand Euclidean Distance. Our goal is to find similarity measures that are not\nonly computationally more efficient but can also better guide a Genetic\nAlgorithm (GA) to search for optimal minimized test suites, thus reducing the\noverall search time. Experimental results show that the best configuration of\nLTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a\nslightly greater saving rate of testing time (41.72% versus 41.02%, on\naverage); (b) attaining a significantly higher fault detection rate (0.84\nversus 0.81, on average); and, most importantly, (c) minimizing test suites\nnearly five times faster on average, with higher gains for larger test suites\nand systems, thus achieving much higher scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test suites tend to grow when software evolves, making it often infeasible to\nexecute all test cases with the allocated testing budgets, especially for large\nsoftware systems. Test suite minimization (TSM) is employed to improve the\nefficiency of software testing by removing redundant test cases, thus reducing\ntesting time and resources, while maintaining the fault detection capability of\nthe test suite. Most existing TSM approaches rely on code coverage (white-box)\nor model-based features, which are not always available to test engineers.\nRecent TSM approaches that rely only on test code (black-box) have been\nproposed, such as ATM and FAST-R. To address the scalability, we propose LTM\n(Language model-based Test suite Minimization), a novel, scalable, and\nblack-box similarity-based TSM approach based on large language models (LLMs),\nwhich is the first application of LLMs in the context of TSM. To support\nsimilarity measurement for test code embeddings, we investigate five\npre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder,\nand CodeLlama, on which we compute two similarity measures: Cosine Similarity\nand Euclidean Distance. Our goal is to find similarity measures that are not\nonly computationally more efficient but can also better guide a Genetic\nAlgorithm (GA) to search for optimal minimized test suites, thus reducing the\noverall search time. Experimental results show that the best configuration of\nLTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a\nslightly greater saving rate of testing time (41.72% versus 41.02%, on\naverage); (b) attaining a significantly higher fault detection rate (0.84\nversus 0.81, on average); and, most importantly, (c) minimizing test suites\nnearly five times faster on average, with higher gains for larger test suites\nand systems, thus achieving much higher scalability."
                },
                "authors": [
                    {
                        "name": "Rongqi Pan"
                    },
                    {
                        "name": "Taher A. Ghaleb"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1109/TSE.2024.3469582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TSE.2024.3469582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.01397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.01397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15339v2",
                "updated": "2024-09-30T20:18:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    18,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-27T18:04:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    4,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function"
                },
                "summary": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Bin Bi"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Shiva Kumar Pentyala"
                    },
                    {
                        "name": "Zixu James Zhu"
                    },
                    {
                        "name": "Sitaram Asur"
                    },
                    {
                        "name": "Na Claire Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Na Claire Cheng"
                },
                "author": "Na Claire Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07151v2",
                "updated": "2024-09-30T20:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    16,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2023-12-12T10:41:04Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    10,
                    41,
                    4,
                    1,
                    346,
                    0
                ],
                "title": "The Gaussian-Linear Hidden Markov model: a Python package",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gaussian-Linear Hidden Markov model: a Python package"
                },
                "summary": "We propose the Gaussian-Linear Hidden Markov model (GLHMM), a generalisation\nof different types of HMMs commonly used in neuroscience. In short, the GLHMM\nis a general framework where linear regression is used to flexibly parameterise\nthe Gaussian state distribution, thereby accommodating a wide range of uses --\nincluding unsupervised, encoding and decoding models. GLHMM is implemented as a\nPython toolbox with an emphasis on statistical testing and out-of-sample\nprediction -- i.e. aimed at finding and characterising brain-behaviour\nassociations. The toolbox uses a stochastic variational inference approach,\nenabling it to handle large data sets at reasonable computational time. The\napproach can be applied to several data modalities, including animal recordings\nor non-brain data, and applied over a broad range of experimental paradigms.\nFor demonstration, we show examples with fMRI, electrocorticography,\nmagnetoencephalography and pupillometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose the Gaussian-Linear Hidden Markov model (GLHMM), a generalisation\nof different types of HMMs commonly used in neuroscience. In short, the GLHMM\nis a general framework where linear regression is used to flexibly parameterise\nthe Gaussian state distribution, thereby accommodating a wide range of uses --\nincluding unsupervised, encoding and decoding models. GLHMM is implemented as a\nPython toolbox with an emphasis on statistical testing and out-of-sample\nprediction -- i.e. aimed at finding and characterising brain-behaviour\nassociations. The toolbox uses a stochastic variational inference approach,\nenabling it to handle large data sets at reasonable computational time. The\napproach can be applied to several data modalities, including animal recordings\nor non-brain data, and applied over a broad range of experimental paradigms.\nFor demonstration, we show examples with fMRI, electrocorticography,\nmagnetoencephalography and pupillometry."
                },
                "authors": [
                    {
                        "name": "Diego Vidaurre"
                    },
                    {
                        "name": "Laura Masaracchia"
                    },
                    {
                        "name": "Nick Y. Larsen"
                    },
                    {
                        "name": "Lenno R. P. T Ruijters"
                    },
                    {
                        "name": "Sonsoles Alonso"
                    },
                    {
                        "name": "Christine Ahrends"
                    },
                    {
                        "name": "Mark W. Woolrich"
                    }
                ],
                "author_detail": {
                    "name": "Mark W. Woolrich"
                },
                "author": "Mark W. Woolrich",
                "arxiv_comment": "24 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21054v2",
                "updated": "2024-09-30T19:51:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    51,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-24T12:07:54Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    7,
                    54,
                    2,
                    206,
                    0
                ],
                "title": "Sentiment Reasoning for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Reasoning for Healthcare"
                },
                "summary": "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed"
                },
                "authors": [
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Bach Phan Tat"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Jerry Ngo"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Anh Totti Nguyen"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "Preprint, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.12618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12618v2",
                "updated": "2024-10-01T17:50:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    50,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-19T09:44:17Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    9,
                    44,
                    17,
                    3,
                    263,
                    0
                ],
                "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning"
                },
                "summary": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention."
                },
                "authors": [
                    {
                        "name": "Santosh Kumar Radha"
                    },
                    {
                        "name": "Yasamin Nouri Jelyani"
                    },
                    {
                        "name": "Ara Ghukasyan"
                    },
                    {
                        "name": "Oktay Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Oktay Goktas"
                },
                "author": "Oktay Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18924v2",
                "updated": "2024-10-01T17:49:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    49,
                    0,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T17:17:15Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow"
                },
                "summary": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration."
                },
                "authors": [
                    {
                        "name": "Huizi Yu"
                    },
                    {
                        "name": "Jiayan Zhou"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Anye Shi"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Guang Chen"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Trisha Gupte"
                    },
                    {
                        "name": "Ming-Li Chen"
                    },
                    {
                        "name": "Zahra Azizi"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Themistocles L. Assimes"
                    },
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Lizhou Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lizhou Fan"
                },
                "author": "Lizhou Fan",
                "arxiv_comment": "42 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01247v2",
                "updated": "2024-10-01T17:21:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    21,
                    28,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-02T13:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    29,
                    44,
                    0,
                    246,
                    0
                ],
                "title": "Conversational Complexity for Assessing Risk in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Complexity for Assessing Risk in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm."
                },
                "authors": [
                    {
                        "name": "John Burden"
                    },
                    {
                        "name": "Manuel Cebrian"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jose Hernandez-Orallo"
                },
                "author": "Jose Hernandez-Orallo",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12857v2",
                "updated": "2024-10-01T17:13:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    13,
                    38,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-09T15:06:14Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    15,
                    6,
                    14,
                    1,
                    191,
                    0
                ],
                "title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis"
                },
                "summary": "In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers."
                },
                "authors": [
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Jiaqi Tan"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Zhenmin Weng"
                    },
                    {
                        "name": "Chenghua Gong"
                    },
                    {
                        "name": "Long Zeng"
                    },
                    {
                        "name": "Renjing Cui"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09111v2",
                "updated": "2024-10-01T17:10:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    10,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-12T09:24:34Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    9,
                    24,
                    34,
                    4,
                    194,
                    0
                ],
                "title": "Inference Optimization of Foundation Models on AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimization of Foundation Models on AI Accelerators"
                },
                "summary": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context."
                },
                "authors": [
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Kailash Budhathoki"
                    },
                    {
                        "name": "Liangfu Chen"
                    },
                    {
                        "name": "Jonas Kübler"
                    },
                    {
                        "name": "Jiaji Huang"
                    },
                    {
                        "name": "Matthäus Kleindessner"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "George Karypis"
                    }
                ],
                "author_detail": {
                    "name": "George Karypis"
                },
                "author": "George Karypis",
                "arxiv_comment": "[v2] Tutorial website added [v1] Tutorial published at KDD 2024.\n  Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17328v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17328v3",
                "updated": "2024-10-01T16:45:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    45,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-25T07:25:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    7,
                    25,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "Dual-Space Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Space Knowledge Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Zengkui Sun"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "The camera-ready version for EMNLP 2024 main conference. 17 pages, 11\n  figures, code available at: https://github.com/songmzhang/DSKD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17328v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17328v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20252v2",
                "updated": "2024-10-01T16:34:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    34,
                    13,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T12:42:25Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    42,
                    25,
                    0,
                    274,
                    0
                ],
                "title": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?"
                },
                "summary": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Morgan Fouesneau"
                    },
                    {
                        "name": "Ivelina G. Momcheva"
                    },
                    {
                        "name": "Urmila Chadayammuri"
                    },
                    {
                        "name": "Mariia Demianenko"
                    },
                    {
                        "name": "Antoine Dumont"
                    },
                    {
                        "name": "Raphael E. Hviding"
                    },
                    {
                        "name": "K. Angelique Kahle"
                    },
                    {
                        "name": "Nadiia Pulatova"
                    },
                    {
                        "name": "Bhavesh Rajpoot"
                    },
                    {
                        "name": "Marten B. Scheuck"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Dmitry Semenov"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    }
                ],
                "author_detail": {
                    "name": "Jaime I. Villaseñor"
                },
                "author": "Jaime I. Villaseñor",
                "arxiv_comment": "Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04530v3",
                "updated": "2024-10-01T16:12:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    16,
                    12,
                    51,
                    1,
                    275,
                    0
                ],
                "published": "2023-12-07T18:50:01Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    18,
                    50,
                    1,
                    3,
                    341,
                    0
                ],
                "title": "Camera Height Doesn't Change: Unsupervised Training for Metric Monocular\n  Road-Scene Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera Height Doesn't Change: Unsupervised Training for Metric Monocular\n  Road-Scene Depth Estimation"
                },
                "summary": "In this paper, we introduce a novel training method for making any monocular\ndepth network learn absolute scale and estimate metric road-scene depth just\nfrom regular training data, i.e., driving videos. We refer to this training\nframework as FUMET. The key idea is to leverage cars found on the road as\nsources of scale supervision and to incorporate them in network training\nrobustly. FUMET detects and estimates the sizes of cars in a frame and\naggregates scale information extracted from them into an estimate of the camera\nheight whose consistency across the entire video sequence is enforced as scale\nsupervision. This realizes robust unsupervised training of any, otherwise\nscale-oblivious, monocular depth network so that they become not only\nscale-aware but also metric-accurate without the need for auxiliary sensors and\nextra supervision. Extensive experiments on the KITTI and the Cityscapes\ndatasets show the effectiveness of FUMET, which achieves state-of-the-art\naccuracy. We also show that FUMET enables training on mixed datasets of\ndifferent camera heights, which leads to larger-scale training and better\ngeneralization. Metric depth reconstruction is essential in any road-scene\nvisual modeling, and FUMET democratizes its deployment by establishing the\nmeans to convert any model into a metric depth estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel training method for making any monocular\ndepth network learn absolute scale and estimate metric road-scene depth just\nfrom regular training data, i.e., driving videos. We refer to this training\nframework as FUMET. The key idea is to leverage cars found on the road as\nsources of scale supervision and to incorporate them in network training\nrobustly. FUMET detects and estimates the sizes of cars in a frame and\naggregates scale information extracted from them into an estimate of the camera\nheight whose consistency across the entire video sequence is enforced as scale\nsupervision. This realizes robust unsupervised training of any, otherwise\nscale-oblivious, monocular depth network so that they become not only\nscale-aware but also metric-accurate without the need for auxiliary sensors and\nextra supervision. Extensive experiments on the KITTI and the Cityscapes\ndatasets show the effectiveness of FUMET, which achieves state-of-the-art\naccuracy. We also show that FUMET enables training on mixed datasets of\ndifferent camera heights, which leads to larger-scale training and better\ngeneralization. Metric depth reconstruction is essential in any road-scene\nvisual modeling, and FUMET democratizes its deployment by establishing the\nmeans to convert any model into a metric depth estimator."
                },
                "authors": [
                    {
                        "name": "Genki Kinoshita"
                    },
                    {
                        "name": "Ko Nishino"
                    }
                ],
                "author_detail": {
                    "name": "Ko Nishino"
                },
                "author": "Ko Nishino",
                "arxiv_comment": "ECCV 2024. Project page:\n  https://vision.ist.i.kyoto-u.ac.jp/research/fumet/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09662v2",
                "updated": "2024-10-01T15:52:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    52,
                    15,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-15T19:04:30Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    19,
                    4,
                    30,
                    2,
                    136,
                    0
                ],
                "title": "Large-Scale Security Analysis of Real-World Backend Deployments Speaking\n  IoT-Focused Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-Scale Security Analysis of Real-World Backend Deployments Speaking\n  IoT-Focused Protocols"
                },
                "summary": "Internet-of-Things (IoT) devices, ranging from smart home assistants to\nhealth devices, are pervasive: Forecasts estimate their number to reach 29\nbillion by 2030. Understanding the security of their machine-to-machine\ncommunication is crucial. Prior work focused on identifying devices'\nvulnerabilities or proposed protocol-specific solutions. Instead, we\ninvestigate the security of backends speaking IoT protocols, that is, the\nbackbone of the IoT ecosystem.\n  We focus on three real-world protocols for our large-scale analysis: MQTT,\nCoAP, and XMPP. We gather a dataset of over 337,000 backends, augment it with\ngeographical and provider data, and perform non-invasive active measurements to\ninvestigate three major security threats: information leakage, weak\nauthentication, and denial of service. Our results provide quantitative\nevidence of a problematic immaturity in the IoT ecosystem. Among other issues,\nwe find that 9.44% backends expose information, 30.38% CoAP-speaking backends\nare vulnerable to denial of service attacks, and 99.84% of MQTT- and\nXMPP-speaking backends use insecure transport protocols (only 0.16% adopt TLS,\nof which 70.93% adopt a vulnerable version).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet-of-Things (IoT) devices, ranging from smart home assistants to\nhealth devices, are pervasive: Forecasts estimate their number to reach 29\nbillion by 2030. Understanding the security of their machine-to-machine\ncommunication is crucial. Prior work focused on identifying devices'\nvulnerabilities or proposed protocol-specific solutions. Instead, we\ninvestigate the security of backends speaking IoT protocols, that is, the\nbackbone of the IoT ecosystem.\n  We focus on three real-world protocols for our large-scale analysis: MQTT,\nCoAP, and XMPP. We gather a dataset of over 337,000 backends, augment it with\ngeographical and provider data, and perform non-invasive active measurements to\ninvestigate three major security threats: information leakage, weak\nauthentication, and denial of service. Our results provide quantitative\nevidence of a problematic immaturity in the IoT ecosystem. Among other issues,\nwe find that 9.44% backends expose information, 30.38% CoAP-speaking backends\nare vulnerable to denial of service attacks, and 99.84% of MQTT- and\nXMPP-speaking backends use insecure transport protocols (only 0.16% adopt TLS,\nof which 70.93% adopt a vulnerable version)."
                },
                "authors": [
                    {
                        "name": "Carlotta Tagliaro"
                    },
                    {
                        "name": "Martina Komsic"
                    },
                    {
                        "name": "Andrea Continella"
                    },
                    {
                        "name": "Kevin Borgolte"
                    },
                    {
                        "name": "Martina Lindorfer"
                    }
                ],
                "author_detail": {
                    "name": "Martina Lindorfer"
                },
                "author": "Martina Lindorfer",
                "arxiv_doi": "10.1145/3678890.3678899",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678899",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appeared at the 27th International Symposium on Research in Attacks,\n  Intrusions and Defenses (RAID 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06917v2",
                "updated": "2024-10-01T15:50:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    50,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-09T14:52:52Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    52,
                    52,
                    1,
                    191,
                    0
                ],
                "title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13214v2",
                "updated": "2024-10-01T15:48:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    48,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2023-05-22T16:45:50Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    16,
                    45,
                    50,
                    0,
                    142,
                    0
                ],
                "title": "Atomic Inference for NLI with Generated Facts as Atoms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic Inference for NLI with Generated Facts as Atoms"
                },
                "summary": "With recent advances, neural models can achieve human-level performance on\nvarious natural language tasks. However, there are no guarantees that any\nexplanations from these models are faithful, i.e. that they reflect the inner\nworkings of the model. Atomic inference overcomes this issue, providing\ninterpretable and faithful model decisions. This approach involves making\npredictions for different components (or atoms) of an instance, before using\ninterpretable and deterministic rules to derive the overall prediction based on\nthe individual atom-level predictions. We investigate the effectiveness of\nusing LLM-generated facts as atoms, decomposing Natural Language Inference\npremises into lists of facts. While directly using generated facts in atomic\ninference systems can result in worse performance, with 1) a multi-stage fact\ngeneration process, and 2) a training regime that incorporates the facts, our\nfact-based method outperforms other approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent advances, neural models can achieve human-level performance on\nvarious natural language tasks. However, there are no guarantees that any\nexplanations from these models are faithful, i.e. that they reflect the inner\nworkings of the model. Atomic inference overcomes this issue, providing\ninterpretable and faithful model decisions. This approach involves making\npredictions for different components (or atoms) of an instance, before using\ninterpretable and deterministic rules to derive the overall prediction based on\nthe individual atom-level predictions. We investigate the effectiveness of\nusing LLM-generated facts as atoms, decomposing Natural Language Inference\npremises into lists of facts. While directly using generated facts in atomic\ninference systems can result in worse performance, with 1) a multi-stage fact\ngeneration process, and 2) a training regime that incorporates the facts, our\nfact-based method outperforms other approaches."
                },
                "authors": [
                    {
                        "name": "Joe Stacey"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Haim Dubossarsky"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03354v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03354v3",
                "updated": "2024-10-01T15:41:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    41,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-06T09:15:25Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    9,
                    15,
                    25,
                    1,
                    219,
                    0
                ],
                "title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums"
                },
                "summary": "Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence."
                },
                "authors": [
                    {
                        "name": "Vanessa Clairoux-Trepanier"
                    },
                    {
                        "name": "Isa-May Beauchamp"
                    },
                    {
                        "name": "Estelle Ruellan"
                    },
                    {
                        "name": "Masarah Paquet-Clouston"
                    },
                    {
                        "name": "Serge-Olivier Paquette"
                    },
                    {
                        "name": "Eric Clay"
                    }
                ],
                "author_detail": {
                    "name": "Eric Clay"
                },
                "author": "Eric Clay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03354v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03354v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10774v2",
                "updated": "2024-10-01T15:39:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    39,
                    48,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-16T17:59:10Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    59,
                    10,
                    1,
                    107,
                    0
                ],
                "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents"
                },
                "summary": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models."
                },
                "authors": [
                    {
                        "name": "Liyan Tang"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19346v3",
                "updated": "2024-10-01T15:28:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    28,
                    16,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-28T12:04:28Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    12,
                    4,
                    28,
                    3,
                    88,
                    0
                ],
                "title": "Large Language Models Are Unconscious of Unreasonability in Math\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Unconscious of Unreasonability in Math\n  Problems"
                },
                "summary": "Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12701v2",
                "updated": "2024-10-01T15:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    3,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-21T11:50:16Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    11,
                    50,
                    16,
                    1,
                    142,
                    0
                ],
                "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering"
                },
                "summary": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available."
                },
                "authors": [
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04908v3",
                "updated": "2024-10-01T14:22:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    14,
                    22,
                    15,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-07T21:34:40Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    21,
                    34,
                    40,
                    3,
                    67,
                    0
                ],
                "title": "Self-Adapting Large Visual-Language Models to Edge Devices across Visual\n  Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Adapting Large Visual-Language Models to Edge Devices across Visual\n  Modalities"
                },
                "summary": "Recent advancements in Vision-Language (VL) models have sparked interest in\ntheir deployment on edge devices, yet challenges in handling diverse visual\nmodalities, manual annotation, and computational constraints remain. We\nintroduce EdgeVL, a novel framework that bridges this gap by seamlessly\nintegrating dual-modality knowledge distillation and quantization-aware\ncontrastive learning. This approach enables the adaptation of large VL models,\nlike CLIP, for efficient use with both RGB and non-RGB images on\nresource-limited devices without the need for manual annotations. EdgeVL not\nonly transfers visual language alignment capabilities to compact models but\nalso maintains feature quality post-quantization, significantly enhancing\nopen-vocabulary classification performance across various visual modalities.\nOur work represents the first systematic effort to adapt large VL models for\nedge deployment, showcasing up to 15.4% accuracy improvements on multiple\ndatasets and up to 93-fold reduction in model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Vision-Language (VL) models have sparked interest in\ntheir deployment on edge devices, yet challenges in handling diverse visual\nmodalities, manual annotation, and computational constraints remain. We\nintroduce EdgeVL, a novel framework that bridges this gap by seamlessly\nintegrating dual-modality knowledge distillation and quantization-aware\ncontrastive learning. This approach enables the adaptation of large VL models,\nlike CLIP, for efficient use with both RGB and non-RGB images on\nresource-limited devices without the need for manual annotations. EdgeVL not\nonly transfers visual language alignment capabilities to compact models but\nalso maintains feature quality post-quantization, significantly enhancing\nopen-vocabulary classification performance across various visual modalities.\nOur work represents the first systematic effort to adapt large VL models for\nedge deployment, showcasing up to 15.4% accuracy improvements on multiple\ndatasets and up to 93-fold reduction in model size."
                },
                "authors": [
                    {
                        "name": "Kaiwen Cai"
                    },
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Charles Fleming"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "arxiv_comment": "ECCV2024 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01453v3",
                "updated": "2024-10-01T13:46:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    46,
                    4,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-02T16:36:26Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    36,
                    26,
                    3,
                    123,
                    0
                ],
                "title": "Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?"
                },
                "summary": "We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs"
                },
                "authors": [
                    {
                        "name": "Lakshmi Nair"
                    },
                    {
                        "name": "Evana Gizzi"
                    },
                    {
                        "name": "Jivko Sinapov"
                    }
                ],
                "author_detail": {
                    "name": "Jivko Sinapov"
                },
                "author": "Jivko Sinapov",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16167v2",
                "updated": "2024-10-01T13:16:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    16,
                    45,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T15:08:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging."
                },
                "authors": [
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09864v2",
                "updated": "2024-10-01T13:07:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    7,
                    2,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-14T09:22:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    9,
                    22,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data"
                },
                "summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications."
                },
                "authors": [
                    {
                        "name": "Grigor Bezirganyan"
                    },
                    {
                        "name": "Sana Sellami"
                    },
                    {
                        "name": "Laure Berti-Équille"
                    },
                    {
                        "name": "Sébastien Fournier"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Fournier"
                },
                "author": "Sébastien Fournier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19979v2",
                "updated": "2024-10-01T13:04:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    4,
                    55,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T06:07:12Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    7,
                    12,
                    0,
                    274,
                    0
                ],
                "title": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model"
                },
                "summary": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimi Suzuki"
                },
                "author": "Yoshimi Suzuki",
                "arxiv_comment": "Long paper accepted to EMNLP 2024 Main. 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17946v2",
                "updated": "2024-10-01T13:01:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    13,
                    1,
                    40,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-26T15:20:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation"
                },
                "summary": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19924v2",
                "updated": "2024-10-01T12:43:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    43,
                    9,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T03:58:43Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    58,
                    43,
                    0,
                    274,
                    0
                ],
                "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning."
                },
                "authors": [
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yihan Xi"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Ufuk Topcu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "Updated link to code repository",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05904v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05904v3",
                "updated": "2024-10-01T12:08:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    8,
                    23,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-09T17:00:22Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    0,
                    22,
                    3,
                    130,
                    0
                ],
                "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"
                },
                "summary": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Jonathan Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Herzig"
                },
                "author": "Jonathan Herzig",
                "arxiv_comment": "Accepted as a long paper at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05904v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05904v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10122v3",
                "updated": "2024-10-01T12:07:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    7,
                    31,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-16T10:59:44Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    10,
                    59,
                    44,
                    3,
                    320,
                    0
                ],
                "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LLaVA: Learning United Visual Representation by Alignment Before\n  Projection"
                },
                "summary": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM. Code address:\n\\href{https://github.com/PKU-YuanGroup/Video-LLaVA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM. Code address:\n\\href{https://github.com/PKU-YuanGroup/Video-LLaVA}"
                },
                "authors": [
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jiaxi Cui"
                    },
                    {
                        "name": "Munan Ning"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12269v2",
                "updated": "2024-10-01T11:26:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    26,
                    11,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-18T04:55:09Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    55,
                    9,
                    1,
                    170,
                    0
                ],
                "title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization"
                },
                "summary": "Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework."
                },
                "authors": [
                    {
                        "name": "Kwangwook Seo"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19676v2",
                "updated": "2024-10-01T10:42:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    42,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-29T12:08:20Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    8,
                    20,
                    6,
                    273,
                    0
                ],
                "title": "See Detail Say Clear: Towards Brain CT Report Generation via\n  Pathological Clue-driven Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See Detail Say Clear: Towards Brain CT Report Generation via\n  Pathological Clue-driven Representation Learning"
                },
                "summary": "Brain CT report generation is significant to aid physicians in diagnosing\ncranial diseases. Recent studies concentrate on handling the consistency\nbetween visual and textual pathological features to improve the coherence of\nreport. However, there exist some challenges: 1) Redundant visual representing:\nMassive irrelevant areas in 3D scans distract models from representing salient\nvisual contexts. 2) Shifted semantic representing: Limited medical corpus\ncauses difficulties for models to transfer the learned textual representations\nto generative layers. This study introduces a Pathological Clue-driven\nRepresentation Learning (PCRL) model to build cross-modal representations based\non pathological clues and naturally adapt them for accurate report generation.\nSpecifically, we construct pathological clues from perspectives of segmented\nregions, pathological entities, and report themes, to fully grasp visual\npathological patterns and learn cross-modal feature representations. To adapt\nthe representations for the text generation task, we bridge the gap between\nrepresentation learning and report generation by using a unified large language\nmodel (LLM) with task-tailored instructions. These crafted instructions enable\nthe LLM to be flexibly fine-tuned across tasks and smoothly transfer the\nsemantic representation for report generation. Experiments demonstrate that our\nmethod outperforms previous methods and achieves SoTA performance. Our code is\navailable at \"https://github.com/Chauncey-Jheng/PCRL-MRG\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain CT report generation is significant to aid physicians in diagnosing\ncranial diseases. Recent studies concentrate on handling the consistency\nbetween visual and textual pathological features to improve the coherence of\nreport. However, there exist some challenges: 1) Redundant visual representing:\nMassive irrelevant areas in 3D scans distract models from representing salient\nvisual contexts. 2) Shifted semantic representing: Limited medical corpus\ncauses difficulties for models to transfer the learned textual representations\nto generative layers. This study introduces a Pathological Clue-driven\nRepresentation Learning (PCRL) model to build cross-modal representations based\non pathological clues and naturally adapt them for accurate report generation.\nSpecifically, we construct pathological clues from perspectives of segmented\nregions, pathological entities, and report themes, to fully grasp visual\npathological patterns and learn cross-modal feature representations. To adapt\nthe representations for the text generation task, we bridge the gap between\nrepresentation learning and report generation by using a unified large language\nmodel (LLM) with task-tailored instructions. These crafted instructions enable\nthe LLM to be flexibly fine-tuned across tasks and smoothly transfer the\nsemantic representation for report generation. Experiments demonstrate that our\nmethod outperforms previous methods and achieves SoTA performance. Our code is\navailable at \"https://github.com/Chauncey-Jheng/PCRL-MRG\"."
                },
                "authors": [
                    {
                        "name": "Chengxin Zheng"
                    },
                    {
                        "name": "Junzhong Ji"
                    },
                    {
                        "name": "Yanzhao Shi"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "arxiv_comment": "Our work has been accepted by EMNLP2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19382v2",
                "updated": "2024-10-01T10:28:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    10,
                    28,
                    32,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-28T15:13:04Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    13,
                    4,
                    5,
                    272,
                    0
                ],
                "title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with\n  Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly impacted\nthe domain of multi-hop question answering (MHQA), where systems are required\nto aggregate information and infer answers from disparate pieces of text.\nHowever, the autoregressive nature of LLMs inherently poses a challenge as\nerrors may accumulate if mistakes are made in the intermediate reasoning steps.\nThis paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question\nAnswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to\nidentify optimal reasoning paths in MHQA tasks, mitigating the error\npropagation from sequential reasoning processes. Unlike previous works, we\npropose a zero-shot prompting method, which relies solely on instructions\nwithout the support of hand-crafted few-shot examples that typically require\ndomain expertise. We also introduce a behavioral cloning approach (MZQA-BC)\ntrained on self-generated MCTS inference trajectories, achieving an over\n10-fold increase in reasoning speed with bare compromise in performance. The\nefficacy of our method is validated on standard benchmarks such as HotpotQA,\n2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly impacted\nthe domain of multi-hop question answering (MHQA), where systems are required\nto aggregate information and infer answers from disparate pieces of text.\nHowever, the autoregressive nature of LLMs inherently poses a challenge as\nerrors may accumulate if mistakes are made in the intermediate reasoning steps.\nThis paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question\nAnswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to\nidentify optimal reasoning paths in MHQA tasks, mitigating the error\npropagation from sequential reasoning processes. Unlike previous works, we\npropose a zero-shot prompting method, which relies solely on instructions\nwithout the support of hand-crafted few-shot examples that typically require\ndomain expertise. We also introduce a behavioral cloning approach (MZQA-BC)\ntrained on self-generated MCTS inference trajectories, achieving an over\n10-fold increase in reasoning speed with bare compromise in performance. The\nefficacy of our method is validated on standard benchmarks such as HotpotQA,\n2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing\nframeworks."
                },
                "authors": [
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Jaewook Shin"
                    },
                    {
                        "name": "Youngjin Ahn"
                    },
                    {
                        "name": "Seokin Seo"
                    },
                    {
                        "name": "Ohjoon Kwon"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kee-Eung Kim"
                },
                "author": "Kee-Eung Kim",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v3",
                "updated": "2024-10-02T07:38:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    7,
                    38,
                    2,
                    2,
                    276,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08202v2",
                "updated": "2024-10-01T09:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    55,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-15T15:10:01Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    10,
                    1,
                    3,
                    228,
                    0
                ],
                "title": "Towards Practical Human Motion Prediction with LiDAR Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Human Motion Prediction with LiDAR Point Clouds"
                },
                "summary": "Human motion prediction is crucial for human-centric multimedia understanding\nand interacting. Current methods typically rely on ground truth human poses as\nobserved input, which is not practical for real-world scenarios where only raw\nvisual sensor data is available. To implement these methods in practice, a\npre-phrase of pose estimation is essential. However, such two-stage approaches\noften lead to performance degradation due to the accumulation of errors.\nMoreover, reducing raw visual data to sparse keypoint representations\nsignificantly diminishes the density of information, resulting in the loss of\nfine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first\nsingle-LiDAR-based 3D human motion prediction approach, which receives the raw\nLiDAR point cloud as input and forecasts future 3D human poses directly.\nBuilding upon our novel structure-aware body feature descriptor, LiDAR-HMP\nadaptively maps the observed motion manifold to future poses and effectively\nmodels the spatial-temporal correlations of human motions for further\nrefinement of prediction results. Extensive experiments show that our method\nachieves state-of-the-art performance on two public benchmarks and demonstrates\nremarkable robustness and efficacy in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion prediction is crucial for human-centric multimedia understanding\nand interacting. Current methods typically rely on ground truth human poses as\nobserved input, which is not practical for real-world scenarios where only raw\nvisual sensor data is available. To implement these methods in practice, a\npre-phrase of pose estimation is essential. However, such two-stage approaches\noften lead to performance degradation due to the accumulation of errors.\nMoreover, reducing raw visual data to sparse keypoint representations\nsignificantly diminishes the density of information, resulting in the loss of\nfine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first\nsingle-LiDAR-based 3D human motion prediction approach, which receives the raw\nLiDAR point cloud as input and forecasts future 3D human poses directly.\nBuilding upon our novel structure-aware body feature descriptor, LiDAR-HMP\nadaptively maps the observed motion manifold to future poses and effectively\nmodels the spatial-temporal correlations of human motions for further\nrefinement of prediction results. Extensive experiments show that our method\nachieves state-of-the-art performance on two public benchmarks and demonstrates\nremarkable robustness and efficacy in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Xiao Han"
                    },
                    {
                        "name": "Yiming Ren"
                    },
                    {
                        "name": "Yichen Yao"
                    },
                    {
                        "name": "Yujing Sun"
                    },
                    {
                        "name": "Yuexin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuexin Ma"
                },
                "author": "Yuexin Ma",
                "arxiv_comment": "Accepted by ACM MM Oral 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18943v2",
                "updated": "2024-10-01T09:20:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    20,
                    58,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T17:44:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models"
                },
                "summary": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler."
                },
                "authors": [
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "yuelin bai"
                    },
                    {
                        "name": "Run Luo"
                    },
                    {
                        "name": "Longze Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15301v2",
                "updated": "2024-10-01T09:05:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    5,
                    45,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-27T15:03:01Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    3,
                    1,
                    1,
                    240,
                    0
                ],
                "title": "The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization"
                },
                "summary": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Minghai Qin"
                    }
                ],
                "author_detail": {
                    "name": "Minghai Qin"
                },
                "author": "Minghai Qin",
                "arxiv_comment": "27 pages, 41 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v3",
                "updated": "2024-10-01T08:50:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    50,
                    1,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10691v2",
                "updated": "2024-10-01T08:48:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    48,
                    34,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-20T09:42:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    42,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches"
                },
                "summary": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge."
                },
                "authors": [
                    {
                        "name": "Yanjie Dong"
                    },
                    {
                        "name": "Haijun Zhang"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17518v2",
                "updated": "2024-10-01T08:08:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    8,
                    42,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-26T04:01:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    1,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "Multi-Designated Detector Watermarking for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Designated Detector Watermarking for Language Models"
                },
                "summary": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics."
                },
                "authors": [
                    {
                        "name": "Zhengan Huang"
                    },
                    {
                        "name": "Gongxian Zeng"
                    },
                    {
                        "name": "Xin Mu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19262v2",
                "updated": "2024-10-01T08:05:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    5,
                    23,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-28T09:36:55Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    9,
                    36,
                    55,
                    3,
                    88,
                    0
                ],
                "title": "Removing the need for ground truth UWB data collection: self-supervised\n  ranging error correction using deep reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing the need for ground truth UWB data collection: self-supervised\n  ranging error correction using deep reinforcement learning"
                },
                "summary": "Indoor positioning using UWB technology has gained interest due to its\ncentimeter-level accuracy potential. However, multipath effects and\nnon-line-of-sight conditions cause ranging errors between anchors and tags.\nExisting approaches for mitigating these ranging errors rely on collecting\nlarge labeled datasets, making them impractical for real-world deployments.\nThis paper proposes a novel self-supervised deep reinforcement learning\napproach that does not require labeled ground truth data. A reinforcement\nlearning agent uses the channel impulse response as a state and predicts\ncorrections to minimize the error between corrected and estimated ranges. The\nagent learns, self-supervised, by iteratively improving corrections that are\ngenerated by combining the predictability of trajectories with filtering and\nsmoothening. Experiments on real-world UWB measurements demonstrate comparable\nperformance to state-of-the-art supervised methods, overcoming data dependency\nand lack of generalizability limitations. This makes self-supervised deep\nreinforcement learning a promising solution for practical and scalable\nUWB-ranging error correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor positioning using UWB technology has gained interest due to its\ncentimeter-level accuracy potential. However, multipath effects and\nnon-line-of-sight conditions cause ranging errors between anchors and tags.\nExisting approaches for mitigating these ranging errors rely on collecting\nlarge labeled datasets, making them impractical for real-world deployments.\nThis paper proposes a novel self-supervised deep reinforcement learning\napproach that does not require labeled ground truth data. A reinforcement\nlearning agent uses the channel impulse response as a state and predicts\ncorrections to minimize the error between corrected and estimated ranges. The\nagent learns, self-supervised, by iteratively improving corrections that are\ngenerated by combining the predictability of trajectories with filtering and\nsmoothening. Experiments on real-world UWB measurements demonstrate comparable\nperformance to state-of-the-art supervised methods, overcoming data dependency\nand lack of generalizability limitations. This makes self-supervised deep\nreinforcement learning a promising solution for practical and scalable\nUWB-ranging error correction."
                },
                "authors": [
                    {
                        "name": "Dieter Coppens"
                    },
                    {
                        "name": "Ben Van Herbruggen"
                    },
                    {
                        "name": "Adnan Shahid"
                    },
                    {
                        "name": "Eli De Poorter"
                    }
                ],
                "author_detail": {
                    "name": "Eli De Poorter"
                },
                "author": "Eli De Poorter",
                "arxiv_comment": "13 pages, 9 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19872v2",
                "updated": "2024-10-01T07:34:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    34,
                    25,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:13:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    13,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration"
                },
                "summary": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant\nchallenges for effective knowledge editing. Current methods, including\nintrinsic knowledge editing and external knowledge resorting, each possess\nstrengths and weaknesses, struggling to balance the desired properties of\nreliability, generality, and locality when applied to MLLMs. In this paper, we\npropose UniKE, a novel multimodal editing method that establishes a unified\nperspective and paradigm for intrinsic knowledge editing and external knowledge\nresorting. Both types of knowledge are conceptualized as vectorized key-value\nmemories, with the corresponding editing processes resembling the assimilation\nand accommodation phases of human cognition, conducted at the same semantic\nlevels. Within such a unified framework, we further promote knowledge\ncollaboration by disentangling the knowledge representations into the semantic\nand truthfulness spaces. Extensive experiments validate the effectiveness of\nour method, which ensures that the post-edit MLLM simultaneously maintains\nexcellent reliability, generality, and locality. The code for UniKE will be\navailable at \\url{https://github.com/beepkh/UniKE}."
                },
                "authors": [
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Zhaoyu Fan"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Qianru Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianru Sun"
                },
                "author": "Qianru Sun",
                "arxiv_comment": "Accepted by NeurIPS 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18618v2",
                "updated": "2024-10-01T07:29:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    29,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-27T10:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback"
                },
                "summary": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback."
                },
                "authors": [
                    {
                        "name": "Jaepill Choi"
                    },
                    {
                        "name": "Kyubyung Chae"
                    },
                    {
                        "name": "Jiwoo Song"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19898v2",
                "updated": "2024-10-01T07:11:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    11,
                    44,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:56:35Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    56,
                    35,
                    0,
                    274,
                    0
                ],
                "title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs"
                },
                "summary": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0."
                },
                "authors": [
                    {
                        "name": "Yuho Lee"
                    },
                    {
                        "name": "Taewon Yun"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Accepted at EMNLP-Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19663v2",
                "updated": "2024-10-01T06:35:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    35,
                    24,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-29T11:29:57Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    11,
                    29,
                    57,
                    6,
                    273,
                    0
                ],
                "title": "Identifying Knowledge Editing Types in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Knowledge Editing Types in Large Language Models"
                },
                "summary": "Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Huijun Liu"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yu"
                },
                "author": "Jie Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18384v2",
                "updated": "2024-10-01T06:17:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    17,
                    52,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-29T02:43:23Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    43,
                    23,
                    0,
                    120,
                    0
                ],
                "title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions"
                },
                "summary": "Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions."
                },
                "authors": [
                    {
                        "name": "Jordan Meadows"
                    },
                    {
                        "name": "Tamsin James"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19389v2",
                "updated": "2024-10-01T06:07:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    7,
                    24,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-27T17:59:01Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    59,
                    1,
                    3,
                    179,
                    0
                ],
                "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding"
                },
                "summary": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "NeurIPS-2024. Project page:\n  https://lxtgh.github.io/project/omg_llava/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20441v2",
                "updated": "2024-10-01T06:03:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    3,
                    22,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T16:00:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    0,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-adaptive Zero-shot Chain-of-Thought Prompting"
                },
                "summary": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Renchu Guan"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v2",
                "updated": "2024-10-01T05:55:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    55,
                    33,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08642v2",
                "updated": "2024-10-01T05:42:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    42,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-13T08:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "title": "CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks"
                },
                "summary": "Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%)."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Junzhe Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Jing Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jing Bai"
                },
                "author": "Jing Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v2",
                "updated": "2024-10-01T05:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    37,
                    7,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13647v2",
                "updated": "2024-10-01T05:28:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    28,
                    54,
                    1,
                    275,
                    0
                ],
                "published": "2024-07-18T16:25:17Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    25,
                    17,
                    3,
                    200,
                    0
                ],
                "title": "Weak-to-Strong Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Reasoning"
                },
                "summary": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}."
                },
                "authors": [
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yan Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v2",
                "updated": "2024-10-01T05:19:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    19,
                    17,
                    1,
                    275,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Vision and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Under Review in the Special Issue of ACM Transactions on Software\n  Engineering and Methodology (TOSEM): 2030 Software Engineering Roadmap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16674v3",
                "updated": "2024-10-01T04:45:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    45,
                    4,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-25T07:06:14Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "title": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models"
                },
                "summary": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    }
                ],
                "author_detail": {
                    "name": "Toyotaro Suzumura"
                },
                "author": "Toyotaro Suzumura",
                "arxiv_comment": "Risks: The 1st International Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommendation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12566v3",
                "updated": "2024-10-01T04:42:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    42,
                    48,
                    1,
                    275,
                    0
                ],
                "published": "2024-06-18T12:52:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    52,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users."
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Xin Yu"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04732v3",
                "updated": "2024-10-01T04:41:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    41,
                    53,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-07T18:35:54Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    18,
                    35,
                    54,
                    3,
                    67,
                    0
                ],
                "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We from Intelligent Visual Deductive Reasoning?"
                },
                "summary": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "He Bai"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Shuangfei Zhai"
                    },
                    {
                        "name": "Josh Susskind"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "arxiv_comment": "COLM 2024. https://github.com/apple/ml-rpm-bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19894v2",
                "updated": "2024-10-01T04:35:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    35,
                    5,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T02:53:03Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    53,
                    3,
                    0,
                    274,
                    0
                ],
                "title": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation"
                },
                "summary": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02262v2",
                "updated": "2024-10-01T04:10:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    10,
                    34,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-03T22:56:43Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    22,
                    56,
                    43,
                    4,
                    307,
                    0
                ],
                "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs"
                },
                "summary": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA ."
                },
                "authors": [
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Chandan Singh"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Bin Yu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "arxiv_comment": "The 12th International Conference on Learning Representations (ICLR\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v3",
                "updated": "2024-10-01T03:12:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    12,
                    35,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19868v2",
                "updated": "2024-10-01T02:57:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    57,
                    50,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-30T01:59:38Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    1,
                    59,
                    38,
                    0,
                    274,
                    0
                ],
                "title": "The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for\n  Digital Library Document Recommendation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for\n  Digital Library Document Recommendation Tasks"
                },
                "summary": "This paper investigates the performance of several representative large\nmodels in the field of literature recommendation and explores potential biases.\nThe results indicate that while some large models' recommendations can be\nsomewhat satisfactory after simple manual screening, overall, the accuracy of\nthese models in specific literature recommendation tasks is generally moderate.\nAdditionally, the models tend to recommend literature that is timely,\ncollaborative, and expands or deepens the field. In scholar recommendation\ntasks. There is no evidence to suggest that LLMs exacerbate inequalities\nrelated to gender, race, or the level of development of countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of several representative large\nmodels in the field of literature recommendation and explores potential biases.\nThe results indicate that while some large models' recommendations can be\nsomewhat satisfactory after simple manual screening, overall, the accuracy of\nthese models in specific literature recommendation tasks is generally moderate.\nAdditionally, the models tend to recommend literature that is timely,\ncollaborative, and expands or deepens the field. In scholar recommendation\ntasks. There is no evidence to suggest that LLMs exacerbate inequalities\nrelated to gender, race, or the level of development of countries."
                },
                "authors": [
                    {
                        "name": "Yifan Tian"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "There are some aspects in the original research that need to be\n  supplemented with robustness checks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01314v2",
                "updated": "2024-10-01T02:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    56,
                    30,
                    1,
                    275,
                    0
                ],
                "published": "2023-12-03T08:09:45Z",
                "published_parsed": [
                    2023,
                    12,
                    3,
                    8,
                    9,
                    45,
                    6,
                    337,
                    0
                ],
                "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian"
                },
                "summary": "Norwegian, spoken by only 5 million population, is under-representative\nwithin the most impressive breakthroughs in NLP tasks. To the best of our\nknowledge, there has not yet been a comprehensive evaluation of the existing\nlanguage models (LMs) on Norwegian generation tasks during the article writing\nprocess. To fill this gap, we 1) compiled the existing Norwegian dataset and\npre-trained 4 Norwegian Open Language Models varied from parameter scales and\narchitectures, collectively called NorGLM; 2) introduced a comprehensive\nbenchmark, NLEBench, for evaluating natural language generation capabilities in\nNorwegian, encompassing translation and human annotation. Based on the\ninvestigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5\nhas limited capability in understanding the Norwegian context; 2) the increase\nin model parameter scales demonstrates limited impact on the performance of\ndownstream tasks when the pre-training dataset is constrained in size; 3)\nsmaller models also demonstrate the reasoning capability through\nChain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be\nused to verify the generalizability of LLMs on natural language understanding\nand, meanwhile, test the interconnectedness of these NLP tasks. We share our\nresources and code for reproducibility under a CC BY-NC 4.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Norwegian, spoken by only 5 million population, is under-representative\nwithin the most impressive breakthroughs in NLP tasks. To the best of our\nknowledge, there has not yet been a comprehensive evaluation of the existing\nlanguage models (LMs) on Norwegian generation tasks during the article writing\nprocess. To fill this gap, we 1) compiled the existing Norwegian dataset and\npre-trained 4 Norwegian Open Language Models varied from parameter scales and\narchitectures, collectively called NorGLM; 2) introduced a comprehensive\nbenchmark, NLEBench, for evaluating natural language generation capabilities in\nNorwegian, encompassing translation and human annotation. Based on the\ninvestigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5\nhas limited capability in understanding the Norwegian context; 2) the increase\nin model parameter scales demonstrates limited impact on the performance of\ndownstream tasks when the pre-training dataset is constrained in size; 3)\nsmaller models also demonstrate the reasoning capability through\nChain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be\nused to verify the generalizability of LLMs on natural language understanding\nand, meanwhile, test the interconnectedness of these NLP tasks. We share our\nresources and code for reproducibility under a CC BY-NC 4.0 license."
                },
                "authors": [
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Terje Farup"
                    },
                    {
                        "name": "Even W. Lauvrak"
                    },
                    {
                        "name": "Jon Espen Ingvaldsen"
                    },
                    {
                        "name": "Simen Eide"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Zhirong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhirong Yang"
                },
                "author": "Zhirong Yang",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference. Code available at\n  https://github.com/Smartmedia-AI/NorGLM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15004v3",
                "updated": "2024-10-02T03:46:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    3,
                    46,
                    17,
                    2,
                    276,
                    0
                ],
                "published": "2024-03-22T07:32:21Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    7,
                    32,
                    21,
                    4,
                    82,
                    0
                ],
                "title": "ParFormer: A Vision Transformer with Parallel Mixer and Sparse Channel\n  Attention Patch Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParFormer: A Vision Transformer with Parallel Mixer and Sparse Channel\n  Attention Patch Embedding"
                },
                "summary": "Convolutional Neural Networks (CNNs) and Transformers have achieved\nremarkable success in computer vision tasks. However, their deep architectures\noften lead to high computational redundancy, making them less suitable for\nresource-constrained environments, such as edge devices. This paper introduces\nParFormer, a novel vision transformer that addresses this challenge by\nincorporating a Parallel Mixer and a Sparse Channel Attention Patch Embedding\n(SCAPE). By combining convolutional and attention mechanisms, ParFormer\nimproves feature extraction. This makes spatial feature extraction more\nefficient and cuts down on unnecessary computation. The SCAPE module further\nreduces computational redundancy while preserving essential feature information\nduring down-sampling. Experimental results on the ImageNet-1K dataset show that\nParFormer-T achieves 78.9\\% Top-1 accuracy with a high throughput on a GPU that\noutperforms other small models with 2.56$\\times$ higher throughput than\nMobileViT-S, 0.24\\% faster than FasterNet-T2, and 1.79$\\times$ higher than\nEdgeNeXt-S. For edge device deployment, ParFormer-T excels with a throughput of\n278.1 images/sec, which is 1.38 $\\times$ higher than EdgeNeXt-S and\n2.36$\\times$ higher than MobileViT-S, making it highly suitable for real-time\napplications in resource-constrained settings. The larger variant, ParFormer-L,\nreaches 83.5\\% Top-1 accuracy, offering a balanced trade-off between accuracy\nand efficiency, surpassing many state-of-the-art models. In COCO object\ndetection, ParFormer-M achieves 40.7 AP for object detection and 37.6 AP for\ninstance segmentation, surpassing models like ResNet-50, PVT-S and\nPoolFormer-S24 with significantly higher efficiency. These results validate\nParFormer as a highly efficient and scalable model for both high-performance\nand resource-constrained scenarios, making it an ideal solution for edge-based\nAI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) and Transformers have achieved\nremarkable success in computer vision tasks. However, their deep architectures\noften lead to high computational redundancy, making them less suitable for\nresource-constrained environments, such as edge devices. This paper introduces\nParFormer, a novel vision transformer that addresses this challenge by\nincorporating a Parallel Mixer and a Sparse Channel Attention Patch Embedding\n(SCAPE). By combining convolutional and attention mechanisms, ParFormer\nimproves feature extraction. This makes spatial feature extraction more\nefficient and cuts down on unnecessary computation. The SCAPE module further\nreduces computational redundancy while preserving essential feature information\nduring down-sampling. Experimental results on the ImageNet-1K dataset show that\nParFormer-T achieves 78.9\\% Top-1 accuracy with a high throughput on a GPU that\noutperforms other small models with 2.56$\\times$ higher throughput than\nMobileViT-S, 0.24\\% faster than FasterNet-T2, and 1.79$\\times$ higher than\nEdgeNeXt-S. For edge device deployment, ParFormer-T excels with a throughput of\n278.1 images/sec, which is 1.38 $\\times$ higher than EdgeNeXt-S and\n2.36$\\times$ higher than MobileViT-S, making it highly suitable for real-time\napplications in resource-constrained settings. The larger variant, ParFormer-L,\nreaches 83.5\\% Top-1 accuracy, offering a balanced trade-off between accuracy\nand efficiency, surpassing many state-of-the-art models. In COCO object\ndetection, ParFormer-M achieves 40.7 AP for object detection and 37.6 AP for\ninstance segmentation, surpassing models like ResNet-50, PVT-S and\nPoolFormer-S24 with significantly higher efficiency. These results validate\nParFormer as a highly efficient and scalable model for both high-performance\nand resource-constrained scenarios, making it an ideal solution for edge-based\nAI applications."
                },
                "authors": [
                    {
                        "name": "Novendra Setyawan"
                    },
                    {
                        "name": "Ghufron Wahyu Kurniawan"
                    },
                    {
                        "name": "Chi-Chia Sun"
                    },
                    {
                        "name": "Jun-Wei Hsieh"
                    },
                    {
                        "name": "Jing-Ming Guo"
                    },
                    {
                        "name": "Wen-Kai Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Kai Kuo"
                },
                "author": "Wen-Kai Kuo",
                "arxiv_comment": "Under Review in IEEE Transactions on Cognitive and Developmental\n  System",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13140v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13140v3",
                "updated": "2024-10-01T01:50:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    50,
                    19,
                    1,
                    275,
                    0
                ],
                "published": "2023-08-25T02:33:11Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    2,
                    33,
                    11,
                    4,
                    237,
                    0
                ],
                "title": "Learn With Imagination: Safe Set Guided State-wise Constrained Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn With Imagination: Safe Set Guided State-wise Constrained Policy\n  Optimization"
                },
                "summary": "Deep reinforcement learning (RL) excels in various control tasks, yet the\nabsence of safety guarantees hampers its real-world applicability. In\nparticular, explorations during learning usually results in safety violations,\nwhile the RL agent learns from those mistakes. On the other hand, safe control\ntechniques ensure persistent safety satisfaction but demand strong priors on\nsystem dynamics, which is usually hard to obtain in practice. To address these\nproblems, we present Safe Set Guided State-wise Constrained Policy Optimization\n(S-3PO), a pioneering algorithm generating state-wise safe optimal policies\nwith zero training violations, i.e., learning without mistakes. S-3PO first\nemploys a safety-oriented monitor with black-box dynamics to ensure safe\nexploration. It then enforces an \"imaginary\" cost for the RL agent to converge\nto optimal behaviors within safety constraints. S-3PO outperforms existing\nmethods in high-dimensional robotics tasks, managing state-wise constraints\nwith zero training violation. This innovation marks a significant stride\ntowards real-world safe RL deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (RL) excels in various control tasks, yet the\nabsence of safety guarantees hampers its real-world applicability. In\nparticular, explorations during learning usually results in safety violations,\nwhile the RL agent learns from those mistakes. On the other hand, safe control\ntechniques ensure persistent safety satisfaction but demand strong priors on\nsystem dynamics, which is usually hard to obtain in practice. To address these\nproblems, we present Safe Set Guided State-wise Constrained Policy Optimization\n(S-3PO), a pioneering algorithm generating state-wise safe optimal policies\nwith zero training violations, i.e., learning without mistakes. S-3PO first\nemploys a safety-oriented monitor with black-box dynamics to ensure safe\nexploration. It then enforces an \"imaginary\" cost for the RL agent to converge\nto optimal behaviors within safety constraints. S-3PO outperforms existing\nmethods in high-dimensional robotics tasks, managing state-wise constraints\nwith zero training violation. This innovation marks a significant stride\ntowards real-world safe RL deployment."
                },
                "authors": [
                    {
                        "name": "Feihan Li"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Weiye Zhao"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Tianhao Wei"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13140v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13140v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03247v3",
                "updated": "2024-10-01T01:48:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    48,
                    58,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-06T15:07:08Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    15,
                    7,
                    8,
                    1,
                    219,
                    0
                ],
                "title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons"
                },
                "summary": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon."
                },
                "authors": [
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Wanting Wen"
                    },
                    {
                        "name": "Yu Sheng"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Daniel Dajun Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Dajun Zeng"
                },
                "author": "Daniel Dajun Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08978v2",
                "updated": "2024-10-01T01:40:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    40,
                    14,
                    1,
                    275,
                    0
                ],
                "published": "2024-08-16T19:01:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    19,
                    1,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses"
                },
                "summary": "The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation."
                },
                "authors": [
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Yinghao Yang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08369v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08369v4",
                "updated": "2024-10-01T01:24:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    1,
                    24,
                    5,
                    1,
                    275,
                    0
                ],
                "published": "2023-11-14T18:32:52Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    18,
                    32,
                    52,
                    1,
                    318,
                    0
                ],
                "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection"
                },
                "summary": "To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance."
                },
                "authors": [
                    {
                        "name": "Ryuto Koike"
                    },
                    {
                        "name": "Masahiro Kaneko"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "EMNLP 2024 Findings camera ready. Dataset available at\n  https://github.com/ryuryukke/HowYouPromptMatters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08369v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08369v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01908v2",
                "updated": "2024-10-01T00:30:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    30,
                    6,
                    1,
                    275,
                    0
                ],
                "published": "2024-02-02T21:21:06Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    21,
                    21,
                    6,
                    4,
                    33,
                    0
                ],
                "title": "Large language models should not replace human participants because they\n  can misportray and flatten identity groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models should not replace human participants because they\n  can misportray and flatten identity groups"
                },
                "summary": "Large language models (LLMs) are increasing in capability and popularity,\npropelling their application in new domains -- including as replacements for\nhuman participants in computational social science, user testing, annotation\ntasks, and more. In many settings, researchers seek to distribute their surveys\nto a sample of participants that are representative of the underlying human\npopulation of interest. This means in order to be a suitable replacement, LLMs\nwill need to be able to capture the influence of positionality (i.e., relevance\nof social identities like gender and race). However, we show that there are two\ninherent limitations in the way current LLMs are trained that prevent this. We\nargue analytically for why LLMs are likely to both misportray and flatten the\nrepresentations of demographic groups, then empirically show this on 4 LLMs\nthrough a series of human studies with 3200 participants across 16 demographic\nidentities. We also discuss a third limitation about how identity prompts can\nessentialize identities. Throughout, we connect each limitation to a pernicious\nhistory that explains why it is harmful for marginalized demographic groups.\nOverall, we urge caution in use cases where LLMs are intended to replace human\nparticipants whose identities are relevant to the task at hand. At the same\ntime, in cases where the goal is to supplement rather than replace (e.g., pilot\nstudies), we provide inference-time techniques that we empirically demonstrate\ndo reduce, but do not remove, these harms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasing in capability and popularity,\npropelling their application in new domains -- including as replacements for\nhuman participants in computational social science, user testing, annotation\ntasks, and more. In many settings, researchers seek to distribute their surveys\nto a sample of participants that are representative of the underlying human\npopulation of interest. This means in order to be a suitable replacement, LLMs\nwill need to be able to capture the influence of positionality (i.e., relevance\nof social identities like gender and race). However, we show that there are two\ninherent limitations in the way current LLMs are trained that prevent this. We\nargue analytically for why LLMs are likely to both misportray and flatten the\nrepresentations of demographic groups, then empirically show this on 4 LLMs\nthrough a series of human studies with 3200 participants across 16 demographic\nidentities. We also discuss a third limitation about how identity prompts can\nessentialize identities. Throughout, we connect each limitation to a pernicious\nhistory that explains why it is harmful for marginalized demographic groups.\nOverall, we urge caution in use cases where LLMs are intended to replace human\nparticipants whose identities are relevant to the task at hand. At the same\ntime, in cases where the goal is to supplement rather than replace (e.g., pilot\nstudies), we provide inference-time techniques that we empirically demonstrate\ndo reduce, but do not remove, these harms."
                },
                "authors": [
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Jamie Morgenstern"
                    },
                    {
                        "name": "John P. Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P. Dickerson"
                },
                "author": "John P. Dickerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17633v2",
                "updated": "2024-10-01T00:17:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    17,
                    41,
                    1,
                    275,
                    0
                ],
                "published": "2024-05-27T20:00:38Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    20,
                    0,
                    38,
                    0,
                    148,
                    0
                ],
                "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal\n  Stories with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal\n  Stories with LLMs"
                },
                "summary": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights."
                },
                "authors": [
                    {
                        "name": "Jocelyn Shen"
                    },
                    {
                        "name": "Joel Mire"
                    },
                    {
                        "name": "Hae Won Park"
                    },
                    {
                        "name": "Cynthia Breazeal"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17146v2",
                "updated": "2024-10-01T00:09:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    0,
                    9,
                    49,
                    1,
                    275,
                    0
                ],
                "published": "2024-03-25T19:44:06Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    19,
                    44,
                    6,
                    0,
                    85,
                    0
                ],
                "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-Constrained Large Language Models for Countering Hate Speech"
                },
                "summary": "Automatic counterspeech generation methods have been developed to assist\nefforts in combating hate speech. Existing research focuses on generating\ncounterspeech with linguistic attributes such as being polite, informative, and\nintent-driven. However, the real impact of counterspeech in online environments\nis seldom considered. This study aims to develop methods for generating\ncounterspeech constrained by conversation outcomes and evaluate their\neffectiveness. We experiment with large language models (LLMs) to incorporate\ninto the text generation process two desired conversation outcomes: low\nconversation incivility and non-hateful hater reentry. Specifically, we\nexperiment with instruction prompts, LLM finetuning, and LLM reinforcement\nlearning (RL). Evaluation results show that our methods effectively steer the\ngeneration of counterspeech toward the desired outcomes. Our analyses, however,\nshow that there are differences in the quality and style depending on the\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic counterspeech generation methods have been developed to assist\nefforts in combating hate speech. Existing research focuses on generating\ncounterspeech with linguistic attributes such as being polite, informative, and\nintent-driven. However, the real impact of counterspeech in online environments\nis seldom considered. This study aims to develop methods for generating\ncounterspeech constrained by conversation outcomes and evaluate their\neffectiveness. We experiment with large language models (LLMs) to incorporate\ninto the text generation process two desired conversation outcomes: low\nconversation incivility and non-hateful hater reentry. Specifically, we\nexperiment with instruction prompts, LLM finetuning, and LLM reinforcement\nlearning (RL). Evaluation results show that our methods effectively steer the\ngeneration of counterspeech toward the desired outcomes. Our analyses, however,\nshow that there are differences in the quality and style depending on the\nmodel."
                },
                "authors": [
                    {
                        "name": "Lingzi Hong"
                    },
                    {
                        "name": "Pengcheng Luo"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Xiaoying Song"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoying Song"
                },
                "author": "Xiaoying Song",
                "arxiv_comment": "Accepted for presentation at the EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17648v2",
                "updated": "2024-09-30T22:52:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    52,
                    18,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-26T08:55:21Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "title": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited."
                },
                "authors": [
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Phat Vo"
                    },
                    {
                        "name": "Arman Kizilkale"
                    },
                    {
                        "name": "Aaron Reite"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reite"
                },
                "author": "Aaron Reite",
                "arxiv_comment": "6 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16578v2",
                "updated": "2024-09-30T21:40:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    40,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-25T03:15:17Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    15,
                    17,
                    2,
                    269,
                    0
                ],
                "title": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning"
                },
                "summary": "In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiaheng Hu"
                    },
                    {
                        "name": "Rose Hendrix"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Roberto Martin-Martin"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Kuo-Hao Zeng"
                    },
                    {
                        "name": "Kiana Ehsani"
                    }
                ],
                "author_detail": {
                    "name": "Kiana Ehsani"
                },
                "author": "Kiana Ehsani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17336v2",
                "updated": "2024-09-30T21:25:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    25,
                    23,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-26T02:47:42Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    2,
                    47,
                    42,
                    1,
                    86,
                    0
                ],
                "title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models"
                },
                "summary": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Shunning Liang"
                    },
                    {
                        "name": "Zach Cameron"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Ning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Zhang"
                },
                "author": "Ning Zhang",
                "arxiv_comment": "Accepted by USENIX Security 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14732v2",
                "updated": "2024-09-30T21:25:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    21,
                    25,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-20T20:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    20,
                    55,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text\n  Question Answering with Reasoning and Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text\n  Question Answering with Reasoning and Summarization"
                },
                "summary": "Question answering (QA) over tables and text has gained much popularity over\nthe years. Multi-hop table-text QA requires multiple hops between the table and\ntext, making it a challenging QA task. Although several works have attempted to\nsolve the table-text QA task, most involve training the models and requiring\nlabeled data. In this paper, we have proposed a Retrieval Augmented Generation\n(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop\nTable-Text Question Answering with Reasoning and Summarization. Our model uses\nan enhanced retriever for table-text information retrieval and uses augmented\nknowledge, including table-text summary with decomposed sub-questions with\nanswers for a reasoning-based table-text QA. Using open-source language models,\nour model outperformed all existing prompting methods for table-text QA tasks\non existing table-text QA datasets, such as HybridQA and OTT-QA's development\nset. Our experiments demonstrate the potential of prompt-based approaches using\nopen-source LLMs. Additionally, by using LLaMA3-70B, our model achieved\nstate-of-the-art performance for prompting-based methods on multi-hop\ntable-text QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) over tables and text has gained much popularity over\nthe years. Multi-hop table-text QA requires multiple hops between the table and\ntext, making it a challenging QA task. Although several works have attempted to\nsolve the table-text QA task, most involve training the models and requiring\nlabeled data. In this paper, we have proposed a Retrieval Augmented Generation\n(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop\nTable-Text Question Answering with Reasoning and Summarization. Our model uses\nan enhanced retriever for table-text information retrieval and uses augmented\nknowledge, including table-text summary with decomposed sub-questions with\nanswers for a reasoning-based table-text QA. Using open-source language models,\nour model outperformed all existing prompting methods for table-text QA tasks\non existing table-text QA datasets, such as HybridQA and OTT-QA's development\nset. Our experiments demonstrate the potential of prompt-based approaches using\nopen-source LLMs. Additionally, by using LLaMA3-70B, our model achieved\nstate-of-the-art performance for prompting-based methods on multi-hop\ntable-text QA."
                },
                "authors": [
                    {
                        "name": "Jayetri Bardhan"
                    },
                    {
                        "name": "Bushi Xiao"
                    },
                    {
                        "name": "Daisy Zhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Daisy Zhe Wang"
                },
                "author": "Daisy Zhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v4",
                "updated": "2024-09-30T20:57:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    57,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate twelve LLMs from six model\nfamilies, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$, followed by\nLLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental\nresults are publicly available via https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate twelve LLMs from six model\nfamilies, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$, followed by\nLLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental\nresults are publicly available via https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "11 pages of main text; 19 pages of appendices. Included models:\n  GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7,\n  70, 405}B, Mixtral-8x{7, 22}B, Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v4",
                "updated": "2024-09-30T20:42:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    42,
                    22,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.01397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.01397v5",
                "updated": "2024-09-30T20:29:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    29,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2023-04-03T22:16:52Z",
                "published_parsed": [
                    2023,
                    4,
                    3,
                    22,
                    16,
                    52,
                    0,
                    93,
                    0
                ],
                "title": "LTM: Scalable and Black-box Similarity-based Test Suite Minimization\n  based on Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTM: Scalable and Black-box Similarity-based Test Suite Minimization\n  based on Language Models"
                },
                "summary": "Test suites tend to grow when software evolves, making it often infeasible to\nexecute all test cases with the allocated testing budgets, especially for large\nsoftware systems. Test suite minimization (TSM) is employed to improve the\nefficiency of software testing by removing redundant test cases, thus reducing\ntesting time and resources, while maintaining the fault detection capability of\nthe test suite. Most existing TSM approaches rely on code coverage (white-box)\nor model-based features, which are not always available to test engineers.\nRecent TSM approaches that rely only on test code (black-box) have been\nproposed, such as ATM and FAST-R. To address the scalability, we propose LTM\n(Language model-based Test suite Minimization), a novel, scalable, and\nblack-box similarity-based TSM approach based on large language models (LLMs),\nwhich is the first application of LLMs in the context of TSM. To support\nsimilarity measurement for test code embeddings, we investigate five\npre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder,\nand CodeLlama, on which we compute two similarity measures: Cosine Similarity\nand Euclidean Distance. Our goal is to find similarity measures that are not\nonly computationally more efficient but can also better guide a Genetic\nAlgorithm (GA) to search for optimal minimized test suites, thus reducing the\noverall search time. Experimental results show that the best configuration of\nLTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a\nslightly greater saving rate of testing time (41.72% versus 41.02%, on\naverage); (b) attaining a significantly higher fault detection rate (0.84\nversus 0.81, on average); and, most importantly, (c) minimizing test suites\nnearly five times faster on average, with higher gains for larger test suites\nand systems, thus achieving much higher scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test suites tend to grow when software evolves, making it often infeasible to\nexecute all test cases with the allocated testing budgets, especially for large\nsoftware systems. Test suite minimization (TSM) is employed to improve the\nefficiency of software testing by removing redundant test cases, thus reducing\ntesting time and resources, while maintaining the fault detection capability of\nthe test suite. Most existing TSM approaches rely on code coverage (white-box)\nor model-based features, which are not always available to test engineers.\nRecent TSM approaches that rely only on test code (black-box) have been\nproposed, such as ATM and FAST-R. To address the scalability, we propose LTM\n(Language model-based Test suite Minimization), a novel, scalable, and\nblack-box similarity-based TSM approach based on large language models (LLMs),\nwhich is the first application of LLMs in the context of TSM. To support\nsimilarity measurement for test code embeddings, we investigate five\npre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder,\nand CodeLlama, on which we compute two similarity measures: Cosine Similarity\nand Euclidean Distance. Our goal is to find similarity measures that are not\nonly computationally more efficient but can also better guide a Genetic\nAlgorithm (GA) to search for optimal minimized test suites, thus reducing the\noverall search time. Experimental results show that the best configuration of\nLTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a\nslightly greater saving rate of testing time (41.72% versus 41.02%, on\naverage); (b) attaining a significantly higher fault detection rate (0.84\nversus 0.81, on average); and, most importantly, (c) minimizing test suites\nnearly five times faster on average, with higher gains for larger test suites\nand systems, thus achieving much higher scalability."
                },
                "authors": [
                    {
                        "name": "Rongqi Pan"
                    },
                    {
                        "name": "Taher A. Ghaleb"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1109/TSE.2024.3469582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TSE.2024.3469582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.01397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.01397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15339v2",
                "updated": "2024-09-30T20:18:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    20,
                    18,
                    27,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-27T18:04:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    4,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function"
                },
                "summary": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Bin Bi"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Shiva Kumar Pentyala"
                    },
                    {
                        "name": "Zixu James Zhu"
                    },
                    {
                        "name": "Sitaram Asur"
                    },
                    {
                        "name": "Na Claire Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Na Claire Cheng"
                },
                "author": "Na Claire Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21054v2",
                "updated": "2024-09-30T19:51:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    51,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-24T12:07:54Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    7,
                    54,
                    2,
                    206,
                    0
                ],
                "title": "Sentiment Reasoning for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Reasoning for Healthcare"
                },
                "summary": "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparency in AI decision-making is crucial in healthcare due to the severe\nconsequences of errors, and this is important for building trust among AI and\nusers in sentiment analysis task. Incorporating reasoning capabilities helps\nLarge Language Models (LLMs) understand human emotions within broader contexts,\nhandle nuanced and ambiguous language, and infer underlying sentiments that may\nnot be explicitly stated. In this work, we introduce a new task - Sentiment\nReasoning - for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Our study showed that\nrationale-augmented training enhances model performance in sentiment\nclassification across both human transcript and ASR settings. Also, we found\nthat the generated rationales typically exhibit different vocabularies compared\nto human-generated rationales, but maintain similar semantics. All code, data\n(English-translated and Vietnamese) and models are published online:\nhttps://github.com/leduckhai/MultiMed"
                },
                "authors": [
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Bach Phan Tat"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Jerry Ngo"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Anh Totti Nguyen"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "Preprint, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00523v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00523v4",
                "updated": "2024-09-30T19:03:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    3,
                    11,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-01T18:19:47Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    18,
                    19,
                    47,
                    5,
                    153,
                    0
                ],
                "title": "Stealing Trust: Unraveling Blind Message Attacks in Web3 Authentication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealing Trust: Unraveling Blind Message Attacks in Web3 Authentication"
                },
                "summary": "As the field of Web3 continues its rapid expansion, the security of Web3\nauthentication, often the gateway to various Web3 applications, becomes\nincreasingly crucial. Despite its widespread use as a login method by numerous\nWeb3 applications, the security risks of Web3 authentication have not received\nmuch attention. This paper investigates the vulnerabilities in the Web3\nauthentication process and proposes a new type of attack, dubbed blind message\nattacks. In blind message attacks, attackers trick users into blindly signing\nmessages from target applications by exploiting users' inability to verify the\nsource of messages, thereby achieving unauthorized access to the target\napplication. We have developed Web3AuthChecker, a dynamic detection tool that\ninteracts with Web3 authentication-related APIs to identify vulnerabilities.\nOur evaluation of real-world Web3 applications shows that a staggering 75.8%\n(22/29) of Web3 authentication deployments are at risk of blind message\nattacks. In response to this alarming situation, we implemented Web3AuthGuard\non the open-source wallet MetaMask to alert users of potential attacks. Our\nevaluation results show that Web3AuthGuard can successfully raise alerts in 80%\nof the tested Web3 authentications. We have responsibly reported our findings\nto vulnerable websites and have been assigned two CVE IDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Web3 continues its rapid expansion, the security of Web3\nauthentication, often the gateway to various Web3 applications, becomes\nincreasingly crucial. Despite its widespread use as a login method by numerous\nWeb3 applications, the security risks of Web3 authentication have not received\nmuch attention. This paper investigates the vulnerabilities in the Web3\nauthentication process and proposes a new type of attack, dubbed blind message\nattacks. In blind message attacks, attackers trick users into blindly signing\nmessages from target applications by exploiting users' inability to verify the\nsource of messages, thereby achieving unauthorized access to the target\napplication. We have developed Web3AuthChecker, a dynamic detection tool that\ninteracts with Web3 authentication-related APIs to identify vulnerabilities.\nOur evaluation of real-world Web3 applications shows that a staggering 75.8%\n(22/29) of Web3 authentication deployments are at risk of blind message\nattacks. In response to this alarming situation, we implemented Web3AuthGuard\non the open-source wallet MetaMask to alert users of potential attacks. Our\nevaluation results show that Web3AuthGuard can successfully raise alerts in 80%\nof the tested Web3 authentications. We have responsibly reported our findings\nto vulnerable websites and have been assigned two CVE IDs."
                },
                "authors": [
                    {
                        "name": "Kailun Yan"
                    },
                    {
                        "name": "Xiaokuan Zhang"
                    },
                    {
                        "name": "Wenrui Diao"
                    }
                ],
                "author_detail": {
                    "name": "Wenrui Diao"
                },
                "author": "Wenrui Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00523v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00523v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15268v2",
                "updated": "2024-09-30T18:59:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    59,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-23T17:58:07Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    58,
                    7,
                    0,
                    267,
                    0
                ],
                "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking"
                },
                "summary": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Teresa Datta"
                    },
                    {
                        "name": "Sanjana Nambiar"
                    },
                    {
                        "name": "Raz Besaleli"
                    },
                    {
                        "name": "Samuel Dooley"
                    },
                    {
                        "name": "Max Cembalest"
                    },
                    {
                        "name": "John P. Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P. Dickerson"
                },
                "author": "John P. Dickerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20566v1",
                "updated": "2024-09-30T17:59:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning"
                },
                "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development."
                },
                "authors": [
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Aleksei Timofeev"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Jean-Philippe Fauconnier"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20565v1",
                "updated": "2024-09-30T17:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    59,
                    33,
                    0,
                    274,
                    0
                ],
                "title": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation\n  of LLM-Generated Medical Explanatory Arguments"
                },
                "summary": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts."
                },
                "authors": [
                    {
                        "name": "Iker De la Iglesia"
                    },
                    {
                        "name": "Iakes Goenaga"
                    },
                    {
                        "name": "Johanna Ramirez-Romero"
                    },
                    {
                        "name": "Jose Maria Villa-Gonzalez"
                    },
                    {
                        "name": "Josu Goikoetxea"
                    },
                    {
                        "name": "Ander Barrena"
                    }
                ],
                "author_detail": {
                    "name": "Ander Barrena"
                },
                "author": "Ander Barrena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20557v1",
                "updated": "2024-09-30T17:57:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:57:28Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    57,
                    28,
                    0,
                    274,
                    0
                ],
                "title": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in\n  Instructional Videos"
                },
                "summary": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented planning, or anticipating a series of actions that transition\nan agent from its current state to a predefined objective, is crucial for\ndeveloping intelligent assistants aiding users in daily procedural tasks. The\nproblem presents significant challenges due to the need for comprehensive\nknowledge of temporal and hierarchical task structures, as well as strong\ncapabilities in reasoning and planning. To achieve this, prior work typically\nrelies on extensive training on the target dataset, which often results in\nsignificant dataset bias and a lack of generalization to unseen tasks. In this\nwork, we introduce VidAssist, an integrated framework designed for\nzero/few-shot goal-oriented planning in instructional videos. VidAssist\nleverages large language models (LLMs) as both the knowledge base and the\nassessment tool for generating and evaluating action plans, thus overcoming the\nchallenges of acquiring procedural knowledge from small-scale, low-diversity\ndatasets. Moreover, VidAssist employs a breadth-first search algorithm for\noptimal plan generation, in which a composite of value functions designed for\ngoal-oriented planning is utilized to assess the predicted actions at each\nstep. Extensive experiments demonstrate that VidAssist offers a unified\nframework for different goal-oriented planning setups, e.g., visual planning\nfor assistance (VPA) and procedural planning (PP), and achieves remarkable\nperformance in zero-shot and few-shot setups. Specifically, our few-shot model\noutperforms the prior fully supervised state-of-the-art method by +7.7% in VPA\nand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,\nand models are publicly available at https://sites.google.com/view/vidassist."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Fu-Jen Chu"
                    },
                    {
                        "name": "Kris Kitani"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Xitong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Yang"
                },
                "author": "Xitong Yang",
                "arxiv_comment": "Accepted by ECCV 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20550v1",
                "updated": "2024-09-30T17:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,\n  and Mitigation"
                },
                "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination"
                },
                "authors": [
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11634v2",
                "updated": "2024-09-30T17:51:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    51,
                    11,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-17T15:14:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    14,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance"
                },
                "summary": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter."
                },
                "authors": [
                    {
                        "name": "Kyle Moore"
                    },
                    {
                        "name": "Jesse Roberts"
                    },
                    {
                        "name": "Thao Pham"
                    },
                    {
                        "name": "Oseremhen Ewaleifoh"
                    },
                    {
                        "name": "Doug Fisher"
                    }
                ],
                "author_detail": {
                    "name": "Doug Fisher"
                },
                "author": "Doug Fisher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20548v1",
                "updated": "2024-09-30T17:49:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:49:09Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "title": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robi Butler: Remote Multimodal Interactions with Household Robot\n  Assistant"
                },
                "summary": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements."
                },
                "authors": [
                    {
                        "name": "Anxing Xiao"
                    },
                    {
                        "name": "Nuwan Janaka"
                    },
                    {
                        "name": "Tianrun Hu"
                    },
                    {
                        "name": "Anshul Gupta"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Cunjun Yu"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01261v2",
                "updated": "2024-09-30T17:39:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    39,
                    59,
                    0,
                    274,
                    0
                ],
                "published": "2024-04-01T17:33:38Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    33,
                    38,
                    0,
                    92,
                    0
                ],
                "title": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization"
                },
                "summary": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook."
                },
                "authors": [
                    {
                        "name": "Yekyung Kim"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Aparna Garimella"
                    },
                    {
                        "name": "Varun Manjunatha"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "preprint - 39 pages",
                "arxiv_journal_ref": "1st Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v4",
                "updated": "2024-09-30T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    37,
                    16,
                    0,
                    274,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) identifies a text's position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs)\nGPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the only\nexisting work, Target-Stance Extraction (TSE), which benefits from predefined\ntargets. Unlike TSE, OTSD removes the dependency of a predefined list, making\ntarget generation and evaluation more challenging. We also provide a metric for\nevaluating target quality that correlates well with human judgment. Our\nexperiments reveal that LLMs outperform TSE in target generation when the real\ntarget is explicitly and not explicitly mentioned in the text. Likewise, for\nstance detection, LLMs excel in explicit cases with comparable performance in\nnon-explicit in general."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "14 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v7",
                "updated": "2024-09-30T17:22:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    22,
                    1,
                    0,
                    274,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20521v1",
                "updated": "2024-09-30T17:21:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    21,
                    15,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:21:15Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    21,
                    15,
                    0,
                    274,
                    0
                ],
                "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics\n  Reinforcement Learning"
                },
                "summary": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$."
                },
                "authors": [
                    {
                        "name": "Zhishuai Liu"
                    },
                    {
                        "name": "Weixin Wang"
                    },
                    {
                        "name": "Pan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Pan Xu"
                },
                "author": "Pan Xu",
                "arxiv_comment": "48 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20512v1",
                "updated": "2024-09-30T17:13:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:13:40Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    13,
                    40,
                    0,
                    274,
                    0
                ],
                "title": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis\n  of Perovskite via Language Models"
                },
                "summary": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of accurately predicting toxicity of industrial solvents used\nin perovskite synthesis is a necessary undertaking but is limited by a lack of\na targeted and structured toxicity data. This paper presents a novel framework\nthat combines an automated data extraction using language models, and an\nuncertainty-informed prediction model to fill data gaps and improve prediction\nconfidence. First, we have utilized and compared two approaches to\nautomatically extract relevant data from a corpus of scientific literature on\nsolvents used in perovskite synthesis: smaller bidirectional language models\nlike BERT and ELMo are used for their repeatability and deterministic outputs,\nwhile autoregressive large language model (LLM) such as GPT-3.5 is used to\nleverage its larger training corpus and better response generation. Our novel\n'prompting and verification' technique integrated with an LLM aims at targeted\nextraction and refinement, thereby reducing hallucination and improving the\nquality of the extracted data using the LLM. Next, the extracted data is fed\ninto our pre-trained multi-task binary classification deep learning to predict\nthe ED nature of extracted solvents. We have used a Shannon entropy-based\nuncertainty quantification utilizing the class probabilities obtained from the\nclassification model to quantify uncertainty and identify data gaps in our\npredictions. This approach leads to the curation of a structured dataset for\nsolvents used in perovskite synthesis and their uncertainty-informed virtual\ntoxicity assessment. Additionally, chord diagrams have been used to visualize\nsolvent interactions and prioritize those with potential hazards, revealing\nthat 70% of the solvent interactions were primarily associated with two\nspecific perovskites."
                },
                "authors": [
                    {
                        "name": "Arpan Mukherjee"
                    },
                    {
                        "name": "Deepesh Giri"
                    },
                    {
                        "name": "Krishna Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Rajan"
                },
                "author": "Krishna Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17041v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17041v3",
                "updated": "2024-09-30T17:12:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    12,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2023-11-28T18:53:06Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    18,
                    53,
                    6,
                    1,
                    332,
                    0
                ],
                "title": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties"
                },
                "summary": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}."
                },
                "authors": [
                    {
                        "name": "Keunwoo Peter Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Fengyuan Hu"
                    },
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "16 pages, LaTeX; Accepted to EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17041v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20502v1",
                "updated": "2024-09-30T17:02:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T17:02:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    2,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLLAGE: Collaborative Human-Agent Interaction Generation using\n  Hierarchical Latent Diffusion and Language Models"
                },
                "summary": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision."
                },
                "authors": [
                    {
                        "name": "Divyanshu Daiya"
                    },
                    {
                        "name": "Damon Conover"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "author": "Aniket Bera",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01270v2",
                "updated": "2024-09-30T16:39:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    39,
                    51,
                    0,
                    274,
                    0
                ],
                "published": "2024-07-01T13:21:33Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    21,
                    33,
                    0,
                    183,
                    0
                ],
                "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation"
                },
                "summary": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured."
                },
                "authors": [
                    {
                        "name": "Serene Lim"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "María Pérez-Ortiz"
                },
                "author": "María Pérez-Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03843v3",
                "updated": "2024-09-30T16:16:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    16,
                    4,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-06T08:21:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    8,
                    21,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning\n  of Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts."
                },
                "authors": [
                    {
                        "name": "Jianben He"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08648v2",
                "updated": "2024-09-30T16:15:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    15,
                    40,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-12T21:17:02Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    21,
                    17,
                    2,
                    2,
                    164,
                    0
                ],
                "title": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large\n  Language Models"
                },
                "summary": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When humans create sculptures, we are able to reason about how geometrically\nwe need to alter the clay state to reach our target goal. We are not computing\npoint-wise similarity metrics, or reasoning about low-level positioning of our\ntools, but instead determining the higher-level changes that need to be made.\nIn this work, we propose LLM-Craft, a novel pipeline that leverages large\nlanguage models (LLMs) to iteratively reason about and generate\ndeformation-based crafting action sequences. We simplify and couple the state\nand action representations to further encourage shape-based reasoning. To the\nbest of our knowledge, LLM-Craft is the first system successfully leveraging\nLLMs for complex deformable object interactions. Through our experiments, we\ndemonstrate that with the LLM-Craft framework, LLMs are able to successfully\nreason about the deformation behavior of elasto-plastic objects. Furthermore,\nwe find that LLM-Craft is able to successfully create a set of simple letter\nshapes. Finally, we explore extending the framework to reaching more ambiguous\nsemantic goals, such as \"thinner\" or \"bumpy\". For videos please see our\nwebsite: https://sites.google.com/andrew.cmu.edu/llmcraft."
                },
                "authors": [
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20426v1",
                "updated": "2024-09-30T15:50:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    50,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:50:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    50,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Navigating Threats: A Survey of Physical Adversarial Attacks on LiDAR\n  Perception Systems in Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Threats: A Survey of Physical Adversarial Attacks on LiDAR\n  Perception Systems in Autonomous Vehicles"
                },
                "summary": "Autonomous vehicles (AVs) rely heavily on LiDAR (Light Detection and Ranging)\nsystems for accurate perception and navigation, providing high-resolution 3D\nenvironmental data that is crucial for object detection and classification.\nHowever, LiDAR systems are vulnerable to adversarial attacks, which pose\nsignificant challenges to the safety and robustness of AVs. This survey\npresents a thorough review of the current research landscape on physical\nadversarial attacks targeting LiDAR-based perception systems, covering both\nsingle-modality and multi-modality contexts. We categorize and analyze various\nattack types, including spoofing and physical adversarial object attacks,\ndetailing their methodologies, impacts, and potential real-world implications.\nThrough detailed case studies and analyses, we identify critical challenges and\nhighlight gaps in existing attacks for LiDAR-based systems. Additionally, we\npropose future research directions to enhance the security and resilience of\nthese systems, ultimately contributing to the safer deployment of autonomous\nvehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles (AVs) rely heavily on LiDAR (Light Detection and Ranging)\nsystems for accurate perception and navigation, providing high-resolution 3D\nenvironmental data that is crucial for object detection and classification.\nHowever, LiDAR systems are vulnerable to adversarial attacks, which pose\nsignificant challenges to the safety and robustness of AVs. This survey\npresents a thorough review of the current research landscape on physical\nadversarial attacks targeting LiDAR-based perception systems, covering both\nsingle-modality and multi-modality contexts. We categorize and analyze various\nattack types, including spoofing and physical adversarial object attacks,\ndetailing their methodologies, impacts, and potential real-world implications.\nThrough detailed case studies and analyses, we identify critical challenges and\nhighlight gaps in existing attacks for LiDAR-based systems. Additionally, we\npropose future research directions to enhance the security and resilience of\nthese systems, ultimately contributing to the safer deployment of autonomous\nvehicles."
                },
                "authors": [
                    {
                        "name": "Amira Guesmi"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14744v2",
                "updated": "2024-09-30T15:36:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    36,
                    26,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-23T06:42:21Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    42,
                    21,
                    0,
                    267,
                    0
                ],
                "title": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINKAGE: Listwise Ranking among Varied-Quality References for\n  Non-Factoid QA Evaluation via LLMs"
                },
                "summary": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to\ndiverse potential answers and no objective criterion. The commonly used\nautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measure\nsemantic similarities or answers from different perspectives. Recently, Large\nLanguage Models (LLMs) have been resorted to for NFQA evaluation due to their\ncompelling performance on various NLP tasks. Common approaches include\npointwise scoring of each candidate answer and pairwise comparisons between\nanswers. Inspired by the evolution from pointwise to pairwise to listwise in\nlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,\nthat utilizes LLMs to rank candidate answers in a list of reference answers\nsorted by descending quality. Moreover, for NF questions that do not have\nmulti-grade or any golden answers, we leverage LLMs to generate the reference\nanswer list of various quality to facilitate the listwise evaluation. Extensive\nexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and\nWebGLM show that our method has significantly higher correlations with human\nannotations compared to automatic scores and common pointwise and pairwise\napproaches."
                },
                "authors": [
                    {
                        "name": "Sihui Yang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Wanqing Cui"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Published as a conference paper at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20385v1",
                "updated": "2024-09-30T15:20:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:20:58Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    20,
                    58,
                    0,
                    274,
                    0
                ],
                "title": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wait, but Tylenol is Acetaminophen... Investigating and Improving\n  Language Models' Ability to Resist Requests for Misinformation"
                },
                "summary": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) are trained to follow directions,\nbut this introduces a vulnerability to blindly comply with user requests even\nif they generate wrong information. In medicine, this could accelerate the\ngeneration of misinformation that impacts human well-being.\n  Objectives/Methods: We analyzed compliance to requests to generate misleading\ncontent about medications in settings where models know the request is\nillogical. We investigated whether in-context directions and instruction-tuning\nof LLMs to prioritize logical reasoning over compliance reduced misinformation\nrisk.\n  Results: While all frontier LLMs complied with misinformation requests, both\nprompt-based and parameter-based approaches can improve the detection of logic\nflaws in requests and prevent the dissemination of medical misinformation.\n  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce\nrisks of exploitation for medical misinformation."
                },
                "authors": [
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Brian Anthony"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Hugo Aerts"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Danielle Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle Bitterman"
                },
                "author": "Danielle Bitterman",
                "arxiv_comment": "Submitted for Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20370v1",
                "updated": "2024-09-30T15:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    6,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Perfect Blend: Redefining RLHF with Mixture of Judges"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications."
                },
                "authors": [
                    {
                        "name": "Tengyu Xu"
                    },
                    {
                        "name": "Eryk Helenowski"
                    },
                    {
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Kaiyan Peng"
                    },
                    {
                        "name": "Eric Han"
                    },
                    {
                        "name": "Shaoliang Nie"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Hejia Zhang"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Zhouhao Zeng"
                    },
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Karishma Mandyam"
                    },
                    {
                        "name": "Arya Talabzadeh"
                    },
                    {
                        "name": "Madian Khabsa"
                    },
                    {
                        "name": "Gabriel Cohen"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Han Fang"
                    }
                ],
                "author_detail": {
                    "name": "Han Fang"
                },
                "author": "Han Fang",
                "arxiv_comment": "submitted to conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20365v1",
                "updated": "2024-09-30T15:04:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:04:14Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    4,
                    14,
                    0,
                    274,
                    0
                ],
                "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoINSTA: Zero-shot Long Video Understanding via Informative\n  Spatial-Temporal Reasoning with LLMs"
                },
                "summary": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the video-language domain, recent works in leveraging zero-shot Large\nLanguage Model-based reasoning for video understanding have become competitive\nchallengers to previous end-to-end models. However, long video understanding\npresents unique challenges due to the complexity of reasoning over extended\ntimespans, even for zero-shot LLM-based approaches. The challenge of\ninformation redundancy in long videos prompts the question of what specific\ninformation is essential for large language models (LLMs) and how to leverage\nthem for complex spatial-temporal reasoning in long-form video analysis. We\npropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for\nzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot\nframework for long video understanding using LLMs; (2) an event-based temporal\nreasoning and content-based spatial reasoning approach for LLMs to reason over\nspatial-temporal information in videos; (3) a self-reflective information\nreasoning scheme balancing temporal factors based on information sufficiency\nand prediction confidence. Our model significantly improves the\nstate-of-the-art on three long video question-answering benchmarks: EgoSchema,\nNextQA, and IntentQA, and the open question answering dataset ActivityNetQA.\nThe code is released here: https://github.com/mayhugotong/VideoINSTA."
                },
                "authors": [
                    {
                        "name": "Ruotong Liao"
                    },
                    {
                        "name": "Max Erler"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "EMNLP 2024 Findings; 22 pages; Code:\n  https://github.com/mayhugotong/VideoINSTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20364v1",
                "updated": "2024-09-30T15:03:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:03:55Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    3,
                    55,
                    0,
                    274,
                    0
                ],
                "title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using\n  Large Language Models"
                },
                "summary": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks."
                },
                "authors": [
                    {
                        "name": "Yizhou Huang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kezhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Wang"
                },
                "author": "Kezhi Wang",
                "arxiv_comment": "Submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v3",
                "updated": "2024-09-30T14:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    54,
                    17,
                    0,
                    274,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "EMNLP 2024 Main, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08424v2",
                "updated": "2024-09-30T14:25:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    25,
                    39,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-13T11:16:43Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    11,
                    16,
                    43,
                    2,
                    73,
                    0
                ],
                "title": "Distract Large Language Models for Automatic Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distract Large Language Models for Automatic Jailbreak Attack"
                },
                "summary": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies."
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]