[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v1",
                "updated": "2025-09-08T00:57:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v2",
                "updated": "2025-09-10T05:02:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    5,
                    2,
                    14,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.07980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07980v1",
                "updated": "2025-09-09T17:59:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
                },
                "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Huiwen Bao"
                    },
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13844v3",
                "updated": "2025-09-09T17:58:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    58,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2024-05-22T17:13:47Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    13,
                    47,
                    2,
                    143,
                    0
                ],
                "title": "Counterfactual Cocycles: A Framework for Robust and Coherent\n  Counterfactual Transports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Cocycles: A Framework for Robust and Coherent\n  Counterfactual Transports"
                },
                "summary": "Estimating joint distributions (a.k.a. couplings) over counterfactual\noutcomes is central to personalized decision-making and treatment risk\nassessment. Two emergent frameworks with identifiability guarantees are: (i)\nbijective structural causal models (SCMs), which are flexible but brittle to\nmis-specified latent noise; and (ii) optimal-transport (OT) methods, which\navoid latent noise assumptions but can produce incoherent counterfactual\ntransports which fail to identify higher-order couplings. In this work, we\nbridge the gap with \\emph{counterfactual cocycles}: a framework for\ncounterfactual transports that use algebraic structure to provide coherence and\nidentifiability guarantees. Every counterfactual cocycle corresponds to an\nequivalence class of SCMs, however the cocycle is invariant to the latent noise\ndistribution, enabling us to sidestep various mis-specification problems. We\ncharacterize the structure of all identifiable counterfactual cocycles; propose\nflexible model parameterizations; introduce a novel cocycle estimator that\navoids any distributional assumptions; and derive mis-specification robustness\nproperties of the resulting counterfactual inference method. We demonstrate\nstate-of-the-art performance and noise-robustness of counterfactual cocycles\nacross synthetic benchmarks and a 401(k) eligibility study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating joint distributions (a.k.a. couplings) over counterfactual\noutcomes is central to personalized decision-making and treatment risk\nassessment. Two emergent frameworks with identifiability guarantees are: (i)\nbijective structural causal models (SCMs), which are flexible but brittle to\nmis-specified latent noise; and (ii) optimal-transport (OT) methods, which\navoid latent noise assumptions but can produce incoherent counterfactual\ntransports which fail to identify higher-order couplings. In this work, we\nbridge the gap with \\emph{counterfactual cocycles}: a framework for\ncounterfactual transports that use algebraic structure to provide coherence and\nidentifiability guarantees. Every counterfactual cocycle corresponds to an\nequivalence class of SCMs, however the cocycle is invariant to the latent noise\ndistribution, enabling us to sidestep various mis-specification problems. We\ncharacterize the structure of all identifiable counterfactual cocycles; propose\nflexible model parameterizations; introduce a novel cocycle estimator that\navoids any distributional assumptions; and derive mis-specification robustness\nproperties of the resulting counterfactual inference method. We demonstrate\nstate-of-the-art performance and noise-robustness of counterfactual cocycles\nacross synthetic benchmarks and a 401(k) eligibility study."
                },
                "authors": [
                    {
                        "name": "Hugh Dance"
                    },
                    {
                        "name": "Benjamin Bloem-Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Bloem-Reddy"
                },
                "author": "Benjamin Bloem-Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07969v1",
                "updated": "2025-09-09T17:54:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    54,
                    21,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:54:21Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    54,
                    21,
                    1,
                    252,
                    0
                ],
                "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search"
                },
                "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems."
                },
                "authors": [
                    {
                        "name": "Xin Lai"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Tao Liu"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "Code, datasets, models are available at\n  https://github.com/Mini-o3/Mini-o3. Project Page: https://mini-o3.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07968v1",
                "updated": "2025-09-09T17:53:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    53,
                    58,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:53:58Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    53,
                    58,
                    1,
                    252,
                    0
                ],
                "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge"
                },
                "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified."
                },
                "authors": [
                    {
                        "name": "Lukas Haas"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Giovanni D'Antonio"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Dipanjan Das"
                    }
                ],
                "author_detail": {
                    "name": "Dipanjan Das"
                },
                "author": "Dipanjan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07966v1",
                "updated": "2025-09-09T17:52:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    52,
                    26,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:52:26Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    52,
                    26,
                    1,
                    252,
                    0
                ],
                "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images"
                },
                "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA."
                },
                "authors": [
                    {
                        "name": "Boammani Aser Lompo"
                    },
                    {
                        "name": "Marc Haraoui"
                    }
                ],
                "author_detail": {
                    "name": "Marc Haraoui"
                },
                "author": "Marc Haraoui",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07964v1",
                "updated": "2025-09-09T17:51:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    51,
                    35,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:51:35Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    51,
                    35,
                    1,
                    252,
                    0
                ],
                "title": "Dark Energy Survey Year 6 Results: Redshift Calibration of the MagLim++\n  Lens Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark Energy Survey Year 6 Results: Redshift Calibration of the MagLim++\n  Lens Sample"
                },
                "summary": "In this work, we derive and calibrate the redshift distribution of the\nMagLim++ lens galaxy sample used in the Dark Energy Survey Year 6 (DES Y6)\n3x2pt cosmology analysis. The 3x2pt analysis combines galaxy clustering from\nthe lens galaxy sample and weak gravitational lensing. The redshift\ndistributions are inferred using the SOMPZ method - a Self-Organizing Map\nframework that combines deep-field multi-band photometry, wide-field data, and\na synthetic source injection (Balrog) catalog. Key improvements over the DES\nYear 3 (Y3) calibration include a noise-weighted SOM metric, an expanded Balrog\ncatalogue, and an improved scheme for propagating systematic uncertainties,\nwhich allows us to generate O($10^8$) redshift realizations that collectively\nspan the dominant sources of uncertainty. These realizations are then combined\nwith independent clustering-redshift measurements via importance sampling. The\nresulting calibration achieves typical uncertainties on the mean redshift of\n1-2%, corresponding to a 20-30% average reduction relative to DES Y3. We\ncompress the $n(z)$ uncertainties into a small number of orthogonal modes for\nuse in cosmological inference. Marginalizing over these modes leads to only a\nminor degradation in cosmological constraints. This analysis establishes the\nMagLim++ sample as a robust lens sample for precision cosmology with DES Y6 and\nprovides a scalable framework for future surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we derive and calibrate the redshift distribution of the\nMagLim++ lens galaxy sample used in the Dark Energy Survey Year 6 (DES Y6)\n3x2pt cosmology analysis. The 3x2pt analysis combines galaxy clustering from\nthe lens galaxy sample and weak gravitational lensing. The redshift\ndistributions are inferred using the SOMPZ method - a Self-Organizing Map\nframework that combines deep-field multi-band photometry, wide-field data, and\na synthetic source injection (Balrog) catalog. Key improvements over the DES\nYear 3 (Y3) calibration include a noise-weighted SOM metric, an expanded Balrog\ncatalogue, and an improved scheme for propagating systematic uncertainties,\nwhich allows us to generate O($10^8$) redshift realizations that collectively\nspan the dominant sources of uncertainty. These realizations are then combined\nwith independent clustering-redshift measurements via importance sampling. The\nresulting calibration achieves typical uncertainties on the mean redshift of\n1-2%, corresponding to a 20-30% average reduction relative to DES Y3. We\ncompress the $n(z)$ uncertainties into a small number of orthogonal modes for\nuse in cosmological inference. Marginalizing over these modes leads to only a\nminor degradation in cosmological constraints. This analysis establishes the\nMagLim++ sample as a robust lens sample for precision cosmology with DES Y6 and\nprovides a scalable framework for future surveys."
                },
                "authors": [
                    {
                        "name": "G. Giannini"
                    },
                    {
                        "name": "A. Alarcon"
                    },
                    {
                        "name": "W. d'Assignies"
                    },
                    {
                        "name": "G. M. Bernstein"
                    },
                    {
                        "name": "M. A. Troxel"
                    },
                    {
                        "name": "C. Chang"
                    },
                    {
                        "name": "B. Yin"
                    },
                    {
                        "name": "A. Amon"
                    },
                    {
                        "name": "J. Myles"
                    },
                    {
                        "name": "N. Weaverdyck"
                    },
                    {
                        "name": "A. Porredon"
                    },
                    {
                        "name": "D. Anbajagane"
                    },
                    {
                        "name": "S. Avila"
                    },
                    {
                        "name": "K. Bechtol"
                    },
                    {
                        "name": "M. R. Becker"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "M. Crocce"
                    },
                    {
                        "name": "D. Gruen"
                    },
                    {
                        "name": "M. Rodriguez-Monroy"
                    },
                    {
                        "name": "C. Sánchez"
                    },
                    {
                        "name": "D. Sanchez Cid"
                    },
                    {
                        "name": "I. Sevilla-Noarbe"
                    },
                    {
                        "name": "M. Aguena"
                    },
                    {
                        "name": "S. Allam"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "F. Andrade-Oliveira"
                    },
                    {
                        "name": "D. Bacon"
                    },
                    {
                        "name": "S. Bocquet"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "R. Camilleri"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "R. Cawthon"
                    },
                    {
                        "name": "L. N. da Costa"
                    },
                    {
                        "name": "M. E. da Silva Pereira"
                    },
                    {
                        "name": "T. M. Davis"
                    },
                    {
                        "name": "J. De Vicente"
                    },
                    {
                        "name": "D. L. DePoy"
                    },
                    {
                        "name": "S. Desai"
                    },
                    {
                        "name": "H. T. Diehl"
                    },
                    {
                        "name": "S. Dodelson"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "C. Doux"
                    },
                    {
                        "name": "A. Drlica-Wagner"
                    },
                    {
                        "name": "J. Elvin-Poole"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "A. E. Evrard"
                    },
                    {
                        "name": "B. Flaugher"
                    },
                    {
                        "name": "J. Frieman"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "P. Giles"
                    },
                    {
                        "name": "R. A. Gruendl"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "K. Herner"
                    },
                    {
                        "name": "S. R. Hinton"
                    },
                    {
                        "name": "D. L. Hollowood"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. Huterer"
                    },
                    {
                        "name": "D. J. James"
                    },
                    {
                        "name": "K. Kuehn"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "H. Lin"
                    },
                    {
                        "name": "J. L. Marshall"
                    },
                    {
                        "name": "J. Mena-Fernández"
                    },
                    {
                        "name": "F. Menanteau"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Muir"
                    },
                    {
                        "name": "R. L. C. Ogando"
                    },
                    {
                        "name": "D. Petravick"
                    },
                    {
                        "name": "A. A. Plazas Malagón"
                    },
                    {
                        "name": "J. Prat"
                    },
                    {
                        "name": "M. Raveri"
                    },
                    {
                        "name": "E. S. Rykoff"
                    },
                    {
                        "name": "S. Samuroff"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "T. Shin"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "E. Suchyta"
                    },
                    {
                        "name": "M. E. C. Swanson"
                    },
                    {
                        "name": "G. Tarle"
                    },
                    {
                        "name": "D. Thomas"
                    },
                    {
                        "name": "C. To"
                    },
                    {
                        "name": "D. L. Tucker"
                    },
                    {
                        "name": "V. Vikram"
                    },
                    {
                        "name": "M. Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "M. Yamamoto"
                },
                "author": "M. Yamamoto",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21627v2",
                "updated": "2025-09-09T17:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    37,
                    26,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-27T18:02:12Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    2,
                    12,
                    1,
                    147,
                    0
                ],
                "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives"
                },
                "summary": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we develop an efficient heuristic algorithm that allows\nproviders to significantly overcharge users without raising suspicion.\nCrucially, we demonstrate that the cost of running the algorithm is lower than\nthe additional revenue from overcharging users, highlighting the vulnerability\nof users under the current pay-per-token pricing mechanism. Further, we show\nthat, to eliminate the financial incentive to strategize, a pricing mechanism\nmust price tokens linearly on their character count. While this makes a\nprovider's profit margin vary across tokens, we introduce a simple prescription\nunder which the provider who adopts such an incentive-compatible pricing\nmechanism can maintain the average profit margin they had under the\npay-per-token pricing mechanism. Along the way, to illustrate and complement\nour theoretical results, we conduct experiments with several large language\nmodels from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$\nfamilies, and input prompts from the LMSYS Chatbot Arena platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we develop an efficient heuristic algorithm that allows\nproviders to significantly overcharge users without raising suspicion.\nCrucially, we demonstrate that the cost of running the algorithm is lower than\nthe additional revenue from overcharging users, highlighting the vulnerability\nof users under the current pay-per-token pricing mechanism. Further, we show\nthat, to eliminate the financial incentive to strategize, a pricing mechanism\nmust price tokens linearly on their character count. While this makes a\nprovider's profit margin vary across tokens, we introduce a simple prescription\nunder which the provider who adopts such an incentive-compatible pricing\nmechanism can maintain the average profit margin they had under the\npay-per-token pricing mechanism. Along the way, to illustrate and complement\nour theoretical results, we conduct experiments with several large language\nmodels from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$\nfamilies, and input prompts from the LMSYS Chatbot Arena platform."
                },
                "authors": [
                    {
                        "name": "Ander Artola Velasco"
                    },
                    {
                        "name": "Stratis Tsirtsis"
                    },
                    {
                        "name": "Nastaran Okati"
                    },
                    {
                        "name": "Manuel Gomez-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gomez-Rodriguez"
                },
                "author": "Manuel Gomez-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07941v1",
                "updated": "2025-09-09T17:21:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    21,
                    20,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:21:20Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    21,
                    20,
                    1,
                    252,
                    0
                ],
                "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented\n  Code Generation"
                },
                "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io."
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Chenxiong Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chenxiong Qian"
                },
                "author": "Chenxiong Qian",
                "arxiv_comment": "This paper has been accepted by the ACM Conference on Computer and\n  Communications Security (CCS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07939v1",
                "updated": "2025-09-09T17:19:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    33,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:19:33Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    33,
                    1,
                    252,
                    0
                ],
                "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured\n  Attack Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured\n  Attack Trees"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"
                },
                "authors": [
                    {
                        "name": "Katsuaki Nakano"
                    },
                    {
                        "name": "Reza Feyyazi"
                    },
                    {
                        "name": "Shanchieh Jay Yang"
                    },
                    {
                        "name": "Michael Zuzak"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zuzak"
                },
                "author": "Michael Zuzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07938v1",
                "updated": "2025-09-09T17:19:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    9,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:19:09Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    9,
                    1,
                    252,
                    0
                ],
                "title": "The JADE code. II. Modeling the coupled orbital and atmospheric\n  evolution of GJ 436 b to constrain its migration and companion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The JADE code. II. Modeling the coupled orbital and atmospheric\n  evolution of GJ 436 b to constrain its migration and companion"
                },
                "summary": "The observed architecture and modeled evolution of close-in exoplanets\nprovide crucial insights into their formation pathways and survival mechanisms.\nTo investigate these fundamental questions, we employed JADE, a comprehensive\nnumerical code that models the coupled evolution of atmospheres and dynamics\nover secular timescales, rooted in present-day observations. JADE integrates\nphotoevaporation with migration driven by von Zeipel-Lidov-Kozai (ZLK) cycles\nfrom an external perturber, allowing us to explore evolutionary scenarios where\ndynamical and atmospheric processes influence each other. Here, we specifically\nconsidered GJ 436 b, a warm Neptune with an eccentric orbit and polar\nspin-orbit angle that has survived within the \"hot Neptune desert\" despite\nongoing atmospheric escape. Our extensive exploration included over 500 000\nsimulations in a framework that combines precomputed grids with Bayesian\ninference. This allowed us to constrain GJ 436 b's initial conditions and the\nproperties of its putative companion within a ZLK hypothesis. Our results\nsuggest that GJ 436 b formed at ~ 0.3 AU and, despite its current substantial\natmospheric erosion, has experienced minimal cumulative mass loss throughout\nits history, thanks to a late inward migration triggered by a distant companion\ninducing ZLK oscillations. We find that initial mutual inclinations of 80{\\deg}\n- 100{\\deg} with this companion best reproduce the observed polar orbit. By\ncombining our explored constraints with radial velocity detection limits, we\nidentified the viable parameter space for the hypothetical GJ 436 c. We found\nthat it strongly disfavors stellar and brown dwarf masses, which offers a\nuseful guide for future observational searches. This work demonstrates how\ncoupled modeling can shed light on the interplay shaping close-in exoplanets\nand explain the survival of volatile-rich worlds near the edges of the desert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observed architecture and modeled evolution of close-in exoplanets\nprovide crucial insights into their formation pathways and survival mechanisms.\nTo investigate these fundamental questions, we employed JADE, a comprehensive\nnumerical code that models the coupled evolution of atmospheres and dynamics\nover secular timescales, rooted in present-day observations. JADE integrates\nphotoevaporation with migration driven by von Zeipel-Lidov-Kozai (ZLK) cycles\nfrom an external perturber, allowing us to explore evolutionary scenarios where\ndynamical and atmospheric processes influence each other. Here, we specifically\nconsidered GJ 436 b, a warm Neptune with an eccentric orbit and polar\nspin-orbit angle that has survived within the \"hot Neptune desert\" despite\nongoing atmospheric escape. Our extensive exploration included over 500 000\nsimulations in a framework that combines precomputed grids with Bayesian\ninference. This allowed us to constrain GJ 436 b's initial conditions and the\nproperties of its putative companion within a ZLK hypothesis. Our results\nsuggest that GJ 436 b formed at ~ 0.3 AU and, despite its current substantial\natmospheric erosion, has experienced minimal cumulative mass loss throughout\nits history, thanks to a late inward migration triggered by a distant companion\ninducing ZLK oscillations. We find that initial mutual inclinations of 80{\\deg}\n- 100{\\deg} with this companion best reproduce the observed polar orbit. By\ncombining our explored constraints with radial velocity detection limits, we\nidentified the viable parameter space for the hypothetical GJ 436 c. We found\nthat it strongly disfavors stellar and brown dwarf masses, which offers a\nuseful guide for future observational searches. This work demonstrates how\ncoupled modeling can shed light on the interplay shaping close-in exoplanets\nand explain the survival of volatile-rich worlds near the edges of the desert."
                },
                "authors": [
                    {
                        "name": "M. Attia"
                    },
                    {
                        "name": "V. Bourrier"
                    },
                    {
                        "name": "E. Bolmont"
                    },
                    {
                        "name": "L. Mignon"
                    },
                    {
                        "name": "J. -B. Delisle"
                    },
                    {
                        "name": "H. Beust"
                    },
                    {
                        "name": "N. C. Hara"
                    },
                    {
                        "name": "C. Mordasini"
                    }
                ],
                "author_detail": {
                    "name": "C. Mordasini"
                },
                "author": "C. Mordasini",
                "arxiv_comment": "Accepted for publication in A&A. Abstract abridged for arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07933v1",
                "updated": "2025-09-09T17:17:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    17,
                    6,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:17:06Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    17,
                    6,
                    1,
                    252,
                    0
                ],
                "title": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation"
                },
                "summary": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security."
                },
                "authors": [
                    {
                        "name": "Wanni Vidulige Ishan Perera"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Fan liang"
                    },
                    {
                        "name": "Junyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Zhang"
                },
                "author": "Junyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07928v1",
                "updated": "2025-09-09T17:13:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    13,
                    31,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:13:31Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    13,
                    31,
                    1,
                    252,
                    0
                ],
                "title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s"
                },
                "summary": "As local AI grows in popularity, there is a critical gap between the\nbenchmark performance of object detectors and their practical viability on\nconsumer-grade hardware. While models like YOLOv10s promise real-time speeds,\nthese metrics are typically achieved on high-power, desktop-class GPUs. This\npaper reveals that on resource-constrained systems, such as laptops with RTX\n4060 GPUs, performance is not compute-bound but is instead dominated by\nsystem-level bottlenecks, as illustrated by a simple bottleneck test. To\novercome this hardware-level constraint, we introduce a Two-Pass Adaptive\nInference algorithm, a model-independent approach that requires no\narchitectural changes. This study mainly focuses on adaptive inference\nstrategies and undertakes a comparative analysis of architectural early-exit\nand resolution-adaptive routing, highlighting their respective trade-offs\nwithin a unified evaluation framework. The system uses a fast, low-resolution\npass and only escalates to a high-resolution model pass when detection\nconfidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x\nspeedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.\nThis work provides a practical and reproducible blueprint for deploying\nhigh-performance, real-time AI on consumer-grade devices by shifting the focus\nfrom pure model optimization to hardware-aware inference strategies that\nmaximize throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As local AI grows in popularity, there is a critical gap between the\nbenchmark performance of object detectors and their practical viability on\nconsumer-grade hardware. While models like YOLOv10s promise real-time speeds,\nthese metrics are typically achieved on high-power, desktop-class GPUs. This\npaper reveals that on resource-constrained systems, such as laptops with RTX\n4060 GPUs, performance is not compute-bound but is instead dominated by\nsystem-level bottlenecks, as illustrated by a simple bottleneck test. To\novercome this hardware-level constraint, we introduce a Two-Pass Adaptive\nInference algorithm, a model-independent approach that requires no\narchitectural changes. This study mainly focuses on adaptive inference\nstrategies and undertakes a comparative analysis of architectural early-exit\nand resolution-adaptive routing, highlighting their respective trade-offs\nwithin a unified evaluation framework. The system uses a fast, low-resolution\npass and only escalates to a high-resolution model pass when detection\nconfidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x\nspeedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.\nThis work provides a practical and reproducible blueprint for deploying\nhigh-performance, real-time AI on consumer-grade devices by shifting the focus\nfrom pure model optimization to hardware-aware inference strategies that\nmaximize throughput."
                },
                "authors": [
                    {
                        "name": "Mahmudul Islam Masum"
                    },
                    {
                        "name": "Miad Islam"
                    },
                    {
                        "name": "Arif I. Sarwat"
                    }
                ],
                "author_detail": {
                    "name": "Arif I. Sarwat"
                },
                "author": "Arif I. Sarwat",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07925v1",
                "updated": "2025-09-09T17:07:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    7,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:07:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    7,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models"
                },
                "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ."
                },
                "authors": [
                    {
                        "name": "Tuo Wang"
                    },
                    {
                        "name": "Adithya Kulkarni"
                    },
                    {
                        "name": "Tyler Cody"
                    },
                    {
                        "name": "Peter A. Beling"
                    },
                    {
                        "name": "Yujun Yan"
                    },
                    {
                        "name": "Dawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhou"
                },
                "author": "Dawei Zhou",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00949v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00949v5",
                "updated": "2025-09-09T17:04:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    4,
                    3,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-02T01:35:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    35,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Llama-Nemotron: Efficient Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Nemotron: Efficient Reasoning Models"
                },
                "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Alexander Bukharin"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Bilal Kartal"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Adi Renduchintala"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Makesh Narsimhan"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Rabeeh Karimi Mahabadi"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Bor-Yiing Su"
                    },
                    {
                        "name": "Guyue Huang"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Robert McQueen"
                    },
                    {
                        "name": "Izzy Putterman"
                    },
                    {
                        "name": "George Lam"
                    },
                    {
                        "name": "Arun Venkatesan"
                    },
                    {
                        "name": "Sherry Wu"
                    },
                    {
                        "name": "Vinh Nguyen"
                    },
                    {
                        "name": "Manoj Kilaru"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Anna Warno"
                    },
                    {
                        "name": "Abhilash Somasamudramath"
                    },
                    {
                        "name": "Sandip Bhaskar"
                    },
                    {
                        "name": "Maka Dong"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Scot Junkin"
                    },
                    {
                        "name": "Oleksandr Romanenko"
                    },
                    {
                        "name": "Pedro Larroy"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Marco Rovinelli"
                    },
                    {
                        "name": "Viji Balas"
                    },
                    {
                        "name": "Nicholas Edelman"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Muthu Subramaniam"
                    },
                    {
                        "name": "Smita Ithape"
                    },
                    {
                        "name": "Karthik Ramamoorthy"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Suguna Varshini Velury"
                    },
                    {
                        "name": "Omri Almog"
                    },
                    {
                        "name": "Joyjit Daw"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Nikki Pope"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Seth Schneider"
                    },
                    {
                        "name": "Guillermo Siman"
                    },
                    {
                        "name": "Tomasz Grzegorzek"
                    },
                    {
                        "name": "Pablo Ribalta"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Chris Alexiuk"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Ann Guan"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jonah Alben"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Eric Chung"
                    }
                ],
                "author_detail": {
                    "name": "Eric Chung"
                },
                "author": "Eric Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00949v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00949v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07920v1",
                "updated": "2025-09-09T17:00:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    0,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:00:42Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    0,
                    42,
                    1,
                    252,
                    0
                ],
                "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion"
                },
                "summary": "Joint reconstruction of human-object interaction marks a significant\nmilestone in comprehending the intricate interrelations between humans and\ntheir surrounding environment. Nevertheless, previous optimization methods\noften struggle to achieve physically plausible reconstruction results due to\nthe lack of prior knowledge about human-object interactions. In this paper, we\nintroduce ScoreHOI, an effective diffusion-based optimizer that introduces\ndiffusion priors for the precise recovery of human-object interactions. By\nharnessing the controllability within score-guided sampling, the diffusion\nmodel can reconstruct a conditional distribution of human and object pose given\nthe image observation and object feature. During inference, the ScoreHOI\neffectively improves the reconstruction results by guiding the denoising\nprocess with specific physical constraints. Furthermore, we propose a\ncontact-driven iterative refinement approach to enhance the contact\nplausibility and improve the reconstruction accuracy. Extensive evaluations on\nstandard benchmarks demonstrate ScoreHOI's superior performance over\nstate-of-the-art methods, highlighting its ability to achieve a precise and\nrobust improvement in joint human-object interaction reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint reconstruction of human-object interaction marks a significant\nmilestone in comprehending the intricate interrelations between humans and\ntheir surrounding environment. Nevertheless, previous optimization methods\noften struggle to achieve physically plausible reconstruction results due to\nthe lack of prior knowledge about human-object interactions. In this paper, we\nintroduce ScoreHOI, an effective diffusion-based optimizer that introduces\ndiffusion priors for the precise recovery of human-object interactions. By\nharnessing the controllability within score-guided sampling, the diffusion\nmodel can reconstruct a conditional distribution of human and object pose given\nthe image observation and object feature. During inference, the ScoreHOI\neffectively improves the reconstruction results by guiding the denoising\nprocess with specific physical constraints. Furthermore, we propose a\ncontact-driven iterative refinement approach to enhance the contact\nplausibility and improve the reconstruction accuracy. Extensive evaluations on\nstandard benchmarks demonstrate ScoreHOI's superior performance over\nstate-of-the-art methods, highlighting its ability to achieve a precise and\nrobust improvement in joint human-object interaction reconstruction."
                },
                "authors": [
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Jinpeng Liu"
                    },
                    {
                        "name": "Yixuan Zhu"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07909v1",
                "updated": "2025-09-09T16:53:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    53,
                    21,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:53:21Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    53,
                    21,
                    1,
                    252,
                    0
                ],
                "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Scaling Laws for Large Language Models via Inverse Problems"
                },
                "summary": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness."
                },
                "authors": [
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhiliang Chen"
                    },
                    {
                        "name": "Rachael Hwee Ling Sim"
                    },
                    {
                        "name": "Rui Qiao"
                    },
                    {
                        "name": "Jingtan Wang"
                    },
                    {
                        "name": "Nhung Bui"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "Zi-Yu Khoo"
                    },
                    {
                        "name": "Zitong Zhao"
                    },
                    {
                        "name": "Xinyi Xu"
                    },
                    {
                        "name": "Apivich Hemachandra"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted at EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07908v1",
                "updated": "2025-09-09T16:51:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    51,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:51:16Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    51,
                    16,
                    1,
                    252,
                    0
                ],
                "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories"
                },
                "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse."
                },
                "authors": [
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Debora Nozza"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21117v3",
                "updated": "2025-09-10T10:32:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    32,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-29T18:56:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    56,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts"
                },
                "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."
                },
                "authors": [
                    {
                        "name": "Hanhua Hong"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiqi Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "11 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07824v2",
                "updated": "2025-09-09T16:35:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    35,
                    39,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-09T14:48:43Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    14,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs"
                },
                "summary": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility."
                },
                "authors": [
                    {
                        "name": "Yao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yao Yan"
                },
                "author": "Yao Yan",
                "arxiv_comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR\n  May 2025 cycle, reviews pending)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v2",
                "updated": "2025-09-10T11:05:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    5,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07128v2",
                "updated": "2025-09-09T16:20:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    20,
                    1,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-10T23:47:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    47,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping"
                },
                "summary": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse"
                },
                "authors": [
                    {
                        "name": "Danrui Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Sam S. Sohn"
                    },
                    {
                        "name": "Kaidong Hu"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Mubbasir Kapadia"
                    }
                ],
                "author_detail": {
                    "name": "Mubbasir Kapadia"
                },
                "author": "Mubbasir Kapadia",
                "arxiv_comment": "37 pages, 13 figures, 8 tables. Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06213v2",
                "updated": "2025-09-09T16:15:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    15,
                    39,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-07T21:22:14Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    21,
                    22,
                    14,
                    6,
                    250,
                    0
                ],
                "title": "Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments\n  and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments\n  and Reinforcement Learning"
                },
                "summary": "We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)\nenvironment, a complex puzzle in which an agent must infer and execute hidden\nrules to clear a 6$\\times$6 board by placing game pieces into buckets. We\nexplore two state representation strategies, namely Feature-Centric (FC) and\nObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic\n(A2C) algorithm for training. The agent has access only to partial observations\nand must simultaneously infer the governing rule and learn the optimal policy\nthrough experience. We evaluate our models across multiple rule-based and\ntrial-list-based experimental setups, analyzing transfer effects and the impact\nof representation on learning efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)\nenvironment, a complex puzzle in which an agent must infer and execute hidden\nrules to clear a 6$\\times$6 board by placing game pieces into buckets. We\nexplore two state representation strategies, namely Feature-Centric (FC) and\nObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic\n(A2C) algorithm for training. The agent has access only to partial observations\nand must simultaneously infer the governing rule and learn the optimal policy\nthrough experience. We evaluate our models across multiple rule-based and\ntrial-list-based experimental setups, analyzing transfer effects and the impact\nof representation on learning efficiency."
                },
                "authors": [
                    {
                        "name": "Christo Mathew"
                    },
                    {
                        "name": "Wentian Wang"
                    },
                    {
                        "name": "Jacob Feldman"
                    },
                    {
                        "name": "Lazaros K. Gallos"
                    },
                    {
                        "name": "Paul B. Kantor"
                    },
                    {
                        "name": "Vladimir Menkov"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20404v2",
                "updated": "2025-09-09T16:13:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    13,
                    55,
                    1,
                    252,
                    0
                ],
                "published": "2024-05-30T18:16:41Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    16,
                    41,
                    3,
                    151,
                    0
                ],
                "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."
                },
                "authors": [
                    {
                        "name": "Yurui Chang"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Yujia Wang"
                    },
                    {
                        "name": "Jinghui Chen"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.20404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07889v1",
                "updated": "2025-09-09T16:12:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    12,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:12:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    12,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing"
                },
                "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask."
                },
                "authors": [
                    {
                        "name": "Chengyan Wu"
                    },
                    {
                        "name": "Yiqiang Cai"
                    },
                    {
                        "name": "Yufei Cheng"
                    },
                    {
                        "name": "Yun Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yun Xue"
                },
                "author": "Yun Xue",
                "arxiv_comment": "NLPCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19951v2",
                "updated": "2025-09-09T16:05:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    5,
                    18,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-25T16:28:24Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    28,
                    24,
                    1,
                    84,
                    0
                ],
                "title": "Audio-centric Video Understanding Benchmark without Text Shortcut",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-centric Video Understanding Benchmark without Text Shortcut"
                },
                "summary": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with\na particular focus on auditory information. AVUT introduces a suite of\ncarefully designed audio-centric tasks, holistically testing the understanding\nof both audio content and audio-visual interactions in videos. Moreover, this\nwork points out the text shortcut problem that largely exists in other\nbenchmarks where the correct answer can be found from question text alone\nwithout needing videos. AVUT addresses this problem by proposing a answer\npermutation-based filtering mechanism. A thorough evaluation across a diverse\nrange of open-source and proprietary multimodal LLMs is performed, followed by\nthe analyses of deficiencies in audio-visual LLMs. Demos and data are available\nat https://github.com/lark-png/AVUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with\na particular focus on auditory information. AVUT introduces a suite of\ncarefully designed audio-centric tasks, holistically testing the understanding\nof both audio content and audio-visual interactions in videos. Moreover, this\nwork points out the text shortcut problem that largely exists in other\nbenchmarks where the correct answer can be found from question text alone\nwithout needing videos. AVUT addresses this problem by proposing a answer\npermutation-based filtering mechanism. A thorough evaluation across a diverse\nrange of open-source and proprietary multimodal LLMs is performed, followed by\nthe analyses of deficiencies in audio-visual LLMs. Demos and data are available\nat https://github.com/lark-png/AVUT."
                },
                "authors": [
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted for publication in the Proceedings of EMNLP 2025 (Main\n  Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11790v2",
                "updated": "2025-09-09T16:05:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    5,
                    10,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-14T18:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    18,
                    27,
                    2,
                    4,
                    73,
                    0
                ],
                "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial\n  Planning in LMMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial\n  Planning in LMMs"
                },
                "summary": "Human reasoning relies on constructing and manipulating mental models --\nsimplified internal representations of situations that we use to understand and\nsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid\nreasoning) externalize these mental models, abstracting irrelevant details to\nefficiently capture how entities interact with each other. In contrast, Large\nLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason\nthrough text, limiting their effectiveness in complex multi-step tasks. In this\npaper, we propose Visual Thinking, a zero-shot framework that enables LMMs to\nreason through multiple chains of (self-generated) conceptual diagrams,\nsignificantly enhancing their combinatorial planning capabilities. Our approach\ndoes not require any human initialization beyond the natural language\ndescription of the task. It integrates both textual and diagrammatic reasoning\nwithin an optimized Graph-of-Thought inference framework, enhanced by beam\nsearch and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves LMMs' performance (e.g.,\nGPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other\ntext-only search-based inference methods. On more difficult planning domains\nwith solution depths up to 40, our approach outperforms even the o1-preview\nreasoning model (e.g., 16 percentage points improvement in Floor Tiles). These\nresults highlight the value of conceptual diagrams as a reasoning medium in\nLMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human reasoning relies on constructing and manipulating mental models --\nsimplified internal representations of situations that we use to understand and\nsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid\nreasoning) externalize these mental models, abstracting irrelevant details to\nefficiently capture how entities interact with each other. In contrast, Large\nLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason\nthrough text, limiting their effectiveness in complex multi-step tasks. In this\npaper, we propose Visual Thinking, a zero-shot framework that enables LMMs to\nreason through multiple chains of (self-generated) conceptual diagrams,\nsignificantly enhancing their combinatorial planning capabilities. Our approach\ndoes not require any human initialization beyond the natural language\ndescription of the task. It integrates both textual and diagrammatic reasoning\nwithin an optimized Graph-of-Thought inference framework, enhanced by beam\nsearch and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves LMMs' performance (e.g.,\nGPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other\ntext-only search-based inference methods. On more difficult planning domains\nwith solution depths up to 40, our approach outperforms even the o1-preview\nreasoning model (e.g., 16 percentage points improvement in Floor Tiles). These\nresults highlight the value of conceptual diagrams as a reasoning medium in\nLMMs."
                },
                "authors": [
                    {
                        "name": "Nasim Borazjanizadeh"
                    },
                    {
                        "name": "Roei Herzig"
                    },
                    {
                        "name": "Eduard Oks"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Karlinsky"
                },
                "author": "Leonid Karlinsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07879v1",
                "updated": "2025-09-09T16:00:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    0,
                    3,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:00:03Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    0,
                    3,
                    1,
                    252,
                    0
                ],
                "title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability\n  with Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Membership Inference Test (aMINT): Enhancing Model Auditability\n  with Multi-Task Learning"
                },
                "summary": "Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "In Proc. IEEE/CVF Intenational Conference on Computer Vision, ICCV,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07874v1",
                "updated": "2025-09-09T15:57:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    40,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:57:40Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    40,
                    1,
                    252,
                    0
                ],
                "title": "Forecasting dementia incidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting dementia incidence"
                },
                "summary": "This paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for\ndementia using a multi-state Cox model. The multi-state model addresses\nproblems of both interval censoring arising from infrequent measurement and\nalso measurement error in dementia. Second, we feed the estimated mean and\nvariance of the time trend into a Kalman filter to infer the population level\ndementia process. Using data from the English Longitudinal Study of Aging\n(ELSA), we find that dementia incidence is no longer declining in England.\nFurthermore, our forecast is that future incidence remains constant, although\nthere is considerable uncertainty in this forecast. Our two-step estimation\nprocedure has significant computational advantages by combining a multi-state\nmodel with a time series method. To account for the short sample that is\navailable for dementia, we derive expressions for the Kalman filter's\nconvergence speed, size, and power to detect changes and conclude our estimator\nperforms well even in short samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for\ndementia using a multi-state Cox model. The multi-state model addresses\nproblems of both interval censoring arising from infrequent measurement and\nalso measurement error in dementia. Second, we feed the estimated mean and\nvariance of the time trend into a Kalman filter to infer the population level\ndementia process. Using data from the English Longitudinal Study of Aging\n(ELSA), we find that dementia incidence is no longer declining in England.\nFurthermore, our forecast is that future incidence remains constant, although\nthere is considerable uncertainty in this forecast. Our two-step estimation\nprocedure has significant computational advantages by combining a multi-state\nmodel with a time series method. To account for the short sample that is\navailable for dementia, we derive expressions for the Kalman filter's\nconvergence speed, size, and power to detect changes and conclude our estimator\nperforms well even in short samples."
                },
                "authors": [
                    {
                        "name": "Jérôme R. Simons"
                    },
                    {
                        "name": "Yuntao Chen"
                    },
                    {
                        "name": "Eric Brunner"
                    },
                    {
                        "name": "Eric French"
                    }
                ],
                "author_detail": {
                    "name": "Eric French"
                },
                "author": "Eric French",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62, 91, 92",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07873v1",
                "updated": "2025-09-09T15:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    25,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:57:25Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    25,
                    1,
                    252,
                    0
                ],
                "title": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through\n  Sentiment-based Backchannels and Active Listening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through\n  Sentiment-based Backchannels and Active Listening"
                },
                "summary": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Go-Eum Cha"
                    },
                    {
                        "name": "Sooyeon Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Sooyeon Jeong"
                },
                "author": "Sooyeon Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07869v1",
                "updated": "2025-09-09T15:56:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:56:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "Are Humans as Brittle as Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Humans as Brittle as Large Language Models?"
                },
                "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07867v1",
                "updated": "2025-09-09T15:55:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    55,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:55:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    55,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models"
                },
                "summary": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise."
                },
                "authors": [
                    {
                        "name": "Augustin Crespin"
                    },
                    {
                        "name": "Ioannis Kostis"
                    },
                    {
                        "name": "Hélène Verhaeghe"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "arxiv_comment": "presented at\"LLMs meet Constraint Solving\" Workshop at CP2025 in\n  Glasgow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07864v1",
                "updated": "2025-09-09T15:51:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via\n  Layer-to-head Attention Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via\n  Layer-to-head Attention Diagnostics"
                },
                "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency."
                },
                "authors": [
                    {
                        "name": "Tiancheng Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Lijie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Hu"
                },
                "author": "Lijie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07860v1",
                "updated": "2025-09-09T15:40:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    40,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:40:23Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    40,
                    23,
                    1,
                    252,
                    0
                ],
                "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis"
                },
                "summary": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence."
                },
                "authors": [
                    {
                        "name": "Guanzhi Deng"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Yu-Keung Ng"
                    },
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Peijun Zheng"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Dapeng Wu"
                    },
                    {
                        "name": "Yinqiao Li"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07858v1",
                "updated": "2025-09-09T15:38:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    38,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:38:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    38,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data\n  Synthesizers to Empower Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data\n  Synthesizers to Empower Code LLMs"
                },
                "summary": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Linmei Hu"
                    },
                    {
                        "name": "Luhao Zhang"
                    },
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Haomin Fu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Mengdi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Zhang"
                },
                "author": "Mengdi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07846v1",
                "updated": "2025-09-09T15:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    22,
                    33,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    22,
                    33,
                    1,
                    252,
                    0
                ],
                "title": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A\n  Comparative RAG Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A\n  Comparative RAG Study"
                },
                "summary": "Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively."
                },
                "authors": [
                    {
                        "name": "Amay Jain"
                    },
                    {
                        "name": "Liu Cui"
                    },
                    {
                        "name": "Si Chen"
                    }
                ],
                "author_detail": {
                    "name": "Si Chen"
                },
                "author": "Si Chen",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21393v2",
                "updated": "2025-09-09T15:20:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    20,
                    7,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-29T08:14:38Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    38,
                    4,
                    241,
                    0
                ],
                "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs"
                },
                "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."
                },
                "authors": [
                    {
                        "name": "Guofu Liao"
                    },
                    {
                        "name": "Taotao Wang"
                    },
                    {
                        "name": "Shengli Zhang"
                    },
                    {
                        "name": "Jiqun Zhang"
                    },
                    {
                        "name": "Shi Long"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07839v1",
                "updated": "2025-09-09T15:13:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    13,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:13:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    13,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "Enhancements in Score-based Channel Estimation for Real-Time Wireless\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancements in Score-based Channel Estimation for Real-Time Wireless\n  Systems"
                },
                "summary": "We propose enhancements to score-based generative modeling techniques for\nlow-latency pilot-based channel estimation in a point-to-point single-carrier\nmultiple-input multiple-output (MIMO) wireless system. Building on recent\nadvances in score-based models, we investigate a specific noise schedule design\nand sampling acceleration by step-skipping to reduce the number of denoising\nsteps during inference. We additionally propose a single-step signal-to-noise\nratio informed denoiser as an extreme case of the step-skipping approach. Our\nmethods achieve significant latency reductions without performance degradation,\nas demonstrated on a synthetic channel dataset representing an urban macrocell\nMIMO communications scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose enhancements to score-based generative modeling techniques for\nlow-latency pilot-based channel estimation in a point-to-point single-carrier\nmultiple-input multiple-output (MIMO) wireless system. Building on recent\nadvances in score-based models, we investigate a specific noise schedule design\nand sampling acceleration by step-skipping to reduce the number of denoising\nsteps during inference. We additionally propose a single-step signal-to-noise\nratio informed denoiser as an extreme case of the step-skipping approach. Our\nmethods achieve significant latency reductions without performance degradation,\nas demonstrated on a synthetic channel dataset representing an urban macrocell\nMIMO communications scenario."
                },
                "authors": [
                    {
                        "name": "Florian Strasser"
                    },
                    {
                        "name": "Marion Bäro"
                    },
                    {
                        "name": "Wolfgang Utschick"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Utschick"
                },
                "author": "Wolfgang Utschick",
                "arxiv_comment": "Presented at 28th International Workshop on Smart Antennas 2025,\n  https://www.wsa2025.fau.de/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07829v1",
                "updated": "2025-09-09T15:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    7,
                    14,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:07:14Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    7,
                    14,
                    1,
                    252,
                    0
                ],
                "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost"
                },
                "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings."
                },
                "authors": [
                    {
                        "name": "Mihai Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    },
                    {
                        "name": "Andreea Tomescu"
                    },
                    {
                        "name": "Andrei Piscoran"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Piscoran"
                },
                "author": "Andrei Piscoran",
                "arxiv_comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07825v1",
                "updated": "2025-09-09T15:01:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    1,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:01:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    1,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language\n  Model"
                },
                "summary": "3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding."
                },
                "authors": [
                    {
                        "name": "Zhuoxu Huang"
                    },
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Jungong Han"
                    }
                ],
                "author_detail": {
                    "name": "Jungong Han"
                },
                "author": "Jungong Han",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02603v2",
                "updated": "2025-09-09T14:58:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    58,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-19T16:44:14Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    44,
                    14,
                    1,
                    324,
                    0
                ],
                "title": "Generative AI as a Tool for Enhancing Reflective Learning in Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI as a Tool for Enhancing Reflective Learning in Students"
                },
                "summary": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems."
                },
                "authors": [
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Jiazi Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiazi Hu"
                },
                "author": "Jiazi Hu",
                "arxiv_comment": "Accepted by IEEE TALE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07819v1",
                "updated": "2025-09-09T14:56:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    56,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:56:37Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    56,
                    37,
                    1,
                    252,
                    0
                ],
                "title": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in\n  Knowledge Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in\n  Knowledge Communities"
                },
                "summary": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness."
                },
                "authors": [
                    {
                        "name": "Moyan Zhou"
                    },
                    {
                        "name": "Soobin Cho"
                    },
                    {
                        "name": "Loren Terveen"
                    }
                ],
                "author_detail": {
                    "name": "Loren Terveen"
                },
                "author": "Loren Terveen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07817v1",
                "updated": "2025-09-09T14:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    55,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:55:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    55,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems"
                },
                "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters."
                },
                "authors": [
                    {
                        "name": "Xiaolin Chen"
                    },
                    {
                        "name": "Xuemeng Song"
                    },
                    {
                        "name": "Haokun Wen"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07808v1",
                "updated": "2025-09-09T14:47:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    47,
                    41,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:47:41Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    47,
                    41,
                    1,
                    252,
                    0
                ],
                "title": "A dense dark matter core of the subhalo in the strong lensing system\n  JVAS B1938+666",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dense dark matter core of the subhalo in the strong lensing system\n  JVAS B1938+666"
                },
                "summary": "The nature of dark matter remains unknown, motivating the study of fuzzy/wave\ndark matter (FDM/$\\psi$DM) and self-interacting dark matter (SIDM) as\nalternative frameworks to address small-scale discrepancies in halo profiles\ninferred from observations. This study presents a non-parametric reconstruction\nof the mass distribution of the previously-found, dark subhalo in the\nstrong-lensing system JVAS B1938+666. Compared with the standard\nNavarro-Frenk-White (NFW) profile, both SIDM and $\\psi$DM\n($m_{\\psi}=1.32^{+0.22}_{-0.31}\\times 10^{-22} \\, \\rm eV$) provide\nsignificantly better fits to the resulting density profile. Moreover, the SIDM\nmodel is favored over $\\psi$DM with a Bayes factor of 14.44. The reconstructed\ndensity profile features a characteristic kiloparsec-scale core ($r_c \\approx\n0.5 \\, \\rm kpc$) with central density $\\rho_c \\approx 2.5\\times 10^{7}\\, \\rm\nM_{\\odot} \\, kpc^{-3} $, exhibiting remarkable consistency with the core-halo\nmass scaling relations observed in Local Group dwarf spheroidals. These\nfindings offer insights that may help address the core-cusp discrepancy in\n$\\Lambda$CDM substructure predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nature of dark matter remains unknown, motivating the study of fuzzy/wave\ndark matter (FDM/$\\psi$DM) and self-interacting dark matter (SIDM) as\nalternative frameworks to address small-scale discrepancies in halo profiles\ninferred from observations. This study presents a non-parametric reconstruction\nof the mass distribution of the previously-found, dark subhalo in the\nstrong-lensing system JVAS B1938+666. Compared with the standard\nNavarro-Frenk-White (NFW) profile, both SIDM and $\\psi$DM\n($m_{\\psi}=1.32^{+0.22}_{-0.31}\\times 10^{-22} \\, \\rm eV$) provide\nsignificantly better fits to the resulting density profile. Moreover, the SIDM\nmodel is favored over $\\psi$DM with a Bayes factor of 14.44. The reconstructed\ndensity profile features a characteristic kiloparsec-scale core ($r_c \\approx\n0.5 \\, \\rm kpc$) with central density $\\rho_c \\approx 2.5\\times 10^{7}\\, \\rm\nM_{\\odot} \\, kpc^{-3} $, exhibiting remarkable consistency with the core-halo\nmass scaling relations observed in Local Group dwarf spheroidals. These\nfindings offer insights that may help address the core-cusp discrepancy in\n$\\Lambda$CDM substructure predictions."
                },
                "authors": [
                    {
                        "name": "Lei Lei"
                    },
                    {
                        "name": "Yi-Ying Wang"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jiang Dong"
                    },
                    {
                        "name": "Ze-Fan Wang"
                    },
                    {
                        "name": "Wei-Long Lin"
                    },
                    {
                        "name": "Yi-Ping Shu"
                    },
                    {
                        "name": "Xiao-Yue Cao"
                    },
                    {
                        "name": "Da-Neng Yang"
                    },
                    {
                        "name": "Yi-Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhong Fan"
                },
                "author": "Yi-Zhong Fan",
                "arxiv_doi": "10.3847/2041-8213/ae047c",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ae047c",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.07808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 8 figures, Accepted for publication on ApJL",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07642v3",
                "updated": "2025-09-09T14:42:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    53,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-09T11:07:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    7,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review"
                },
                "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."
                },
                "authors": [
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Yuanbo Kong"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Hayden Kwok-Hay So"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Liya Zhu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted to EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04575v2",
                "updated": "2025-09-09T14:42:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    48,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-04T18:01:00Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    18,
                    1,
                    0,
                    3,
                    247,
                    0
                ],
                "title": "Bootstrapping Task Spaces for Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Task Spaces for Self-Improvement"
                },
                "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training."
                },
                "authors": [
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16582v3",
                "updated": "2025-09-09T14:42:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2023-10-25T12:16:33Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    12,
                    16,
                    33,
                    2,
                    298,
                    0
                ],
                "title": "UPLex: Fine-Grained Personality Control in Large Language Models via\n  Unsupervised Lexical Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UPLex: Fine-Grained Personality Control in Large Language Models via\n  Unsupervised Lexical Modulation"
                },
                "summary": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities."
                },
                "authors": [
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07794v1",
                "updated": "2025-09-09T14:31:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    31,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:31:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    31,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey"
                },
                "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Xinxuan Lv"
                    },
                    {
                        "name": "Junjie Zou"
                    },
                    {
                        "name": "Tongna Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suchao An"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "arxiv_comment": "38 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12116v2",
                "updated": "2025-09-09T14:29:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    29,
                    20,
                    1,
                    252,
                    0
                ],
                "published": "2025-07-16T10:33:29Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    33,
                    29,
                    2,
                    197,
                    0
                ],
                "title": "Euclid preparation. Simulating thousands of Euclid spectroscopic skies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. Simulating thousands of Euclid spectroscopic skies"
                },
                "summary": "We present two extensive sets of 3500+1000 simulations of dark matter haloes\non the past light cone, and two corresponding sets of simulated (`mock') galaxy\ncatalogues that represent the Euclid spectroscopic sample. The simulations were\nproduced with the latest version of the PINOCCHIO code, and provide the\nlargest, public set of simulated skies. Mock galaxy catalogues were obtained by\npopulating haloes with galaxies using an halo occupation distribution (HOD)\nmodel extracted from the Flagship galaxy catalogue provided by Euclid\nCollaboration. The Geppetto set of 3500 simulated skies was obtained by tiling\na 1.2 Gpc/h box to cover a light-cone whose sky footprint is a circle of 30 deg\nradius, for an area of 2763 deg$^2$ and a minimum halo mass of\n$1.5\\times10^{11}$ Msun/h. The relatively small box size makes this set unfit\nfor measuring very large scales. The EuclidLargeBox set consists of 1000\nsimulations of 3.38 Gpc/h, with the same mass resolution and a footprint that\ncovers half of the sky, excluding the Milky Way zone of avoidance. From this we\nproduced a set of 1000 EuclidLargeMocks on the 30 deg radius footprint, whose\ncomoving volume is fully contained in the simulation box. We validated the two\nsets of catalogues by analysing number densities, power spectra, and 2-point\ncorrelation functions, showing that the Flagship spectroscopic catalogue is\nconsistent with being one of the realisations of the simulated sets, although\nwe noticed small deviations limited to the quadrupole at k>0.2 h/Mpc. We show\ncosmological parameter inference from these catalogues and demonstrate that\nusing one realisation of EuclidLargeMocks in place of the Flagship mock\nproduces the same posteriors, to within the expected shift given by sample\nvariance. These simulated skies will be used for the galaxy clustering analysis\nof Euclid's Data Release 1 (DR1).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present two extensive sets of 3500+1000 simulations of dark matter haloes\non the past light cone, and two corresponding sets of simulated (`mock') galaxy\ncatalogues that represent the Euclid spectroscopic sample. The simulations were\nproduced with the latest version of the PINOCCHIO code, and provide the\nlargest, public set of simulated skies. Mock galaxy catalogues were obtained by\npopulating haloes with galaxies using an halo occupation distribution (HOD)\nmodel extracted from the Flagship galaxy catalogue provided by Euclid\nCollaboration. The Geppetto set of 3500 simulated skies was obtained by tiling\na 1.2 Gpc/h box to cover a light-cone whose sky footprint is a circle of 30 deg\nradius, for an area of 2763 deg$^2$ and a minimum halo mass of\n$1.5\\times10^{11}$ Msun/h. The relatively small box size makes this set unfit\nfor measuring very large scales. The EuclidLargeBox set consists of 1000\nsimulations of 3.38 Gpc/h, with the same mass resolution and a footprint that\ncovers half of the sky, excluding the Milky Way zone of avoidance. From this we\nproduced a set of 1000 EuclidLargeMocks on the 30 deg radius footprint, whose\ncomoving volume is fully contained in the simulation box. We validated the two\nsets of catalogues by analysing number densities, power spectra, and 2-point\ncorrelation functions, showing that the Flagship spectroscopic catalogue is\nconsistent with being one of the realisations of the simulated sets, although\nwe noticed small deviations limited to the quadrupole at k>0.2 h/Mpc. We show\ncosmological parameter inference from these catalogues and demonstrate that\nusing one realisation of EuclidLargeMocks in place of the Flagship mock\nproduces the same posteriors, to within the expected shift given by sample\nvariance. These simulated skies will be used for the galaxy clustering analysis\nof Euclid's Data Release 1 (DR1)."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "G. Parimbelli"
                    },
                    {
                        "name": "M. Y. Elkhashab"
                    },
                    {
                        "name": "J. Salvalaggio"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "M. D. Lepinzan"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "L. Tornatore"
                    },
                    {
                        "name": "G. E. Addison"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "G. Lavaux"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "K. S. McCarthy"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "F. Ducret"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "N. Fourmanoit"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kümmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "F. Rizzo"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. Sánchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "M. Scodeggio"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "G. Fabbian"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "J. Martín-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pöntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "S. Alvi"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "S. Avila"
                    },
                    {
                        "name": "A. Balaguera-Antolinez"
                    },
                    {
                        "name": "P. Bergamini"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Conseil"
                    },
                    {
                        "name": "S. Contarini"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "O. Cucciati"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "A. Enia"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "F. Fontanot"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "C. Hernández-Monteagudo"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "S. Joudaki"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "V. Le Brun"
                    },
                    {
                        "name": "J. Le Graet"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "C. Mancini"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "A. Montoro"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "G. Rodighiero"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahlén"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "J. G. Sorce"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "F. Vernizzi"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_comment": "19 pages, submitted to A&A, revised after referee report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18934v2",
                "updated": "2025-09-09T14:27:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    27,
                    58,
                    1,
                    252,
                    0
                ],
                "published": "2024-12-25T15:45:18Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    15,
                    45,
                    18,
                    2,
                    360,
                    0
                ],
                "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference"
                },
                "summary": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts."
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Baizhou Xu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Songzhu Mei"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00795v2",
                "updated": "2025-09-09T14:25:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    25,
                    56,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-01T02:49:26Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    2,
                    49,
                    26,
                    6,
                    152,
                    0
                ],
                "title": "Closing the Gap between TD Learning and Supervised Learning with\n  $Q$-Conditioned Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Gap between TD Learning and Supervised Learning with\n  $Q$-Conditioned Maximization"
                },
                "summary": "Recently, supervised learning (SL) methodology has emerged as an effective\napproach for offline reinforcement learning (RL) due to their simplicity,\nstability, and efficiency. However, recent studies show that SL methods lack\nthe trajectory stitching capability, typically associated with temporal\ndifference (TD)-based approaches. A question naturally surfaces: \\textit{How\ncan we endow SL methods with stitching capability and close its performance gap\nwith TD learning?} To answer this question, we introduce $Q$-conditioned\nmaximization supervised learning for offline goal-conditioned RL, which\nenhances SL with the stitching capability through $Q$-conditioned policy and\n$Q$-conditioned maximization. Concretely, we propose\n\\textbf{G}oal-\\textbf{C}onditioned \\textbf{\\textit{Rein}}forced\n\\textbf{S}upervised \\textbf{L}earning (\\textbf{GC\\textit{Rein}SL}), which\nconsists of (1) estimating the $Q$-function by Normalizing Flows from the\noffline dataset and (2) finding the maximum $Q$-value within the data support\nby integrating $Q$-function maximization with Expectile Regression. In\ninference time, our policy chooses optimal actions based on such a maximum\n$Q$-value. Experimental results from stitching evaluations on offline RL\ndatasets demonstrate that our method outperforms prior SL approaches with\nstitching capabilities and goal data augmentation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, supervised learning (SL) methodology has emerged as an effective\napproach for offline reinforcement learning (RL) due to their simplicity,\nstability, and efficiency. However, recent studies show that SL methods lack\nthe trajectory stitching capability, typically associated with temporal\ndifference (TD)-based approaches. A question naturally surfaces: \\textit{How\ncan we endow SL methods with stitching capability and close its performance gap\nwith TD learning?} To answer this question, we introduce $Q$-conditioned\nmaximization supervised learning for offline goal-conditioned RL, which\nenhances SL with the stitching capability through $Q$-conditioned policy and\n$Q$-conditioned maximization. Concretely, we propose\n\\textbf{G}oal-\\textbf{C}onditioned \\textbf{\\textit{Rein}}forced\n\\textbf{S}upervised \\textbf{L}earning (\\textbf{GC\\textit{Rein}SL}), which\nconsists of (1) estimating the $Q$-function by Normalizing Flows from the\noffline dataset and (2) finding the maximum $Q$-value within the data support\nby integrating $Q$-function maximization with Expectile Regression. In\ninference time, our policy chooses optimal actions based on such a maximum\n$Q$-value. Experimental results from stitching evaluations on offline RL\ndatasets demonstrate that our method outperforms prior SL approaches with\nstitching capabilities and goal data augmentation techniques."
                },
                "authors": [
                    {
                        "name": "Xing Lei"
                    },
                    {
                        "name": "Zifeng Zhuang"
                    },
                    {
                        "name": "Shentao Yang"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Yunhao Luo"
                    },
                    {
                        "name": "Fei Shen"
                    },
                    {
                        "name": "Wenyan Yang"
                    },
                    {
                        "name": "Xuetao Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18993v2",
                "updated": "2025-09-09T14:24:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    24,
                    47,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-26T09:56:51Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    51,
                    2,
                    57,
                    0
                ],
                "title": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering"
                },
                "summary": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures."
                },
                "authors": [
                    {
                        "name": "Teng Lin"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07782v1",
                "updated": "2025-09-09T14:19:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    19,
                    19,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:19:19Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    19,
                    19,
                    1,
                    252,
                    0
                ],
                "title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and\n  High-Quality Novel View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and\n  High-Quality Novel View Synthesis"
                },
                "summary": "RayGauss has achieved state-of-the-art rendering quality for novel-view\nsynthesis on synthetic and indoor scenes by representing radiance and density\nfields with irregularly distributed elliptical basis functions, rendered via\nvolume ray casting using a Bounding Volume Hierarchy (BVH). However, its\ncomputational cost prevents real-time rendering on real-world scenes. Our\napproach, RayGaussX, builds on RayGauss by introducing key contributions that\naccelerate both training and inference. Specifically, we incorporate volumetric\nrendering acceleration strategies such as empty-space skipping and adaptive\nsampling, enhance ray coherence, and introduce scale regularization to reduce\nfalse-positive intersections. Additionally, we propose a new densification\ncriterion that improves density distribution in distant regions, leading to\nenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x\nto 12x faster training and 50x to 80x higher rendering speeds (FPS) on\nreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.\nProject page with videos and code: https://raygaussx.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RayGauss has achieved state-of-the-art rendering quality for novel-view\nsynthesis on synthetic and indoor scenes by representing radiance and density\nfields with irregularly distributed elliptical basis functions, rendered via\nvolume ray casting using a Bounding Volume Hierarchy (BVH). However, its\ncomputational cost prevents real-time rendering on real-world scenes. Our\napproach, RayGaussX, builds on RayGauss by introducing key contributions that\naccelerate both training and inference. Specifically, we incorporate volumetric\nrendering acceleration strategies such as empty-space skipping and adaptive\nsampling, enhance ray coherence, and introduce scale regularization to reduce\nfalse-positive intersections. Additionally, we propose a new densification\ncriterion that improves density distribution in distant regions, leading to\nenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x\nto 12x faster training and 50x to 80x higher rendering speeds (FPS) on\nreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.\nProject page with videos and code: https://raygaussx.github.io/."
                },
                "authors": [
                    {
                        "name": "Hugo Blanc"
                    },
                    {
                        "name": "Jean-Emmanuel Deschaud"
                    },
                    {
                        "name": "Alexis Paljic"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Paljic"
                },
                "author": "Alexis Paljic",
                "arxiv_comment": "Project page with videos and code: https://raygaussx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07777v1",
                "updated": "2025-09-09T14:13:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    13,
                    29,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:13:29Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    13,
                    29,
                    1,
                    252,
                    0
                ],
                "title": "A comprehensive separation of dark matter and baryonic mass components\n  in galaxy clusters II: an overview of the mass distribution in Abell S1063",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive separation of dark matter and baryonic mass components\n  in galaxy clusters II: an overview of the mass distribution in Abell S1063"
                },
                "summary": "In the first paper of this series, we derived mass constraints on the total\nmass and the baryonic components of the galaxy cluster Abell S1063. The main\nfocus was to recover stellar masses and kinematics for cluster members, the\nbrightest cluster galaxy (BCG) and the intra-cluster light (ICL). In this\nsecond paper, we introduce a multi-probe mass modelling approach that\nincorporates constraints on both the total mass and the individual baryonic\ncomponents. We obtain comprehensive mass models of Abell S1063, in which the\ndark matter distribution is disentangled from the baryonic mass at both cluster\nand galaxy scales. The best-fitting mass model achieves an RMS of $0.50\"$ on\nthe multiple image positions. The kinematic profiles of the BCG \\& ICL, as well\nas the X-ray surface brightness of the intra-cluster gas, are accurately\nreproduced within observational uncertainties. However, a $35~\\mathrm{km/s}$\nscatter is required for the cluster member line-of-sight dispersions. This\nmethod yields the most complex parametric mass model with consistency among\nalmost all available mass constraints. We find a $1\\sigma$ agreement between\nthe inferred stellar-to-subhalo mass relation and that predicted by large-scale\ncosmological simulations. The ICL stellar mass derived from our model is\nconsistent with estimates from stellar population modelling. We present the\nfirst multi-probe mass modelling method capable of disentangling the dark\nmatter from the baryonic mass distributions in massive galaxy clusters. Its\nresults, such as the stellar-to-subhalo mass relation or the distribution of\neach mass component, can be directly compared to hydrodynamical cosmological\nsimulations such as illustrisTNG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the first paper of this series, we derived mass constraints on the total\nmass and the baryonic components of the galaxy cluster Abell S1063. The main\nfocus was to recover stellar masses and kinematics for cluster members, the\nbrightest cluster galaxy (BCG) and the intra-cluster light (ICL). In this\nsecond paper, we introduce a multi-probe mass modelling approach that\nincorporates constraints on both the total mass and the individual baryonic\ncomponents. We obtain comprehensive mass models of Abell S1063, in which the\ndark matter distribution is disentangled from the baryonic mass at both cluster\nand galaxy scales. The best-fitting mass model achieves an RMS of $0.50\"$ on\nthe multiple image positions. The kinematic profiles of the BCG \\& ICL, as well\nas the X-ray surface brightness of the intra-cluster gas, are accurately\nreproduced within observational uncertainties. However, a $35~\\mathrm{km/s}$\nscatter is required for the cluster member line-of-sight dispersions. This\nmethod yields the most complex parametric mass model with consistency among\nalmost all available mass constraints. We find a $1\\sigma$ agreement between\nthe inferred stellar-to-subhalo mass relation and that predicted by large-scale\ncosmological simulations. The ICL stellar mass derived from our model is\nconsistent with estimates from stellar population modelling. We present the\nfirst multi-probe mass modelling method capable of disentangling the dark\nmatter from the baryonic mass distributions in massive galaxy clusters. Its\nresults, such as the stellar-to-subhalo mass relation or the distribution of\neach mass component, can be directly compared to hydrodynamical cosmological\nsimulations such as illustrisTNG."
                },
                "authors": [
                    {
                        "name": "Benjamin Beauchesne"
                    },
                    {
                        "name": "Benjamin Clément"
                    },
                    {
                        "name": "Marceau Limousin"
                    },
                    {
                        "name": "Anna Niemiec"
                    },
                    {
                        "name": "Mathilde Jauzac"
                    },
                    {
                        "name": "Belén Alcalde Pampliega"
                    },
                    {
                        "name": "Johan Richard"
                    },
                    {
                        "name": "Guillaume Mahler"
                    },
                    {
                        "name": "Jose M. Diego"
                    },
                    {
                        "name": "Pascale Hibon"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Thomas Connor"
                    },
                    {
                        "name": "Jean-Paul Kneib"
                    },
                    {
                        "name": "Andreas L. Faisst"
                    }
                ],
                "author_detail": {
                    "name": "Andreas L. Faisst"
                },
                "author": "Andreas L. Faisst",
                "arxiv_comment": "Submitted to MNRAS, 24 pages in length with 14 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19618v3",
                "updated": "2025-09-09T14:04:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    4,
                    18,
                    1,
                    252,
                    0
                ],
                "published": "2024-07-29T00:41:11Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    0,
                    41,
                    11,
                    0,
                    211,
                    0
                ],
                "title": "Improving the Estimation of Lifetime Effects in A/B Testing via\n  Treatment Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Estimation of Lifetime Effects in A/B Testing via\n  Treatment Locality"
                },
                "summary": "Utilizing randomized experiments to evaluate the effect of short-term\ntreatments on the short-term outcomes has been well understood and become the\ngolden standard in industrial practice. However, as service systems become\nincreasingly dynamical and personalized, much focus is shifting toward\nmaximizing long-term outcomes, such as customer lifetime value, through\nlifetime exposure to interventions. Our goal is to assess the impact of\ntreatment and control policies on long-term outcomes from relatively short-term\nobservations, such as those generated by A/B testing. A key managerial\nobservation is that many practical treatments are local, affecting only\ntargeted states while leaving other parts of the policy unchanged. This paper\nrigorously investigates whether and how such locality can be exploited to\nimprove estimation of long-term effects in Markov Decision Processes (MDPs), a\nfundamental model of dynamic systems. We first develop optimal inference\ntechniques for general A/B testing in MDPs and establish corresponding\nefficiency bounds. We then propose methods to harness the localized structure\nby sharing information on the non-targeted states. Our new estimator can\nachieve a linear reduction with the number of test arms for a major part of the\nvariance without sacrificing unbiasedness. It also matches a tighter variance\nlower bound that accounts for locality. Furthermore, we extend our framework to\na broad class of differentiable estimators, which encompasses many widely used\napproaches in practice. We show that all such estimators can benefit from\nvariance reduction through information sharing without increasing their bias.\nTogether, these results provide both theoretical foundations and practical\ntools for conducting efficient experiments in dynamic service systems with\nlocal treatments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing randomized experiments to evaluate the effect of short-term\ntreatments on the short-term outcomes has been well understood and become the\ngolden standard in industrial practice. However, as service systems become\nincreasingly dynamical and personalized, much focus is shifting toward\nmaximizing long-term outcomes, such as customer lifetime value, through\nlifetime exposure to interventions. Our goal is to assess the impact of\ntreatment and control policies on long-term outcomes from relatively short-term\nobservations, such as those generated by A/B testing. A key managerial\nobservation is that many practical treatments are local, affecting only\ntargeted states while leaving other parts of the policy unchanged. This paper\nrigorously investigates whether and how such locality can be exploited to\nimprove estimation of long-term effects in Markov Decision Processes (MDPs), a\nfundamental model of dynamic systems. We first develop optimal inference\ntechniques for general A/B testing in MDPs and establish corresponding\nefficiency bounds. We then propose methods to harness the localized structure\nby sharing information on the non-targeted states. Our new estimator can\nachieve a linear reduction with the number of test arms for a major part of the\nvariance without sacrificing unbiasedness. It also matches a tighter variance\nlower bound that accounts for locality. Furthermore, we extend our framework to\na broad class of differentiable estimators, which encompasses many widely used\napproaches in practice. We show that all such estimators can benefit from\nvariance reduction through information sharing without increasing their bias.\nTogether, these results provide both theoretical foundations and practical\ntools for conducting efficient experiments in dynamic service systems with\nlocal treatments."
                },
                "authors": [
                    {
                        "name": "Shuze Chen"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Chonghuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chonghuan Wang"
                },
                "author": "Chonghuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07768v1",
                "updated": "2025-09-09T14:01:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    1,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:01:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    1,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning"
                },
                "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct."
                },
                "authors": [
                    {
                        "name": "Michele Joshua Maggini"
                    },
                    {
                        "name": "Dhia Merzougui"
                    },
                    {
                        "name": "Rabiraj Bandyopadhyay"
                    },
                    {
                        "name": "Gaël Dias"
                    },
                    {
                        "name": "Fabrice Maurel"
                    },
                    {
                        "name": "Pablo Gamallo"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Gamallo"
                },
                "author": "Pablo Gamallo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08994v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08994v3",
                "updated": "2025-09-09T14:00:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    0,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-12T02:07:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    7,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation"
                },
                "summary": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Kaixin Zhang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    },
                    {
                        "name": "Ziqi Li"
                    },
                    {
                        "name": "Yabin Lu"
                    },
                    {
                        "name": "Yingze Li"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Yiming Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Guan"
                },
                "author": "Yiming Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08994v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08994v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07764v1",
                "updated": "2025-09-09T13:59:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    59,
                    0,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:59:00Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    59,
                    0,
                    1,
                    252,
                    0
                ],
                "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework\n  for Computer-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework\n  for Computer-Use Agents"
                },
                "summary": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."
                },
                "authors": [
                    {
                        "name": "Haitao Hu"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Yanpeng Zhao"
                    },
                    {
                        "name": "Yuqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Chen"
                },
                "author": "Yuqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07763v1",
                "updated": "2025-09-09T13:58:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:58:46Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects"
                },
                "summary": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals."
                },
                "authors": [
                    {
                        "name": "Mikel Robredo"
                    },
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Rafael Peñaloza"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Lenarduzzi"
                },
                "author": "Valentina Lenarduzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07759v1",
                "updated": "2025-09-09T13:57:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    57,
                    53,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:57:53Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    57,
                    53,
                    1,
                    252,
                    0
                ],
                "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Long-Document Retrieval in the PLM and LLM Era"
                },
                "summary": "The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Miyang Luo"
                    },
                    {
                        "name": "Tianrui Lv"
                    },
                    {
                        "name": "Yishuai Zhang"
                    },
                    {
                        "name": "Siqi Zhao"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "arxiv_comment": "33 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07755v1",
                "updated": "2025-09-09T13:54:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    54,
                    34,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:54:34Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    54,
                    34,
                    1,
                    252,
                    0
                ],
                "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts"
                },
                "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent."
                },
                "authors": [
                    {
                        "name": "Rochana Prih Hastuti"
                    },
                    {
                        "name": "Rian Adam Rajagede"
                    },
                    {
                        "name": "Mansour Al Ghanim"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04656v2",
                "updated": "2025-09-09T13:49:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    49,
                    46,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-04T20:57:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    20,
                    57,
                    35,
                    3,
                    247,
                    0
                ],
                "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for\n  Arabic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for\n  Arabic LLMs"
                },
                "summary": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\nhttps://github.com/aishaalansari57/AraHalluEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\nhttps://github.com/aishaalansari57/AraHalluEval"
                },
                "authors": [
                    {
                        "name": "Aisha Alansari"
                    },
                    {
                        "name": "Hamzah Luqman"
                    }
                ],
                "author_detail": {
                    "name": "Hamzah Luqman"
                },
                "author": "Hamzah Luqman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16661v2",
                "updated": "2025-09-09T13:48:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    48,
                    45,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-22T13:27:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    27,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP"
                },
                "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."
                },
                "authors": [
                    {
                        "name": "Shinnosuke Ono"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Takuro Fujii"
                    },
                    {
                        "name": "Kosei Buma"
                    },
                    {
                        "name": "Shunsuke Sasaki"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Sasaki"
                },
                "author": "Shunsuke Sasaki",
                "arxiv_comment": "15 pages, 9 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01542v2",
                "updated": "2025-09-09T13:40:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    40,
                    40,
                    1,
                    252,
                    0
                ],
                "published": "2025-04-02T09:30:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation"
                },
                "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices."
                },
                "authors": [
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07730v2",
                "updated": "2025-09-10T04:50:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    4,
                    50,
                    56,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T13:32:29Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    32,
                    29,
                    1,
                    252,
                    0
                ],
                "title": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models"
                },
                "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE."
                },
                "authors": [
                    {
                        "name": "Zexuan Li"
                    },
                    {
                        "name": "Hongliang Dai"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "arxiv_comment": "Accepted by EMNLP2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05429v2",
                "updated": "2025-09-09T13:31:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    31,
                    52,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-05T18:17:57Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    18,
                    17,
                    57,
                    4,
                    248,
                    0
                ],
                "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding Graph Neural Networks against Topology Inference Attacks"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is available at\nhttps://github.com/JeffffffFu/PGR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is available at\nhttps://github.com/JeffffffFu/PGR."
                },
                "authors": [
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Hong Yuan"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Wendy Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wendy Hui Wang"
                },
                "author": "Wendy Hui Wang",
                "arxiv_comment": "Acctepted by ACM CCS'25",
                "arxiv_journal_ref": "In Proceedings of the 32nd ACM Conference on Computer and\n  Communications Security (ACM CCS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07727v1",
                "updated": "2025-09-09T13:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    28,
                    41,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:28:41Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    28,
                    41,
                    1,
                    252,
                    0
                ],
                "title": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?"
                },
                "summary": "With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy."
                },
                "authors": [
                    {
                        "name": "Songkai Ma"
                    },
                    {
                        "name": "Zhaorui Zhang"
                    },
                    {
                        "name": "Sheng Di"
                    },
                    {
                        "name": "Benben Liu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Xiaoyi Lu"
                    },
                    {
                        "name": "Dan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dan Wang"
                },
                "author": "Dan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05952v2",
                "updated": "2025-09-09T13:23:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    23,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-07T07:25:00Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    7,
                    25,
                    0,
                    6,
                    250,
                    0
                ],
                "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow\n  Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow\n  Matching"
                },
                "summary": "Reinforcement Learning (RL) has recently emerged as a powerful technique for\nimproving image and video generation in Diffusion and Flow Matching models,\nspecifically for enhancing output quality and alignment with prompts. A\ncritical step for applying online RL methods on Flow Matching is the\nintroduction of stochasticity into the deterministic framework, commonly\nrealized by Stochastic Differential Equation (SDE). Our investigation reveals a\nsignificant drawback to this approach: SDE-based sampling introduces pronounced\nnoise artifacts in the generated images, which we found to be detrimental to\nthe reward learning process. A rigorous theoretical analysis traces the origin\nof this noise to an excess of stochasticity injected during inference. To\naddress this, we draw inspiration from Denoising Diffusion Implicit Models\n(DDIM) to reformulate the sampling process. Our proposed method,\nCoefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This\nleads to more accurate reward modeling, ultimately enabling faster and more\nstable convergence for reinforcement learning-based optimizers like Flow-GRPO\nand Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has recently emerged as a powerful technique for\nimproving image and video generation in Diffusion and Flow Matching models,\nspecifically for enhancing output quality and alignment with prompts. A\ncritical step for applying online RL methods on Flow Matching is the\nintroduction of stochasticity into the deterministic framework, commonly\nrealized by Stochastic Differential Equation (SDE). Our investigation reveals a\nsignificant drawback to this approach: SDE-based sampling introduces pronounced\nnoise artifacts in the generated images, which we found to be detrimental to\nthe reward learning process. A rigorous theoretical analysis traces the origin\nof this noise to an excess of stochasticity injected during inference. To\naddress this, we draw inspiration from Denoising Diffusion Implicit Models\n(DDIM) to reformulate the sampling process. Our proposed method,\nCoefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This\nleads to more accurate reward modeling, ultimately enabling faster and more\nstable convergence for reinforcement learning-based optimizers like Flow-GRPO\nand Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS"
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Zihao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Yu"
                },
                "author": "Zihao Yu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07711v1",
                "updated": "2025-09-09T13:13:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    13,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:13:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    13,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced\n  Mathematical Reasoning"
                },
                "summary": "As large language models (LLMs) reach high scores on established mathematical\nbenchmarks, such as GSM8K and MATH, the research community has turned to\nInternational Mathematical Olympiad (IMO) problems to push the evaluation\nfrontier. However, existing Olympiad-level benchmarks suffer from practical\nconstraints that introduce grading noise and potential bias, such as\nheterogeneous answer formats requiring model-based judges and a reliance on\npotentially flawed solutions. We introduce RIMO, a two-track benchmark designed\nto preserve peak Olympiad difficulty while eliminating this evaluation noise.\nThe first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique\ninteger answer, allowing for deterministic correctness checking. The second\ntrack, RIMO-P, features 456 proof problems with expert-checked solutions, which\nare decomposed into a sequence of sub-problems to evaluate the step-by-step\nreasoning process via an automated grading system. Our benchmarking of ten\nfrontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these\nsystems excel on older benchmarks, their performance drops sharply on RIMO.\nThese results highlight a substantial gap between current LLM capabilities and\nactual Olympiad-level reasoning. By providing a challenging yet\neasy-to-evaluate suite, RIMO offers a high-resolution yardstick for future\nresearch, presenting a clear target for closing the profound reasoning gap our\nfindings expose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) reach high scores on established mathematical\nbenchmarks, such as GSM8K and MATH, the research community has turned to\nInternational Mathematical Olympiad (IMO) problems to push the evaluation\nfrontier. However, existing Olympiad-level benchmarks suffer from practical\nconstraints that introduce grading noise and potential bias, such as\nheterogeneous answer formats requiring model-based judges and a reliance on\npotentially flawed solutions. We introduce RIMO, a two-track benchmark designed\nto preserve peak Olympiad difficulty while eliminating this evaluation noise.\nThe first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique\ninteger answer, allowing for deterministic correctness checking. The second\ntrack, RIMO-P, features 456 proof problems with expert-checked solutions, which\nare decomposed into a sequence of sub-problems to evaluate the step-by-step\nreasoning process via an automated grading system. Our benchmarking of ten\nfrontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these\nsystems excel on older benchmarks, their performance drops sharply on RIMO.\nThese results highlight a substantial gap between current LLM capabilities and\nactual Olympiad-level reasoning. By providing a challenging yet\neasy-to-evaluate suite, RIMO offers a high-resolution yardstick for future\nresearch, presenting a clear target for closing the profound reasoning gap our\nfindings expose."
                },
                "authors": [
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Yao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Shu"
                },
                "author": "Yao Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05148v2",
                "updated": "2025-09-09T13:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    5,
                    27,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-05T14:38:18Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    38,
                    18,
                    4,
                    248,
                    0
                ],
                "title": "Bulk viscosity from early-time thermalization of cosmic fluids in light\n  of DESI DR2 data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bulk viscosity from early-time thermalization of cosmic fluids in light\n  of DESI DR2 data"
                },
                "summary": "If nonrelativistic dark matter and radiation are allowed to interact,\nreaching an approximate thermal equilibrium, this interaction induces a bulk\nviscous pressure changing the effective one-fluid description of the universe\ndynamics. By modeling such components as perfect fluids, a cosmologically\nrelevant bulk viscous pressure emerges for dark matter particle masses in the\nrange of $1\\,\\text{eV} - 10\\,\\text{eV}$ keeping thermal equilibrium with the\nradiation. Such a transient bulk viscosity introduces significant effects in\nthe expansion rate near the matter-radiation equality (redshift $z_{\\text{eq}}\n\\sim 3400$) and at late times (leading to a higher inferred value of the Hubble\nconstant $H_0$). We use the recent DESI DR2 BAO data to place an upper bound on\nthe free parameter of the model $\\tau_\\text{eq}$ which represents the time\nscale in which each component follows its own internal perfect fluid dynamics\nuntil thermalization occurs. Our main result is encoded in the bound\n$\\tau_\\text{eq} < 1.84 \\times 10^{-10} $ s (2$\\sigma$), with the corresponding\ndimensionless bulk coefficient $\\tilde{\\xi} H_0/H_\\text{eq}<5.94\\times10^{-4}$\n(2$\\sigma$). In practice, this rules out any possible interaction between\nradiation and dark matter prior to the recombination epoch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If nonrelativistic dark matter and radiation are allowed to interact,\nreaching an approximate thermal equilibrium, this interaction induces a bulk\nviscous pressure changing the effective one-fluid description of the universe\ndynamics. By modeling such components as perfect fluids, a cosmologically\nrelevant bulk viscous pressure emerges for dark matter particle masses in the\nrange of $1\\,\\text{eV} - 10\\,\\text{eV}$ keeping thermal equilibrium with the\nradiation. Such a transient bulk viscosity introduces significant effects in\nthe expansion rate near the matter-radiation equality (redshift $z_{\\text{eq}}\n\\sim 3400$) and at late times (leading to a higher inferred value of the Hubble\nconstant $H_0$). We use the recent DESI DR2 BAO data to place an upper bound on\nthe free parameter of the model $\\tau_\\text{eq}$ which represents the time\nscale in which each component follows its own internal perfect fluid dynamics\nuntil thermalization occurs. Our main result is encoded in the bound\n$\\tau_\\text{eq} < 1.84 \\times 10^{-10} $ s (2$\\sigma$), with the corresponding\ndimensionless bulk coefficient $\\tilde{\\xi} H_0/H_\\text{eq}<5.94\\times10^{-4}$\n(2$\\sigma$). In practice, this rules out any possible interaction between\nradiation and dark matter prior to the recombination epoch."
                },
                "authors": [
                    {
                        "name": "Hermano Velten"
                    },
                    {
                        "name": "William Iania"
                    }
                ],
                "author_detail": {
                    "name": "William Iania"
                },
                "author": "William Iania",
                "arxiv_comment": "7 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v4",
                "updated": "2025-09-09T13:01:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    1,
                    7,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Xiaohua Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Jia"
                },
                "author": "Xiaohua Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03547v2",
                "updated": "2025-09-09T12:55:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    55,
                    2,
                    1,
                    252,
                    0
                ],
                "published": "2024-12-04T18:45:30Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    30,
                    2,
                    339,
                    0
                ],
                "title": "Learning from galactic rotation curves: a neural network approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from galactic rotation curves: a neural network approach"
                },
                "summary": "For a galaxy, given its observed rotation curve, can one directly infer\nparameters of the dark matter density profile (such as dark matter particle\nmass $m$, scaling parameter $s$, core-to-envelope transition radius $r_t$ and\nNFW scale radius $r_s$), along with Baryonic parameters (such as the stellar\nmass-to-light ratio $\\Upsilon_*$)? In this work, using simulated rotation\ncurves, we train neural networks, which can then be fed observed rotation\ncurves of dark matter dominated dwarf galaxies from the SPARC catalog, to infer\nparameter values and their uncertainties. Since observed rotation curves have\nerrors, we also explore the very important effect of noise in the training data\non the inference. We employ two different methods to quantify uncertainties in\nthe estimated parameters, and compare the results with those obtained using\nBayesian methods. We find that the trained neural networks can extract\nparameters that describe observations well for the galaxies we studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a galaxy, given its observed rotation curve, can one directly infer\nparameters of the dark matter density profile (such as dark matter particle\nmass $m$, scaling parameter $s$, core-to-envelope transition radius $r_t$ and\nNFW scale radius $r_s$), along with Baryonic parameters (such as the stellar\nmass-to-light ratio $\\Upsilon_*$)? In this work, using simulated rotation\ncurves, we train neural networks, which can then be fed observed rotation\ncurves of dark matter dominated dwarf galaxies from the SPARC catalog, to infer\nparameter values and their uncertainties. Since observed rotation curves have\nerrors, we also explore the very important effect of noise in the training data\non the inference. We employ two different methods to quantify uncertainties in\nthe estimated parameters, and compare the results with those obtained using\nBayesian methods. We find that the trained neural networks can extract\nparameters that describe observations well for the galaxies we studied."
                },
                "authors": [
                    {
                        "name": "Bihag Dave"
                    },
                    {
                        "name": "Gaurav Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Goswami"
                },
                "author": "Gaurav Goswami",
                "arxiv_comment": "30 pages, 11 figures. Updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16665v3",
                "updated": "2025-09-09T12:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    54,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-20T22:27:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    22,
                    27,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust but Verify! A Survey on Verification Design for Test-time Scaling"
                },
                "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io."
                },
                "authors": [
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07676v1",
                "updated": "2025-09-09T12:43:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    43,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:43:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    43,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Unleashing the True Potential of LLMs: A Feedback-Triggered\n  Self-Correction with Long-Term Multipath Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the True Potential of LLMs: A Feedback-Triggered\n  Self-Correction with Long-Term Multipath Decoding"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods."
                },
                "authors": [
                    {
                        "name": "Jipeng Li"
                    },
                    {
                        "name": "Zeyu Gao"
                    },
                    {
                        "name": "Yubin Qi"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Weijian Chen"
                    },
                    {
                        "name": "Qiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Lin"
                },
                "author": "Qiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.10960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.10960v3",
                "updated": "2025-09-09T12:42:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    42,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2022-01-26T14:24:34Z",
                "published_parsed": [
                    2022,
                    1,
                    26,
                    14,
                    24,
                    34,
                    2,
                    26,
                    0
                ],
                "title": "Triplication: an important component of the modern scientific method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triplication: an important component of the modern scientific method"
                },
                "summary": "A scientific-study protocol, as defined here, is designed to deliver results\nfrom which inductive inference is allowed. In the nineteenth century,\ntriplication was introduced into the plant sciences and Fisher's p<0.05 rule\n(1925) was incorporated into a triple-result protocol designed to counter\nrandom/systematic errors. Aims here were to: (1) classify replication\n(including non-replicated) protocols; (2) assess their prevalence in\nplant-science studies published in 2017 for a defined variable construct; and\n(3) explore the theoretical rationale for the use of triplication. Methods: the\nplant sciences were surveyed and a protocol-prevalence report produced;\nassociation versus experimental proportions were analyzed; real-world-data\nproxies were used to show confidence-interval-width patterns with increasing\nreplicate number. Results: triplication was found in ~70% of plant-science\nstudies analyzed, with triple-result protocols observed in ~15%. Theoretical\nconsiderations showed that, even if systematic errors predominate, \"square-root\nrules\" sometimes apply, contributing to the predominance of triplication\n(real-world-data proxy examples given). Conclusions: Triplication was\nextensively applied in the studies analyzed and there are strong methodological\nreasons why (1) triplication, rather than duplication/quadruplication, is the\nmost appropriate standard; (2) triple-result protocols are preferable to global\naveraging approaches; and (3) plant science methodological standards remain\nhigh, despite immense publication pressures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scientific-study protocol, as defined here, is designed to deliver results\nfrom which inductive inference is allowed. In the nineteenth century,\ntriplication was introduced into the plant sciences and Fisher's p<0.05 rule\n(1925) was incorporated into a triple-result protocol designed to counter\nrandom/systematic errors. Aims here were to: (1) classify replication\n(including non-replicated) protocols; (2) assess their prevalence in\nplant-science studies published in 2017 for a defined variable construct; and\n(3) explore the theoretical rationale for the use of triplication. Methods: the\nplant sciences were surveyed and a protocol-prevalence report produced;\nassociation versus experimental proportions were analyzed; real-world-data\nproxies were used to show confidence-interval-width patterns with increasing\nreplicate number. Results: triplication was found in ~70% of plant-science\nstudies analyzed, with triple-result protocols observed in ~15%. Theoretical\nconsiderations showed that, even if systematic errors predominate, \"square-root\nrules\" sometimes apply, contributing to the predominance of triplication\n(real-world-data proxy examples given). Conclusions: Triplication was\nextensively applied in the studies analyzed and there are strong methodological\nreasons why (1) triplication, rather than duplication/quadruplication, is the\nmost appropriate standard; (2) triple-result protocols are preferable to global\naveraging approaches; and (3) plant science methodological standards remain\nhigh, despite immense publication pressures."
                },
                "authors": [
                    {
                        "name": "Jeremy S. C. Clark"
                    },
                    {
                        "name": "Karina Szczypiór-Piasecka"
                    },
                    {
                        "name": "Kamila Rydzewska"
                    },
                    {
                        "name": "Konrad Podsiadło"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Podsiadło"
                },
                "author": "Konrad Podsiadło",
                "arxiv_comment": "41 pages, 6 figures. Supplemental files at\n  https://github.com/Abiologist/Triplication.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.10960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.10960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07650v1",
                "updated": "2025-09-09T12:17:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    17,
                    10,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:17:10Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    17,
                    10,
                    1,
                    252,
                    0
                ],
                "title": "Inference of Intrinsic Rewards and Fairness in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of Intrinsic Rewards and Fairness in Multi-Agent Systems"
                },
                "summary": "From altruism to antagonism, fairness plays a central role in social\ninteractions. But can we truly understand how fair someone is, especially\nwithout explicit knowledge of their preferences? We cast this challenge as a\nmulti-agent inverse reinforcement learning problem, explicitly structuring\nrewards to reflect how agents value the welfare of others. We introduce novel\nBayesian strategies, reasoning about the optimality of demonstrations and\ncharacterisation of equilibria in general-sum Markov games. Our experiments,\nspanning randomised environments and a collaborative cooking task, reveal that\ncoherent notions of fairness can be reliably inferred from demonstrations.\nFurthermore, when isolating fairness components, we obtain a disentangled\nunderstanding of agents preferences. Crucially, we unveil that by placing\nagents in different groups, we can force them to exhibit new facets of their\nreward structures, cutting through ambiguity to answer the central question:\nwho is being fair?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From altruism to antagonism, fairness plays a central role in social\ninteractions. But can we truly understand how fair someone is, especially\nwithout explicit knowledge of their preferences? We cast this challenge as a\nmulti-agent inverse reinforcement learning problem, explicitly structuring\nrewards to reflect how agents value the welfare of others. We introduce novel\nBayesian strategies, reasoning about the optimality of demonstrations and\ncharacterisation of equilibria in general-sum Markov games. Our experiments,\nspanning randomised environments and a collaborative cooking task, reveal that\ncoherent notions of fairness can be reliably inferred from demonstrations.\nFurthermore, when isolating fairness components, we obtain a disentangled\nunderstanding of agents preferences. Crucially, we unveil that by placing\nagents in different groups, we can force them to exhibit new facets of their\nreward structures, cutting through ambiguity to answer the central question:\nwho is being fair?"
                },
                "authors": [
                    {
                        "name": "Victor Villin"
                    },
                    {
                        "name": "Christos Dimitrakakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Dimitrakakis"
                },
                "author": "Christos Dimitrakakis",
                "arxiv_comment": "EWRL18 (2025) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07642v1",
                "updated": "2025-09-09T12:10:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    10,
                    14,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:10:14Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    10,
                    14,
                    1,
                    252,
                    0
                ],
                "title": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment"
                },
                "summary": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space."
                },
                "authors": [
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Oliver Müller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Müller"
                },
                "author": "Oliver Müller",
                "arxiv_comment": "Presented at the 19th International Conference on\n  Wirtschaftsinformatik 2024, W\\\"urzburg, Germany\n  https://aisel.aisnet.org/wi2024/91/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12141v2",
                "updated": "2025-09-09T11:59:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    59,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-13T18:02:41Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    18,
                    2,
                    41,
                    4,
                    164,
                    0
                ],
                "title": "A big red dot at cosmic noon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A big red dot at cosmic noon"
                },
                "summary": "We report the discovery of a little red dot (LRD), dubbed BiRD ('big red\ndot'), at $z=2.33$ in the field around the $z=6.3$ quasar SDSSJ1030+0524. Using\nNIRCam images, we identified it as a bright outlier in the $F200W-F356W$ color\nvs $F356W$ magnitude diagram of point sources in the field. The NIRCam/WFSS\nspectrum reveals the emission from HeI$\\lambda 10830$ and PaG line, both\nshowing a narrow and a broad ($FWHM\\gtrsim 2000\\ \\rm kms^{-1}$) component. The\nHeI line is affected by an absorption feature, tracing dense gas with HeI\ncolumn density in the $2^3S$ level $N\\sim 0.5-1.2\\times 10^{14}\\rm cm^{-2}$,\ndepending on the location of the absorber, which is outflowing at the speed of\n$\\Delta v \\sim -830\\ \\rm kms^{-1}$. As observed in the majority of LRDs, BiRD\ndoes not show X-ray or radio emission. The BH mass and the bolometric\nluminosity, both inferred from the PaG broad component, amount to $M_{\\rm\nBH}\\sim 10^8\\rm M_{\\odot}$ and $L_{\\rm bol}\\sim 2.9\\times 10^{45}\\rm\nergs^{-1}$, respectively. Intriguingly, BiRD presents strict analogies with\nother two LRDs spectroscopically confirmed at cosmic noon, GN-28074 (\"Rosetta\nStone\") at $z=2.26$ and RUBIES-BLAGN-1 at $z=3.1$. The blueshifted HeI\nabsorption detected in all three sources suggests that gas outflows may be\ncommon in LRDs. We derive a first estimate of the space density of LRDs at\n$z<3$ based on JWST data, as a function of $L_{\\rm bol}$ and BH mass. The space\ndensity is only a factor of $\\sim 2-3$ lower than that of UV-selected quasars\nwith comparable $L_{\\rm bol}$ and $z$, meaning that the contribution of LRDs to\nthe broader AGN population is also relevant at cosmic noon. A similar trend is\nalso observed in terms of BH masses. If, as suggested by recent theories, LRDs\nprobe the very first and rapid growth of black hole seeds, our finding may\nsuggest that the formation of black hole seeds remains efficient at least up to\ncosmic noon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the discovery of a little red dot (LRD), dubbed BiRD ('big red\ndot'), at $z=2.33$ in the field around the $z=6.3$ quasar SDSSJ1030+0524. Using\nNIRCam images, we identified it as a bright outlier in the $F200W-F356W$ color\nvs $F356W$ magnitude diagram of point sources in the field. The NIRCam/WFSS\nspectrum reveals the emission from HeI$\\lambda 10830$ and PaG line, both\nshowing a narrow and a broad ($FWHM\\gtrsim 2000\\ \\rm kms^{-1}$) component. The\nHeI line is affected by an absorption feature, tracing dense gas with HeI\ncolumn density in the $2^3S$ level $N\\sim 0.5-1.2\\times 10^{14}\\rm cm^{-2}$,\ndepending on the location of the absorber, which is outflowing at the speed of\n$\\Delta v \\sim -830\\ \\rm kms^{-1}$. As observed in the majority of LRDs, BiRD\ndoes not show X-ray or radio emission. The BH mass and the bolometric\nluminosity, both inferred from the PaG broad component, amount to $M_{\\rm\nBH}\\sim 10^8\\rm M_{\\odot}$ and $L_{\\rm bol}\\sim 2.9\\times 10^{45}\\rm\nergs^{-1}$, respectively. Intriguingly, BiRD presents strict analogies with\nother two LRDs spectroscopically confirmed at cosmic noon, GN-28074 (\"Rosetta\nStone\") at $z=2.26$ and RUBIES-BLAGN-1 at $z=3.1$. The blueshifted HeI\nabsorption detected in all three sources suggests that gas outflows may be\ncommon in LRDs. We derive a first estimate of the space density of LRDs at\n$z<3$ based on JWST data, as a function of $L_{\\rm bol}$ and BH mass. The space\ndensity is only a factor of $\\sim 2-3$ lower than that of UV-selected quasars\nwith comparable $L_{\\rm bol}$ and $z$, meaning that the contribution of LRDs to\nthe broader AGN population is also relevant at cosmic noon. A similar trend is\nalso observed in terms of BH masses. If, as suggested by recent theories, LRDs\nprobe the very first and rapid growth of black hole seeds, our finding may\nsuggest that the formation of black hole seeds remains efficient at least up to\ncosmic noon."
                },
                "authors": [
                    {
                        "name": "Federica Loiacono"
                    },
                    {
                        "name": "Roberto Gilli"
                    },
                    {
                        "name": "Marco Mignoli"
                    },
                    {
                        "name": "Giovanni Mazzolari"
                    },
                    {
                        "name": "Roberto Decarli"
                    },
                    {
                        "name": "Marcella Brusa"
                    },
                    {
                        "name": "Francesco Calura"
                    },
                    {
                        "name": "Marco Chiaberge"
                    },
                    {
                        "name": "Andrea Comastri"
                    },
                    {
                        "name": "Quirino D'Amato"
                    },
                    {
                        "name": "Kazushi Iwasawa"
                    },
                    {
                        "name": "Ignas Juodžbalis"
                    },
                    {
                        "name": "Giorgio Lanzuisi"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Stefano Marchesi"
                    },
                    {
                        "name": "Colin Norman"
                    },
                    {
                        "name": "Alessandro Peca"
                    },
                    {
                        "name": "Isabella Prandoni"
                    },
                    {
                        "name": "Matteo Sapori"
                    },
                    {
                        "name": "Matilde Signorini"
                    },
                    {
                        "name": "Paolo Tozzi"
                    },
                    {
                        "name": "Eros Vanzella"
                    },
                    {
                        "name": "Cristian Vignali"
                    },
                    {
                        "name": "Fabio Vito"
                    },
                    {
                        "name": "Gianni Zamorani"
                    }
                ],
                "author_detail": {
                    "name": "Gianni Zamorani"
                },
                "author": "Gianni Zamorani",
                "arxiv_comment": "14 pages, 7 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07622v1",
                "updated": "2025-09-09T11:52:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    52,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:52:16Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    52,
                    16,
                    1,
                    252,
                    0
                ],
                "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs"
                },
                "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians."
                },
                "authors": [
                    {
                        "name": "Libo Ren"
                    },
                    {
                        "name": "Yee Man Ng"
                    },
                    {
                        "name": "Lifeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Han"
                },
                "author": "Lifeng Han",
                "arxiv_comment": "system paper at CLEF 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07617v1",
                "updated": "2025-09-09T11:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    42,
                    6,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:42:06Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    42,
                    6,
                    1,
                    252,
                    0
                ],
                "title": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling"
                },
                "summary": "Direct Prompt Injection (DPI) attacks pose a critical security threat to\nLarge Language Models (LLMs) due to their low barrier of execution and high\npotential damage. To address the impracticality of existing white-box/gray-box\nmethods and the poor transferability of black-box methods, we propose an\nactivations-guided prompt injection attack framework. We first construct an\nEnergy-based Model (EBM) using activations from a surrogate model to evaluate\nthe quality of adversarial prompts. Guided by the trained EBM, we employ the\ntoken-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize\nadversarial prompts, thereby enabling gradient-free black-box attacks.\nExperimental results demonstrate our superior cross-model transferability,\nachieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%\nimprovement over human-crafted prompts, and maintaining 36.6% ASR on unseen\ntask scenarios. Interpretability analysis reveals a correlation between\nactivations and attack effectiveness, highlighting the critical role of\nsemantic patterns in transferable vulnerability exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Prompt Injection (DPI) attacks pose a critical security threat to\nLarge Language Models (LLMs) due to their low barrier of execution and high\npotential damage. To address the impracticality of existing white-box/gray-box\nmethods and the poor transferability of black-box methods, we propose an\nactivations-guided prompt injection attack framework. We first construct an\nEnergy-based Model (EBM) using activations from a surrogate model to evaluate\nthe quality of adversarial prompts. Guided by the trained EBM, we employ the\ntoken-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize\nadversarial prompts, thereby enabling gradient-free black-box attacks.\nExperimental results demonstrate our superior cross-model transferability,\nachieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%\nimprovement over human-crafted prompts, and maintaining 36.6% ASR on unseen\ntask scenarios. Interpretability analysis reveals a correlation between\nactivations and attack effectiveness, highlighting the critical role of\nsemantic patterns in transferable vulnerability exploitation."
                },
                "authors": [
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Yechao Zhang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "pei Xiaobing"
                    },
                    {
                        "name": "Jing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Wang"
                },
                "author": "Jing Wang",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07604v1",
                "updated": "2025-09-09T11:25:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    25,
                    55,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:25:55Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    25,
                    55,
                    1,
                    252,
                    0
                ],
                "title": "K2-Think: A Parameter-Efficient Reasoning System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K2-Think: A Parameter-Efficient Reasoning System"
                },
                "summary": "K2-Think is a reasoning system that achieves state-of-the-art performance\nwith a 32B parameter model, matching or surpassing much larger models like\nGPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system\nshows that smaller models can compete at the highest levels by combining\nadvanced post-training and test-time computation techniques. The approach is\nbased on six key technical pillars: Long Chain-of-thought Supervised\nFinetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and\nInference-optimized Hardware, all using publicly available open-source\ndatasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art\nscores on public benchmarks for open-source models, while also performing\nstrongly in other areas such as Code and Science. Our results confirm that a\nmore parameter-efficient model like K2-Think 32B can compete with\nstate-of-the-art systems through an integrated post-training recipe that\nincludes long chain-of-thought training and strategic inference-time\nenhancements, making open-source reasoning systems more accessible and\naffordable. K2-Think is freely available at k2think.ai, offering best-in-class\ninference speeds of over 2,000 tokens per second per request via the Cerebras\nWafer-Scale Engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K2-Think is a reasoning system that achieves state-of-the-art performance\nwith a 32B parameter model, matching or surpassing much larger models like\nGPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system\nshows that smaller models can compete at the highest levels by combining\nadvanced post-training and test-time computation techniques. The approach is\nbased on six key technical pillars: Long Chain-of-thought Supervised\nFinetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and\nInference-optimized Hardware, all using publicly available open-source\ndatasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art\nscores on public benchmarks for open-source models, while also performing\nstrongly in other areas such as Code and Science. Our results confirm that a\nmore parameter-efficient model like K2-Think 32B can compete with\nstate-of-the-art systems through an integrated post-training recipe that\nincludes long chain-of-thought training and strategic inference-time\nenhancements, making open-source reasoning systems more accessible and\naffordable. K2-Think is freely available at k2think.ai, offering best-in-class\ninference speeds of over 2,000 tokens per second per request via the Cerebras\nWafer-Scale Engine."
                },
                "authors": [
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Taylor W. Killian"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Hector Ren"
                    },
                    {
                        "name": "Alexander Moreno"
                    },
                    {
                        "name": "Daqian Zhang"
                    },
                    {
                        "name": "Tianjun Zhong"
                    },
                    {
                        "name": "Yuxin Xiong"
                    },
                    {
                        "name": "Yuanzhe Hu"
                    },
                    {
                        "name": "Yutao Xie"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Varad Pimpalkhute"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Aaryamonvikram Singh"
                    },
                    {
                        "name": "Xuezhi Liang"
                    },
                    {
                        "name": "Anze Xie"
                    },
                    {
                        "name": "Jianshu She"
                    },
                    {
                        "name": "Desai Fan"
                    },
                    {
                        "name": "Chengqian Gao"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "John Maggs"
                    },
                    {
                        "name": "Xuezhe Ma"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric P. Xing"
                },
                "author": "Eric P. Xing",
                "arxiv_comment": "To access the K2-Think reasoning system, please visit\n  https://k2think.ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09076v3",
                "updated": "2025-09-09T11:09:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    9,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-10T02:41:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    41,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "A Probabilistic Framework for Imputing Genetic Distances in\n  Spatiotemporal Pathogen Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Framework for Imputing Genetic Distances in\n  Spatiotemporal Pathogen Models"
                },
                "summary": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows."
                },
                "authors": [
                    {
                        "name": "Haley Stone"
                    },
                    {
                        "name": "Jing Du"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Matthew Scotch"
                    },
                    {
                        "name": "David Heslop"
                    },
                    {
                        "name": "Andreas Züfle"
                    },
                    {
                        "name": "Chandini Raina MacIntyre"
                    },
                    {
                        "name": "Flora Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora Salim"
                },
                "author": "Flora Salim",
                "arxiv_doi": "10.1145/3748636.3762779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3748636.3762779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures | Accepted as a full paper in SIGSPATIAL 2025",
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07595v1",
                "updated": "2025-09-09T11:07:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    7,
                    50,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:07:50Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    7,
                    50,
                    1,
                    252,
                    0
                ],
                "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\n  FaaS-hosted MCP Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\n  FaaS-hosted MCP Services"
                },
                "summary": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows."
                },
                "authors": [
                    {
                        "name": "Shiva Sai Krishna Anand Tokal"
                    },
                    {
                        "name": "Vaibhav Jha"
                    },
                    {
                        "name": "Anand Eswaran"
                    },
                    {
                        "name": "Praveen Jayachandran"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07594v1",
                "updated": "2025-09-09T11:06:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    6,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:06:37Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    6,
                    37,
                    1,
                    252,
                    0
                ],
                "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction"
                },
                "summary": "Click-through rate (CTR) prediction plays an important role in online\nadvertising systems. On the one hand, traditional CTR prediction models capture\nthe collaborative signals in tabular data via feature interaction modeling, but\nthey lose semantics in text. On the other hand, Large Language Models (LLMs)\nexcel in understanding the context and meaning behind text, but they face\nchallenges in capturing collaborative signals and they have long inference\nlatency. In this paper, we aim to leverage the benefits of both types of models\nand pursue collaboration, semantics and efficiency. We present ELEC, which is\nan Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for\nthe CTR prediction task. In order to leverage the ability of the LLM but\nsimultaneously keep efficiency, we utilize the pseudo-siamese network which\ncontains a gain network and a vanilla network. We inject the high-level\nrepresentation vector generated by the LLM into a collaborative CTR model to\nform the gain network such that it can take advantage of both tabular modeling\nand textual modeling. However, its reliance on the LLM limits its efficiency.\nWe then distill the knowledge from the gain network to the vanilla network on\nboth the score level and the representation level, such that the vanilla\nnetwork takes only tabular data as input, but can still generate comparable\nperformance as the gain network. Our approach is model-agnostic. It allows for\nthe integration with various existing LLMs and collaborative CTR models.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof ELEC for CTR prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-through rate (CTR) prediction plays an important role in online\nadvertising systems. On the one hand, traditional CTR prediction models capture\nthe collaborative signals in tabular data via feature interaction modeling, but\nthey lose semantics in text. On the other hand, Large Language Models (LLMs)\nexcel in understanding the context and meaning behind text, but they face\nchallenges in capturing collaborative signals and they have long inference\nlatency. In this paper, we aim to leverage the benefits of both types of models\nand pursue collaboration, semantics and efficiency. We present ELEC, which is\nan Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for\nthe CTR prediction task. In order to leverage the ability of the LLM but\nsimultaneously keep efficiency, we utilize the pseudo-siamese network which\ncontains a gain network and a vanilla network. We inject the high-level\nrepresentation vector generated by the LLM into a collaborative CTR model to\nform the gain network such that it can take advantage of both tabular modeling\nand textual modeling. However, its reliance on the LLM limits its efficiency.\nWe then distill the knowledge from the gain network to the vanilla network on\nboth the score level and the representation level, such that the vanilla\nnetwork takes only tabular data as input, but can still generate comparable\nperformance as the gain network. Our approach is model-agnostic. It allows for\nthe integration with various existing LLMs and collaborative CTR models.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof ELEC for CTR prediction."
                },
                "authors": [
                    {
                        "name": "Rui Dong"
                    },
                    {
                        "name": "Wentao Ouyang"
                    },
                    {
                        "name": "Xiangzheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzheng Liu"
                },
                "author": "Xiangzheng Liu",
                "arxiv_doi": "10.1145/3726302.3730188",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730188",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.07594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03303v2",
                "updated": "2025-09-09T11:02:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    2,
                    55,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-06T08:36:01Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "title": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices"
                },
                "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms."
                },
                "authors": [
                    {
                        "name": "Tasnim Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Shahriar"
                },
                "author": "Tasnim Shahriar",
                "arxiv_comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-XX (Primary) 68Txx, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24197v3",
                "updated": "2025-09-09T11:01:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    1,
                    32,
                    1,
                    252,
                    0
                ],
                "published": "2024-10-31T17:55:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "Generative modelling for mass-mapping with fast uncertainty\n  quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modelling for mass-mapping with fast uncertainty\n  quantification"
                },
                "summary": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second."
                },
                "authors": [
                    {
                        "name": "Jessica J. Whitney"
                    },
                    {
                        "name": "Tobías I. Liaudat"
                    },
                    {
                        "name": "Matthew A. Price"
                    },
                    {
                        "name": "Matthijs Mars"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_doi": "10.1093/mnras/staf1356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.24197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated to the MNRAS published paper on 9/9/25",
                "arxiv_journal_ref": "Mon Not R Astron Soc (2025) 2464-2479",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07588v1",
                "updated": "2025-09-09T10:59:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    59,
                    47,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T10:59:47Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    59,
                    47,
                    1,
                    252,
                    0
                ],
                "title": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment"
                },
                "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts."
                },
                "authors": [
                    {
                        "name": "Andrey Sakhovskiy"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_doi": "10.1145/3726302.3729901",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729901",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.07588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"",
                "arxiv_journal_ref": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (2025). Association for\n  Computing Machinery, 1152-1164",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12791v2",
                "updated": "2025-09-09T10:55:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    55,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2024-06-18T09:15:46Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    15,
                    46,
                    1,
                    170,
                    0
                ],
                "title": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour."
                },
                "authors": [
                    {
                        "name": "Qikai Wei"
                    },
                    {
                        "name": "Mingzhi Yang"
                    },
                    {
                        "name": "Jinqiang Wang"
                    },
                    {
                        "name": "Wenwei Mao"
                    },
                    {
                        "name": "Jiabo Xu"
                    },
                    {
                        "name": "Huansheng Ning"
                    }
                ],
                "author_detail": {
                    "name": "Huansheng Ning"
                },
                "author": "Huansheng Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19667v3",
                "updated": "2025-09-09T10:38:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    38,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2025-04-28T10:43:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "Tripartite-GraphRAG via Plugin Ontologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tripartite-GraphRAG via Plugin Ontologies"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs."
                },
                "authors": [
                    {
                        "name": "Michael Banf"
                    },
                    {
                        "name": "Johannes Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Kuhn"
                },
                "author": "Johannes Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07391v3",
                "updated": "2025-09-09T10:16:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    16,
                    0,
                    1,
                    252,
                    0
                ],
                "published": "2024-12-10T10:33:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "A Data-Free Analytical Quantization Scheme for Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-Free Analytical Quantization Scheme for Deep Learning Models"
                },
                "summary": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource-constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on real-world datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource-constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on real-world datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy."
                },
                "authors": [
                    {
                        "name": "Ahmed Luqman"
                    },
                    {
                        "name": "Khuzemah Qazi"
                    },
                    {
                        "name": "Murray Patterson"
                    },
                    {
                        "name": "Malik Jahan Khan"
                    },
                    {
                        "name": "Imdadullah Khan"
                    }
                ],
                "author_detail": {
                    "name": "Imdadullah Khan"
                },
                "author": "Imdadullah Khan",
                "arxiv_comment": "Accepted for publication in IEEE International Conference on Data\n  Mining (ICDM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07571v1",
                "updated": "2025-09-09T10:15:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    15,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T10:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    15,
                    42,
                    1,
                    252,
                    0
                ],
                "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive\n  and Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive\n  and Efficient Inference"
                },
                "summary": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches."
                },
                "authors": [
                    {
                        "name": "Xiyu Guo"
                    },
                    {
                        "name": "Shan Wang"
                    },
                    {
                        "name": "Chunfang Ji"
                    },
                    {
                        "name": "Xuefeng Zhao"
                    },
                    {
                        "name": "Wenhao Xi"
                    },
                    {
                        "name": "Yaoyao Liu"
                    },
                    {
                        "name": "Qinglan Li"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21141v2",
                "updated": "2025-09-09T09:54:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    54,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-28T18:18:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    18,
                    18,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Adaptive LLM Routing under Budget Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive LLM Routing under Budget Constraints"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their varying capabilities and costs pose challenges in practical\napplications. LLM routing addresses this by dynamically selecting the most\nsuitable LLM for each query/task. Previous approaches treat this as a\nsupervised learning problem, assuming complete knowledge of optimal query-LLM\npairings. However, real-world scenarios lack such comprehensive mappings and\nface evolving user queries. We thus propose to study LLM routing as a\ncontextual bandit problem, enabling adaptive decision-making using bandit\nfeedback without requiring exhaustive inference across all LLMs for all queries\n(in contrast to supervised routing). To address this problem, we develop a\nshared embedding space for queries and LLMs, where query and LLM embeddings are\naligned to reflect their affinity. This space is initially learned from offline\nhuman preference data and refined through online bandit feedback. We\ninstantiate this idea through Preference-prior Informed Linucb fOr adaptive\nrouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets\nfor model routing, we introduce an online cost policy modeled as a multi-choice\nknapsack problem, ensuring resource-efficient routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their varying capabilities and costs pose challenges in practical\napplications. LLM routing addresses this by dynamically selecting the most\nsuitable LLM for each query/task. Previous approaches treat this as a\nsupervised learning problem, assuming complete knowledge of optimal query-LLM\npairings. However, real-world scenarios lack such comprehensive mappings and\nface evolving user queries. We thus propose to study LLM routing as a\ncontextual bandit problem, enabling adaptive decision-making using bandit\nfeedback without requiring exhaustive inference across all LLMs for all queries\n(in contrast to supervised routing). To address this problem, we develop a\nshared embedding space for queries and LLMs, where query and LLM embeddings are\naligned to reflect their affinity. This space is initially learned from offline\nhuman preference data and refined through online bandit feedback. We\ninstantiate this idea through Preference-prior Informed Linucb fOr adaptive\nrouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets\nfor model routing, we introduce an online cost policy modeled as a multi-choice\nknapsack problem, ensuring resource-efficient routing."
                },
                "authors": [
                    {
                        "name": "Pranoy Panda"
                    },
                    {
                        "name": "Raghav Magazine"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    },
                    {
                        "name": "Sho Takemori"
                    },
                    {
                        "name": "Vishal Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Sharma"
                },
                "author": "Vishal Sharma",
                "arxiv_comment": "Accepted at EMNLP 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07558v1",
                "updated": "2025-09-09T09:52:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    52,
                    34,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:52:34Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    52,
                    34,
                    1,
                    252,
                    0
                ],
                "title": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR"
                },
                "summary": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization."
                },
                "authors": [
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07555v1",
                "updated": "2025-09-09T09:49:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    49,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:49:23Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    49,
                    23,
                    1,
                    252,
                    0
                ],
                "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition"
                },
                "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xiangrong Zhu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Wei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hu"
                },
                "author": "Wei Hu",
                "arxiv_comment": "Accepted in EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07552v1",
                "updated": "2025-09-09T09:42:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    42,
                    31,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:42:31Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    42,
                    31,
                    1,
                    252,
                    0
                ],
                "title": "PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from\n  One-shot Unposed Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from\n  One-shot Unposed Image"
                },
                "summary": "We present a feed-forward framework for Gaussian full-head synthesis from a\nsingle unposed image. Unlike previous work that relies on time-consuming GAN\ninversion and test-time optimization, our framework can reconstruct the\nGaussian full-head model given a single unposed image in a single forward pass.\nThis enables fast reconstruction and rendering during inference. To mitigate\nthe lack of large-scale 3D head assets, we propose a large-scale synthetic\ndataset from trained 3D GANs and train our framework using only synthetic data.\nFor efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian\nhead generation pipeline, where sparse points from the FLAME model interact\nwith the image features by transformer blocks for feature extraction and coarse\nshape reconstruction, which are then densified for high-fidelity\nreconstruction. To fully leverage the prior knowledge residing in pretrained 3D\nGANs for effective reconstruction, we propose a dual-branch framework that\neffectively aggregates the structured spherical triplane feature and\nunstructured point-based features for more effective Gaussian head\nreconstruction. Experimental results show the effectiveness of our framework\ntowards existing work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a feed-forward framework for Gaussian full-head synthesis from a\nsingle unposed image. Unlike previous work that relies on time-consuming GAN\ninversion and test-time optimization, our framework can reconstruct the\nGaussian full-head model given a single unposed image in a single forward pass.\nThis enables fast reconstruction and rendering during inference. To mitigate\nthe lack of large-scale 3D head assets, we propose a large-scale synthetic\ndataset from trained 3D GANs and train our framework using only synthetic data.\nFor efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian\nhead generation pipeline, where sparse points from the FLAME model interact\nwith the image features by transformer blocks for feature extraction and coarse\nshape reconstruction, which are then densified for high-fidelity\nreconstruction. To fully leverage the prior knowledge residing in pretrained 3D\nGANs for effective reconstruction, we propose a dual-branch framework that\neffectively aggregates the structured spherical triplane feature and\nunstructured point-based features for more effective Gaussian head\nreconstruction. Experimental results show the effectiveness of our framework\ntowards existing work."
                },
                "authors": [
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yisheng He"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Yuan Dong"
                    },
                    {
                        "name": "Weihao Yuan"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Zilong Dong"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10559v2",
                "updated": "2025-09-09T09:37:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    37,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-12T10:33:30Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    33,
                    30,
                    3,
                    163,
                    0
                ],
                "title": "From Images to Insights: Explainable Biodiversity Monitoring with Plain\n  Language Habitat Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Images to Insights: Explainable Biodiversity Monitoring with Plain\n  Language Habitat Explanations"
                },
                "summary": "Explaining why the species lives at a particular location is important for\nunderstanding ecological systems and conserving biodiversity. However, existing\necological workflows are fragmented and often inaccessible to non-specialists.\nWe propose an end-to-end visual-to-causal framework that transforms a species\nimage into interpretable causal insights about its habitat preference. The\nsystem integrates species recognition, global occurrence retrieval,\npseudo-absence sampling, and climate data extraction. We then discover causal\nstructures among environmental features and estimate their influence on species\noccurrence using modern causal inference methods. Finally, we generate\nstatistically grounded, human-readable causal explanations from structured\ntemplates and large language models. We demonstrate the framework on a bee and\na flower species and report early results as part of an ongoing project,\nshowing the potential of the multimodal AI assistant backed up by a recommended\necological modeling practice for describing species habitat in\nhuman-understandable language. Our code is available at:\nhttps://github.com/Yutong-Zhou-cv/BioX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining why the species lives at a particular location is important for\nunderstanding ecological systems and conserving biodiversity. However, existing\necological workflows are fragmented and often inaccessible to non-specialists.\nWe propose an end-to-end visual-to-causal framework that transforms a species\nimage into interpretable causal insights about its habitat preference. The\nsystem integrates species recognition, global occurrence retrieval,\npseudo-absence sampling, and climate data extraction. We then discover causal\nstructures among environmental features and estimate their influence on species\noccurrence using modern causal inference methods. Finally, we generate\nstatistically grounded, human-readable causal explanations from structured\ntemplates and large language models. We demonstrate the framework on a bee and\na flower species and report early results as part of an ongoing project,\nshowing the potential of the multimodal AI assistant backed up by a recommended\necological modeling practice for describing species habitat in\nhuman-understandable language. Our code is available at:\nhttps://github.com/Yutong-Zhou-cv/BioX."
                },
                "authors": [
                    {
                        "name": "Yutong Zhou"
                    },
                    {
                        "name": "Masahiro Ryo"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Ryo"
                },
                "author": "Masahiro Ryo",
                "arxiv_comment": "AISE workshop camera-ready version @ ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02659v2",
                "updated": "2025-09-09T09:31:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    31,
                    59,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-03T09:12:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    12,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Are Economists Always More Introverted? Analyzing Consistency in\n  Persona-Assigned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Economists Always More Introverted? Analyzing Consistency in\n  Persona-Assigned LLMs"
                },
                "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub."
                },
                "authors": [
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Bart Baesens"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "Accepted to EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07546v1",
                "updated": "2025-09-09T09:31:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    31,
                    25,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:31:25Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    31,
                    25,
                    1,
                    252,
                    0
                ],
                "title": "Differential Dynamic Programming for the Optimal Control Problem with an\n  Ellipsoidal Target Set and Its Statistical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Dynamic Programming for the Optimal Control Problem with an\n  Ellipsoidal Target Set and Its Statistical Inference"
                },
                "summary": "This work addresses an extended class of optimal control problems where a\ntarget for a system state has the form of an ellipsoid rather than a fixed,\nsingle point. As a computationally affordable method for resolving the extended\nproblem, we present a revised version of the differential dynamic programming\n(DDP), termed the differential dynamic programming with ellipsoidal target set\n(ETS-DDP). To this end, the problem with an ellipsoidal target set is\nreformulated into an equivalent form with the orthogonal projection operator,\nyielding that the resulting cost functions turn out to be discontinuous at some\npoints. As the DDP usually requires the differentiability of cost functions, in\nthe ETS-DDP formulation we locally approximate the (nonsmooth) cost functions\nto smoothed ones near the path generated at the previous iteration, by\nutilizing the explicit form of the orthogonal projection operator. Moreover, a\nstatistical inference method is also presented for designing the ellipsoidal\ntarget set, based on data on admissible target points collected by expert\ndemonstrations. Via a simulation on autonomous parking of a vehicle, it is seen\nthat the proposed ETS-DDP efficiently derives an admissible state trajectory\nwhile running much faster than the point-targeted DDP, at the expense of\noptimality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses an extended class of optimal control problems where a\ntarget for a system state has the form of an ellipsoid rather than a fixed,\nsingle point. As a computationally affordable method for resolving the extended\nproblem, we present a revised version of the differential dynamic programming\n(DDP), termed the differential dynamic programming with ellipsoidal target set\n(ETS-DDP). To this end, the problem with an ellipsoidal target set is\nreformulated into an equivalent form with the orthogonal projection operator,\nyielding that the resulting cost functions turn out to be discontinuous at some\npoints. As the DDP usually requires the differentiability of cost functions, in\nthe ETS-DDP formulation we locally approximate the (nonsmooth) cost functions\nto smoothed ones near the path generated at the previous iteration, by\nutilizing the explicit form of the orthogonal projection operator. Moreover, a\nstatistical inference method is also presented for designing the ellipsoidal\ntarget set, based on data on admissible target points collected by expert\ndemonstrations. Via a simulation on autonomous parking of a vehicle, it is seen\nthat the proposed ETS-DDP efficiently derives an admissible state trajectory\nwhile running much faster than the point-targeted DDP, at the expense of\noptimality."
                },
                "authors": [
                    {
                        "name": "Sungjun Eom"
                    },
                    {
                        "name": "Gyunghoon Park"
                    }
                ],
                "author_detail": {
                    "name": "Gyunghoon Park"
                },
                "author": "Gyunghoon Park",
                "arxiv_comment": "25th International Conference on Control, Automation and Systems\n  (ICCAS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19548v2",
                "updated": "2025-09-09T09:27:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    27,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-26T20:40:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    40,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "When Large Language Models Meet Speech: A Survey on Integration\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models Meet Speech: A Survey on Integration\n  Approaches"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor"
                },
                "authors": [
                    {
                        "name": "Zhengdong Yang"
                    },
                    {
                        "name": "Shuichiro Shimizu"
                    },
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Chenhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Chu"
                },
                "author": "Chenhui Chu",
                "arxiv_comment": "Accepted at Findings of ACL 2025 (Long Paper)",
                "arxiv_journal_ref": "Findings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07540v1",
                "updated": "2025-09-09T09:16:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    16,
                    45,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:16:45Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    16,
                    45,
                    1,
                    252,
                    0
                ],
                "title": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits\n  with LLM Generated Commits and Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits\n  with LLM Generated Commits and Embeddings"
                },
                "summary": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work."
                },
                "authors": [
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Yide Yin"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Shar Lwin Khin"
                    },
                    {
                        "name": "Ouh Eng Lieh"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.07980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07980v1",
                "updated": "2025-09-09T17:59:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
                },
                "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Huiwen Bao"
                    },
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07968v1",
                "updated": "2025-09-09T17:53:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    53,
                    58,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:53:58Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    53,
                    58,
                    1,
                    252,
                    0
                ],
                "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge"
                },
                "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified."
                },
                "authors": [
                    {
                        "name": "Lukas Haas"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Giovanni D'Antonio"
                    },
                    {
                        "name": "Sasha Goldshtein"
                    },
                    {
                        "name": "Dipanjan Das"
                    }
                ],
                "author_detail": {
                    "name": "Dipanjan Das"
                },
                "author": "Dipanjan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07966v1",
                "updated": "2025-09-09T17:52:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    52,
                    26,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:52:26Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    52,
                    26,
                    1,
                    252,
                    0
                ],
                "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images"
                },
                "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA."
                },
                "authors": [
                    {
                        "name": "Boammani Aser Lompo"
                    },
                    {
                        "name": "Marc Haraoui"
                    }
                ],
                "author_detail": {
                    "name": "Marc Haraoui"
                },
                "author": "Marc Haraoui",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07052v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07052v3",
                "updated": "2025-09-09T17:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    50,
                    59,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-11T15:14:02Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    14,
                    2,
                    0,
                    316,
                    0
                ],
                "title": "Ultra-Wideband Communications: Interference Challenges and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Wideband Communications: Interference Challenges and Solutions"
                },
                "summary": "The idea of ultra-wideband (UWB) communications for short ranges (up to a few\ntens of meters) has been around for nearly three decades. However, despite\nsignificant efforts by the industry, UWB deployment has not yet reached its\npredicted potential. This article, thus, seeks to rectify this situation by\nproviding a practical examination of UWB interference conditions. Through a\nspectrum survey of today's wireless environments, we explore the interference\nthat UWB devices may face from a perspective of outage probability in both\nhigh- and low-rate configurations. We find that by suppressing interference,\nthe outage probability can be reduced by one or more orders of magnitude. In\nthe non-line-of sight channels, in particular, we find that both interference\nsuppression and bandwidth expansion are required to support the minimum data\nrates suggested in the IEEE802.15.4 series of standards. We connect these\nfindings to a recently proposed UWB signaling method based on filter banks and\nshow this method fulfills the above requirements for implementing effective UWB\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The idea of ultra-wideband (UWB) communications for short ranges (up to a few\ntens of meters) has been around for nearly three decades. However, despite\nsignificant efforts by the industry, UWB deployment has not yet reached its\npredicted potential. This article, thus, seeks to rectify this situation by\nproviding a practical examination of UWB interference conditions. Through a\nspectrum survey of today's wireless environments, we explore the interference\nthat UWB devices may face from a perspective of outage probability in both\nhigh- and low-rate configurations. We find that by suppressing interference,\nthe outage probability can be reduced by one or more orders of magnitude. In\nthe non-line-of sight channels, in particular, we find that both interference\nsuppression and bandwidth expansion are required to support the minimum data\nrates suggested in the IEEE802.15.4 series of standards. We connect these\nfindings to a recently proposed UWB signaling method based on filter banks and\nshow this method fulfills the above requirements for implementing effective UWB\nsystems."
                },
                "authors": [
                    {
                        "name": "Brian Nelson"
                    },
                    {
                        "name": "Hussein Moradi"
                    },
                    {
                        "name": "Behrouz Farhang-Boroujeny"
                    }
                ],
                "author_detail": {
                    "name": "Behrouz Farhang-Boroujeny"
                },
                "author": "Behrouz Farhang-Boroujeny",
                "arxiv_comment": "9 pages, 6 figures, submitted to the IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07052v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21627v2",
                "updated": "2025-09-09T17:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    37,
                    26,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-27T18:02:12Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    2,
                    12,
                    1,
                    147,
                    0
                ],
                "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives"
                },
                "summary": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we develop an efficient heuristic algorithm that allows\nproviders to significantly overcharge users without raising suspicion.\nCrucially, we demonstrate that the cost of running the algorithm is lower than\nthe additional revenue from overcharging users, highlighting the vulnerability\nof users under the current pay-per-token pricing mechanism. Further, we show\nthat, to eliminate the financial incentive to strategize, a pricing mechanism\nmust price tokens linearly on their character count. While this makes a\nprovider's profit margin vary across tokens, we introduce a simple prescription\nunder which the provider who adopts such an incentive-compatible pricing\nmechanism can maintain the average profit margin they had under the\npay-per-token pricing mechanism. Along the way, to illustrate and complement\nour theoretical results, we conduct experiments with several large language\nmodels from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$\nfamilies, and input prompts from the LMSYS Chatbot Arena platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we develop an efficient heuristic algorithm that allows\nproviders to significantly overcharge users without raising suspicion.\nCrucially, we demonstrate that the cost of running the algorithm is lower than\nthe additional revenue from overcharging users, highlighting the vulnerability\nof users under the current pay-per-token pricing mechanism. Further, we show\nthat, to eliminate the financial incentive to strategize, a pricing mechanism\nmust price tokens linearly on their character count. While this makes a\nprovider's profit margin vary across tokens, we introduce a simple prescription\nunder which the provider who adopts such an incentive-compatible pricing\nmechanism can maintain the average profit margin they had under the\npay-per-token pricing mechanism. Along the way, to illustrate and complement\nour theoretical results, we conduct experiments with several large language\nmodels from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$\nfamilies, and input prompts from the LMSYS Chatbot Arena platform."
                },
                "authors": [
                    {
                        "name": "Ander Artola Velasco"
                    },
                    {
                        "name": "Stratis Tsirtsis"
                    },
                    {
                        "name": "Nastaran Okati"
                    },
                    {
                        "name": "Manuel Gomez-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gomez-Rodriguez"
                },
                "author": "Manuel Gomez-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07946v1",
                "updated": "2025-09-09T17:31:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    31,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:31:42Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    31,
                    42,
                    1,
                    252,
                    0
                ],
                "title": "Bringing Multi-Modal Multi-Task Federated Foundation Models to Education\n  Domain: Prospects and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Multi-Modal Multi-Task Federated Foundation Models to Education\n  Domain: Prospects and Challenges"
                },
                "summary": "Multi-modal multi-task (M3T) foundation models (FMs) have recently shown\ntransformative potential in artificial intelligence, with emerging applications\nin education. However, their deployment in real-world educational settings is\nhindered by privacy regulations, data silos, and limited domain-specific data\navailability. We introduce M3T Federated Foundation Models (FedFMs) for\neducation: a paradigm that integrates federated learning (FL) with M3T FMs to\nenable collaborative, privacy-preserving training across decentralized\ninstitutions while accommodating diverse modalities and tasks. Subsequently,\nthis position paper aims to unveil M3T FedFMs as a promising yet underexplored\napproach to the education community, explore its potentials, and reveal its\nrelated future research directions. We outline how M3T FedFMs can advance three\ncritical pillars of next-generation intelligent education systems: (i) privacy\npreservation, by keeping sensitive multi-modal student and institutional data\nlocal; (ii) personalization, through modular architectures enabling tailored\nmodels for students, instructors, and institutions; and (iii) equity and\ninclusivity, by facilitating participation from underrepresented and\nresource-constrained entities. We finally identify various open research\nchallenges, including studying of (i) inter-institution heterogeneous privacy\nregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)\nthe unlearning approaches for M3T FedFMs, (iv) the continual learning\nframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must\nbe collectively addressed for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal multi-task (M3T) foundation models (FMs) have recently shown\ntransformative potential in artificial intelligence, with emerging applications\nin education. However, their deployment in real-world educational settings is\nhindered by privacy regulations, data silos, and limited domain-specific data\navailability. We introduce M3T Federated Foundation Models (FedFMs) for\neducation: a paradigm that integrates federated learning (FL) with M3T FMs to\nenable collaborative, privacy-preserving training across decentralized\ninstitutions while accommodating diverse modalities and tasks. Subsequently,\nthis position paper aims to unveil M3T FedFMs as a promising yet underexplored\napproach to the education community, explore its potentials, and reveal its\nrelated future research directions. We outline how M3T FedFMs can advance three\ncritical pillars of next-generation intelligent education systems: (i) privacy\npreservation, by keeping sensitive multi-modal student and institutional data\nlocal; (ii) personalization, through modular architectures enabling tailored\nmodels for students, instructors, and institutions; and (iii) equity and\ninclusivity, by facilitating participation from underrepresented and\nresource-constrained entities. We finally identify various open research\nchallenges, including studying of (i) inter-institution heterogeneous privacy\nregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)\nthe unlearning approaches for M3T FedFMs, (iv) the continual learning\nframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must\nbe collectively addressed for practical deployment."
                },
                "authors": [
                    {
                        "name": "Kasra Borazjani"
                    },
                    {
                        "name": "Naji Khosravan"
                    },
                    {
                        "name": "Rajeev Sahay"
                    },
                    {
                        "name": "Bita Akram"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    }
                ],
                "author_detail": {
                    "name": "Seyyedali Hosseinalipour"
                },
                "author": "Seyyedali Hosseinalipour",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07941v1",
                "updated": "2025-09-09T17:21:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    21,
                    20,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:21:20Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    21,
                    20,
                    1,
                    252,
                    0
                ],
                "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented\n  Code Generation"
                },
                "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io."
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Chenxiong Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chenxiong Qian"
                },
                "author": "Chenxiong Qian",
                "arxiv_comment": "This paper has been accepted by the ACM Conference on Computer and\n  Communications Security (CCS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07939v1",
                "updated": "2025-09-09T17:19:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    33,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:19:33Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    19,
                    33,
                    1,
                    252,
                    0
                ],
                "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured\n  Attack Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured\n  Attack Trees"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"
                },
                "authors": [
                    {
                        "name": "Katsuaki Nakano"
                    },
                    {
                        "name": "Reza Feyyazi"
                    },
                    {
                        "name": "Shanchieh Jay Yang"
                    },
                    {
                        "name": "Michael Zuzak"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zuzak"
                },
                "author": "Michael Zuzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07933v1",
                "updated": "2025-09-09T17:17:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    17,
                    6,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:17:06Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    17,
                    6,
                    1,
                    252,
                    0
                ],
                "title": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation"
                },
                "summary": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security."
                },
                "authors": [
                    {
                        "name": "Wanni Vidulige Ishan Perera"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Fan liang"
                    },
                    {
                        "name": "Junyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Zhang"
                },
                "author": "Junyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07925v1",
                "updated": "2025-09-09T17:07:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    7,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T17:07:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    7,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models"
                },
                "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ."
                },
                "authors": [
                    {
                        "name": "Tuo Wang"
                    },
                    {
                        "name": "Adithya Kulkarni"
                    },
                    {
                        "name": "Tyler Cody"
                    },
                    {
                        "name": "Peter A. Beling"
                    },
                    {
                        "name": "Yujun Yan"
                    },
                    {
                        "name": "Dawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhou"
                },
                "author": "Dawei Zhou",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07909v1",
                "updated": "2025-09-09T16:53:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    53,
                    21,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:53:21Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    53,
                    21,
                    1,
                    252,
                    0
                ],
                "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Scaling Laws for Large Language Models via Inverse Problems"
                },
                "summary": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness."
                },
                "authors": [
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhiliang Chen"
                    },
                    {
                        "name": "Rachael Hwee Ling Sim"
                    },
                    {
                        "name": "Rui Qiao"
                    },
                    {
                        "name": "Jingtan Wang"
                    },
                    {
                        "name": "Nhung Bui"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "Zi-Yu Khoo"
                    },
                    {
                        "name": "Zitong Zhao"
                    },
                    {
                        "name": "Xinyi Xu"
                    },
                    {
                        "name": "Apivich Hemachandra"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted at EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07908v1",
                "updated": "2025-09-09T16:51:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    51,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:51:16Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    51,
                    16,
                    1,
                    252,
                    0
                ],
                "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories"
                },
                "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse."
                },
                "authors": [
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Debora Nozza"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21117v3",
                "updated": "2025-09-10T10:32:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    32,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-29T18:56:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    56,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts"
                },
                "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."
                },
                "authors": [
                    {
                        "name": "Hanhua Hong"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiqi Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "11 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07824v2",
                "updated": "2025-09-09T16:35:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    35,
                    39,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-09T14:48:43Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    14,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs"
                },
                "summary": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility."
                },
                "authors": [
                    {
                        "name": "Yao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yao Yan"
                },
                "author": "Yao Yan",
                "arxiv_comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR\n  May 2025 cycle, reviews pending)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v2",
                "updated": "2025-09-10T11:05:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    5,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07891v1",
                "updated": "2025-09-09T16:20:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    20,
                    36,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:20:36Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    20,
                    36,
                    1,
                    252,
                    0
                ],
                "title": "Unikernels vs. Containers: A Runtime-Level Performance Comparison for\n  Resource-Constrained Edge Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unikernels vs. Containers: A Runtime-Level Performance Comparison for\n  Resource-Constrained Edge Workloads"
                },
                "summary": "The choice between containers and unikernels is a critical trade-off for edge\napplications, balancing the container's ecosystem maturity against unikernel's\nspecialized efficiency. However, until now, how this trade-off behaves under\nthe severe memory constraints of industrial edge environments remains\ninsufficiently investigated, especially across different execution models. This\nwork presents an empirical comparison using Go and Node.js applications,\nrepresenting ahead-of-time (AOT) and just-in-time (JIT) compilation,\nrespectively. While unikernels consistently deliver faster startup times and\noutperform containers for Go-based workloads in resource-constrained\nenvironments, the evaluation results identify a critical performance crossover\nfor Node.js. Below a certain memory threshold, Docker containers maintain\nstable performance for both I/O-bound and CPU-bound applications, while the\nNanos unikernel's performance degrades sharply. This reveals that Linux's\nmemory management capabilities can outweigh the minimalist efficiency of\nunikernels under resource scarcity, a critical trade-off that, until now, has\nnot been adequately quantified for JIT runtimes in this context. These findings\ndemonstrate that the optimal deployment paradigm depends on both runtime\nbehavior and available system resources, underscoring the need for\nworkload-aware deployment strategies in edge computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The choice between containers and unikernels is a critical trade-off for edge\napplications, balancing the container's ecosystem maturity against unikernel's\nspecialized efficiency. However, until now, how this trade-off behaves under\nthe severe memory constraints of industrial edge environments remains\ninsufficiently investigated, especially across different execution models. This\nwork presents an empirical comparison using Go and Node.js applications,\nrepresenting ahead-of-time (AOT) and just-in-time (JIT) compilation,\nrespectively. While unikernels consistently deliver faster startup times and\noutperform containers for Go-based workloads in resource-constrained\nenvironments, the evaluation results identify a critical performance crossover\nfor Node.js. Below a certain memory threshold, Docker containers maintain\nstable performance for both I/O-bound and CPU-bound applications, while the\nNanos unikernel's performance degrades sharply. This reveals that Linux's\nmemory management capabilities can outweigh the minimalist efficiency of\nunikernels under resource scarcity, a critical trade-off that, until now, has\nnot been adequately quantified for JIT runtimes in this context. These findings\ndemonstrate that the optimal deployment paradigm depends on both runtime\nbehavior and available system resources, underscoring the need for\nworkload-aware deployment strategies in edge computing."
                },
                "authors": [
                    {
                        "name": "Hai Dinh-Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Hai Dinh-Tuan"
                },
                "author": "Hai Dinh-Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07128v2",
                "updated": "2025-09-09T16:20:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    20,
                    1,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-10T23:47:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    47,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping"
                },
                "summary": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse"
                },
                "authors": [
                    {
                        "name": "Danrui Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Sam S. Sohn"
                    },
                    {
                        "name": "Kaidong Hu"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Mubbasir Kapadia"
                    }
                ],
                "author_detail": {
                    "name": "Mubbasir Kapadia"
                },
                "author": "Mubbasir Kapadia",
                "arxiv_comment": "37 pages, 13 figures, 8 tables. Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20404v2",
                "updated": "2025-09-09T16:13:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    13,
                    55,
                    1,
                    252,
                    0
                ],
                "published": "2024-05-30T18:16:41Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    16,
                    41,
                    3,
                    151,
                    0
                ],
                "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoPA:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."
                },
                "authors": [
                    {
                        "name": "Yurui Chang"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Yujia Wang"
                    },
                    {
                        "name": "Jinghui Chen"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.20404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07889v1",
                "updated": "2025-09-09T16:12:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    12,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:12:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    12,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing"
                },
                "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask."
                },
                "authors": [
                    {
                        "name": "Chengyan Wu"
                    },
                    {
                        "name": "Yiqiang Cai"
                    },
                    {
                        "name": "Yufei Cheng"
                    },
                    {
                        "name": "Yun Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yun Xue"
                },
                "author": "Yun Xue",
                "arxiv_comment": "NLPCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19951v2",
                "updated": "2025-09-09T16:05:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    5,
                    18,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-25T16:28:24Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    28,
                    24,
                    1,
                    84,
                    0
                ],
                "title": "Audio-centric Video Understanding Benchmark without Text Shortcut",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-centric Video Understanding Benchmark without Text Shortcut"
                },
                "summary": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with\na particular focus on auditory information. AVUT introduces a suite of\ncarefully designed audio-centric tasks, holistically testing the understanding\nof both audio content and audio-visual interactions in videos. Moreover, this\nwork points out the text shortcut problem that largely exists in other\nbenchmarks where the correct answer can be found from question text alone\nwithout needing videos. AVUT addresses this problem by proposing a answer\npermutation-based filtering mechanism. A thorough evaluation across a diverse\nrange of open-source and proprietary multimodal LLMs is performed, followed by\nthe analyses of deficiencies in audio-visual LLMs. Demos and data are available\nat https://github.com/lark-png/AVUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with\na particular focus on auditory information. AVUT introduces a suite of\ncarefully designed audio-centric tasks, holistically testing the understanding\nof both audio content and audio-visual interactions in videos. Moreover, this\nwork points out the text shortcut problem that largely exists in other\nbenchmarks where the correct answer can be found from question text alone\nwithout needing videos. AVUT addresses this problem by proposing a answer\npermutation-based filtering mechanism. A thorough evaluation across a diverse\nrange of open-source and proprietary multimodal LLMs is performed, followed by\nthe analyses of deficiencies in audio-visual LLMs. Demos and data are available\nat https://github.com/lark-png/AVUT."
                },
                "authors": [
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted for publication in the Proceedings of EMNLP 2025 (Main\n  Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11790v2",
                "updated": "2025-09-09T16:05:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    5,
                    10,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-14T18:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    18,
                    27,
                    2,
                    4,
                    73,
                    0
                ],
                "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial\n  Planning in LMMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial\n  Planning in LMMs"
                },
                "summary": "Human reasoning relies on constructing and manipulating mental models --\nsimplified internal representations of situations that we use to understand and\nsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid\nreasoning) externalize these mental models, abstracting irrelevant details to\nefficiently capture how entities interact with each other. In contrast, Large\nLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason\nthrough text, limiting their effectiveness in complex multi-step tasks. In this\npaper, we propose Visual Thinking, a zero-shot framework that enables LMMs to\nreason through multiple chains of (self-generated) conceptual diagrams,\nsignificantly enhancing their combinatorial planning capabilities. Our approach\ndoes not require any human initialization beyond the natural language\ndescription of the task. It integrates both textual and diagrammatic reasoning\nwithin an optimized Graph-of-Thought inference framework, enhanced by beam\nsearch and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves LMMs' performance (e.g.,\nGPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other\ntext-only search-based inference methods. On more difficult planning domains\nwith solution depths up to 40, our approach outperforms even the o1-preview\nreasoning model (e.g., 16 percentage points improvement in Floor Tiles). These\nresults highlight the value of conceptual diagrams as a reasoning medium in\nLMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human reasoning relies on constructing and manipulating mental models --\nsimplified internal representations of situations that we use to understand and\nsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid\nreasoning) externalize these mental models, abstracting irrelevant details to\nefficiently capture how entities interact with each other. In contrast, Large\nLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason\nthrough text, limiting their effectiveness in complex multi-step tasks. In this\npaper, we propose Visual Thinking, a zero-shot framework that enables LMMs to\nreason through multiple chains of (self-generated) conceptual diagrams,\nsignificantly enhancing their combinatorial planning capabilities. Our approach\ndoes not require any human initialization beyond the natural language\ndescription of the task. It integrates both textual and diagrammatic reasoning\nwithin an optimized Graph-of-Thought inference framework, enhanced by beam\nsearch and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves LMMs' performance (e.g.,\nGPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other\ntext-only search-based inference methods. On more difficult planning domains\nwith solution depths up to 40, our approach outperforms even the o1-preview\nreasoning model (e.g., 16 percentage points improvement in Floor Tiles). These\nresults highlight the value of conceptual diagrams as a reasoning medium in\nLMMs."
                },
                "authors": [
                    {
                        "name": "Nasim Borazjanizadeh"
                    },
                    {
                        "name": "Roei Herzig"
                    },
                    {
                        "name": "Eduard Oks"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Karlinsky"
                },
                "author": "Leonid Karlinsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07879v1",
                "updated": "2025-09-09T16:00:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    0,
                    3,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T16:00:03Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    0,
                    3,
                    1,
                    252,
                    0
                ],
                "title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability\n  with Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Membership Inference Test (aMINT): Enhancing Model Auditability\n  with Multi-Task Learning"
                },
                "summary": "Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "In Proc. IEEE/CVF Intenational Conference on Computer Vision, ICCV,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07873v1",
                "updated": "2025-09-09T15:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    25,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:57:25Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    57,
                    25,
                    1,
                    252,
                    0
                ],
                "title": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through\n  Sentiment-based Backchannels and Active Listening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through\n  Sentiment-based Backchannels and Active Listening"
                },
                "summary": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Go-Eum Cha"
                    },
                    {
                        "name": "Sooyeon Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Sooyeon Jeong"
                },
                "author": "Sooyeon Jeong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07869v1",
                "updated": "2025-09-09T15:56:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:56:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "Are Humans as Brittle as Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Humans as Brittle as Large Language Models?"
                },
                "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07867v1",
                "updated": "2025-09-09T15:55:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    55,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:55:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    55,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models"
                },
                "summary": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise."
                },
                "authors": [
                    {
                        "name": "Augustin Crespin"
                    },
                    {
                        "name": "Ioannis Kostis"
                    },
                    {
                        "name": "Hélène Verhaeghe"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "arxiv_comment": "presented at\"LLMs meet Constraint Solving\" Workshop at CP2025 in\n  Glasgow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07864v1",
                "updated": "2025-09-09T15:51:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via\n  Layer-to-head Attention Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via\n  Layer-to-head Attention Diagnostics"
                },
                "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency."
                },
                "authors": [
                    {
                        "name": "Tiancheng Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Lijie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Hu"
                },
                "author": "Lijie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07860v1",
                "updated": "2025-09-09T15:40:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    40,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:40:23Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    40,
                    23,
                    1,
                    252,
                    0
                ],
                "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis"
                },
                "summary": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence."
                },
                "authors": [
                    {
                        "name": "Guanzhi Deng"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Yu-Keung Ng"
                    },
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Peijun Zheng"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Dapeng Wu"
                    },
                    {
                        "name": "Yinqiao Li"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07858v1",
                "updated": "2025-09-09T15:38:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    38,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:38:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    38,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data\n  Synthesizers to Empower Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data\n  Synthesizers to Empower Code LLMs"
                },
                "summary": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Linmei Hu"
                    },
                    {
                        "name": "Luhao Zhang"
                    },
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Haomin Fu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Mengdi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Zhang"
                },
                "author": "Mengdi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07846v1",
                "updated": "2025-09-09T15:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    22,
                    33,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    22,
                    33,
                    1,
                    252,
                    0
                ],
                "title": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A\n  Comparative RAG Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A\n  Comparative RAG Study"
                },
                "summary": "Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively."
                },
                "authors": [
                    {
                        "name": "Amay Jain"
                    },
                    {
                        "name": "Liu Cui"
                    },
                    {
                        "name": "Si Chen"
                    }
                ],
                "author_detail": {
                    "name": "Si Chen"
                },
                "author": "Si Chen",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21393v2",
                "updated": "2025-09-09T15:20:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    20,
                    7,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-29T08:14:38Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    38,
                    4,
                    241,
                    0
                ],
                "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs"
                },
                "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."
                },
                "authors": [
                    {
                        "name": "Guofu Liao"
                    },
                    {
                        "name": "Taotao Wang"
                    },
                    {
                        "name": "Shengli Zhang"
                    },
                    {
                        "name": "Jiqun Zhang"
                    },
                    {
                        "name": "Shi Long"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07829v1",
                "updated": "2025-09-09T15:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    7,
                    14,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:07:14Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    7,
                    14,
                    1,
                    252,
                    0
                ],
                "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost"
                },
                "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings."
                },
                "authors": [
                    {
                        "name": "Mihai Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    },
                    {
                        "name": "Andreea Tomescu"
                    },
                    {
                        "name": "Andrei Piscoran"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Piscoran"
                },
                "author": "Andrei Piscoran",
                "arxiv_comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07825v1",
                "updated": "2025-09-09T15:01:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    1,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T15:01:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    1,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language\n  Model"
                },
                "summary": "3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding."
                },
                "authors": [
                    {
                        "name": "Zhuoxu Huang"
                    },
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Jungong Han"
                    }
                ],
                "author_detail": {
                    "name": "Jungong Han"
                },
                "author": "Jungong Han",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02603v2",
                "updated": "2025-09-09T14:58:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    58,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-19T16:44:14Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    44,
                    14,
                    1,
                    324,
                    0
                ],
                "title": "Generative AI as a Tool for Enhancing Reflective Learning in Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI as a Tool for Enhancing Reflective Learning in Students"
                },
                "summary": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems."
                },
                "authors": [
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Jiazi Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiazi Hu"
                },
                "author": "Jiazi Hu",
                "arxiv_comment": "Accepted by IEEE TALE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07820v1",
                "updated": "2025-09-09T14:57:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    57,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:57:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    57,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking\n  Budget Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking\n  Budget Approach"
                },
                "summary": "The rise of large reasoning language models (LRLMs) has unlocked new\npotential for solving complex tasks. These models operate with a thinking\nbudget, that is, a predefined number of reasoning tokens used to arrive at a\nsolution. We propose a novel approach, inspired by the generator/discriminator\nframework in generative adversarial networks, in which a critic model\nperiodically probes its own reasoning to assess whether it has reached a\nconfident conclusion. If not, reasoning continues until a target certainty\nthreshold is met. This mechanism adaptively balances efficiency and reliability\nby allowing early termination when confidence is high, while encouraging\nfurther reasoning when uncertainty persists. Through experiments on the\nAIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)\nimproves baseline accuracy while reducing token usage. Importantly, extended\nmulti-seed evaluations over 64 runs demonstrate that CGR is stable, reducing\nvariance across seeds and improving exam-like performance under penalty-based\ngrading. Additionally, our token savings analysis shows that CGR can eliminate\nmillions of tokens in aggregate, with tunable trade-offs between certainty\nthresholds and efficiency. Together, these findings highlight certainty as a\npowerful signal for reasoning sufficiency. By integrating confidence into the\nreasoning process, CGR makes large reasoning language models more adaptive,\ntrustworthy, and resource efficient, paving the way for practical deployment in\ndomains where both accuracy and computational cost matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large reasoning language models (LRLMs) has unlocked new\npotential for solving complex tasks. These models operate with a thinking\nbudget, that is, a predefined number of reasoning tokens used to arrive at a\nsolution. We propose a novel approach, inspired by the generator/discriminator\nframework in generative adversarial networks, in which a critic model\nperiodically probes its own reasoning to assess whether it has reached a\nconfident conclusion. If not, reasoning continues until a target certainty\nthreshold is met. This mechanism adaptively balances efficiency and reliability\nby allowing early termination when confidence is high, while encouraging\nfurther reasoning when uncertainty persists. Through experiments on the\nAIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)\nimproves baseline accuracy while reducing token usage. Importantly, extended\nmulti-seed evaluations over 64 runs demonstrate that CGR is stable, reducing\nvariance across seeds and improving exam-like performance under penalty-based\ngrading. Additionally, our token savings analysis shows that CGR can eliminate\nmillions of tokens in aggregate, with tunable trade-offs between certainty\nthresholds and efficiency. Together, these findings highlight certainty as a\npowerful signal for reasoning sufficiency. By integrating confidence into the\nreasoning process, CGR makes large reasoning language models more adaptive,\ntrustworthy, and resource efficient, paving the way for practical deployment in\ndomains where both accuracy and computational cost matter."
                },
                "authors": [
                    {
                        "name": "João Paulo Nogueira"
                    },
                    {
                        "name": "Wentao Sun"
                    },
                    {
                        "name": "Alonso Silva"
                    },
                    {
                        "name": "Laith Zumot"
                    }
                ],
                "author_detail": {
                    "name": "Laith Zumot"
                },
                "author": "Laith Zumot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07819v1",
                "updated": "2025-09-09T14:56:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    56,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:56:37Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    56,
                    37,
                    1,
                    252,
                    0
                ],
                "title": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in\n  Knowledge Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in\n  Knowledge Communities"
                },
                "summary": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness."
                },
                "authors": [
                    {
                        "name": "Moyan Zhou"
                    },
                    {
                        "name": "Soobin Cho"
                    },
                    {
                        "name": "Loren Terveen"
                    }
                ],
                "author_detail": {
                    "name": "Loren Terveen"
                },
                "author": "Loren Terveen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07817v1",
                "updated": "2025-09-09T14:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    55,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:55:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    55,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems"
                },
                "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters."
                },
                "authors": [
                    {
                        "name": "Xiaolin Chen"
                    },
                    {
                        "name": "Xuemeng Song"
                    },
                    {
                        "name": "Haokun Wen"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07642v3",
                "updated": "2025-09-09T14:42:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    53,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-09T11:07:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    7,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient\n  LLM-based Scientific Peer Review"
                },
                "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."
                },
                "authors": [
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Yuanbo Kong"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Hayden Kwok-Hay So"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Liya Zhu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted to EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04575v2",
                "updated": "2025-09-09T14:42:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    48,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-04T18:01:00Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    18,
                    1,
                    0,
                    3,
                    247,
                    0
                ],
                "title": "Bootstrapping Task Spaces for Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Task Spaces for Self-Improvement"
                },
                "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training."
                },
                "authors": [
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16582v3",
                "updated": "2025-09-09T14:42:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    42,
                    44,
                    1,
                    252,
                    0
                ],
                "published": "2023-10-25T12:16:33Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    12,
                    16,
                    33,
                    2,
                    298,
                    0
                ],
                "title": "UPLex: Fine-Grained Personality Control in Large Language Models via\n  Unsupervised Lexical Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UPLex: Fine-Grained Personality Control in Large Language Models via\n  Unsupervised Lexical Modulation"
                },
                "summary": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities."
                },
                "authors": [
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07794v1",
                "updated": "2025-09-09T14:31:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    31,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:31:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    31,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey"
                },
                "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Xinxuan Lv"
                    },
                    {
                        "name": "Junjie Zou"
                    },
                    {
                        "name": "Tongna Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suchao An"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "arxiv_comment": "38 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18934v2",
                "updated": "2025-09-09T14:27:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    27,
                    58,
                    1,
                    252,
                    0
                ],
                "published": "2024-12-25T15:45:18Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    15,
                    45,
                    18,
                    2,
                    360,
                    0
                ],
                "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference"
                },
                "summary": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts."
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Baizhou Xu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Songzhu Mei"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18993v2",
                "updated": "2025-09-09T14:24:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    24,
                    47,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-26T09:56:51Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    51,
                    2,
                    57,
                    0
                ],
                "title": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering"
                },
                "summary": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures."
                },
                "authors": [
                    {
                        "name": "Teng Lin"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07768v1",
                "updated": "2025-09-09T14:01:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    1,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T14:01:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    1,
                    15,
                    1,
                    252,
                    0
                ],
                "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning"
                },
                "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct."
                },
                "authors": [
                    {
                        "name": "Michele Joshua Maggini"
                    },
                    {
                        "name": "Dhia Merzougui"
                    },
                    {
                        "name": "Rabiraj Bandyopadhyay"
                    },
                    {
                        "name": "Gaël Dias"
                    },
                    {
                        "name": "Fabrice Maurel"
                    },
                    {
                        "name": "Pablo Gamallo"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Gamallo"
                },
                "author": "Pablo Gamallo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08994v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08994v3",
                "updated": "2025-09-09T14:00:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    14,
                    0,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-03-12T02:07:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    7,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive\n  Neural Predicate Modulation"
                },
                "summary": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on learned cardinality estimation has made significant progress in\nrecent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We define these\nchallenges as the ``Trilemma of Cardinality Estimation'', where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod to estimate join cardinality by leveraging the probability distributions\nof individual tables in a decoupled manner. (2) To meet the requirements of\nefficiency for DistJoin, we develop Adaptive Neural Predicate Modulation\n(ANPM), a high-throughput distribution estimation model. (3) We demonstrate\nthat an existing similar approach suffers from variance accumulation issues by\nformal variance analysis. To mitigate this problem, DistJoin employs a\nselectivity-based approach to infer join cardinality, effectively reducing\nvariance. In summary, DistJoin not only represents the first data-driven method\nto support both equi and non-equi joins simultaneously but also demonstrates\nsuperior accuracy while enabling fast and flexible updates. The experimental\nresults demonstrate that DistJoin achieves the highest accuracy, robustness to\ndata updates, generality, and comparable update and inference speed relative to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Kaixin Zhang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    },
                    {
                        "name": "Ziqi Li"
                    },
                    {
                        "name": "Yabin Lu"
                    },
                    {
                        "name": "Yingze Li"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Yiming Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Guan"
                },
                "author": "Yiming Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08994v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08994v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07764v1",
                "updated": "2025-09-09T13:59:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    59,
                    0,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:59:00Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    59,
                    0,
                    1,
                    252,
                    0
                ],
                "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework\n  for Computer-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework\n  for Computer-Use Agents"
                },
                "summary": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."
                },
                "authors": [
                    {
                        "name": "Haitao Hu"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Yanpeng Zhao"
                    },
                    {
                        "name": "Yuqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Chen"
                },
                "author": "Yuqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07763v1",
                "updated": "2025-09-09T13:58:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:58:46Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects"
                },
                "summary": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals."
                },
                "authors": [
                    {
                        "name": "Mikel Robredo"
                    },
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Rafael Peñaloza"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Lenarduzzi"
                },
                "author": "Valentina Lenarduzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07759v1",
                "updated": "2025-09-09T13:57:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    57,
                    53,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:57:53Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    57,
                    53,
                    1,
                    252,
                    0
                ],
                "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Long-Document Retrieval in the PLM and LLM Era"
                },
                "summary": "The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Miyang Luo"
                    },
                    {
                        "name": "Tianrui Lv"
                    },
                    {
                        "name": "Yishuai Zhang"
                    },
                    {
                        "name": "Siqi Zhao"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou",
                "arxiv_comment": "33 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07755v1",
                "updated": "2025-09-09T13:54:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    54,
                    34,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:54:34Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    54,
                    34,
                    1,
                    252,
                    0
                ],
                "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts"
                },
                "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent."
                },
                "authors": [
                    {
                        "name": "Rochana Prih Hastuti"
                    },
                    {
                        "name": "Rian Adam Rajagede"
                    },
                    {
                        "name": "Mansour Al Ghanim"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16143v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16143v4",
                "updated": "2025-09-09T13:53:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    53,
                    24,
                    1,
                    252,
                    0
                ],
                "published": "2025-01-27T15:36:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    36,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments"
                },
                "summary": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met."
                },
                "authors": [
                    {
                        "name": "Marco Zambianco"
                    },
                    {
                        "name": "Silvio Cretti"
                    },
                    {
                        "name": "Domenico Siracusa"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Siracusa"
                },
                "author": "Domenico Siracusa",
                "arxiv_doi": "10.1109/TSC.2025.3604373",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TSC.2025.3604373",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.16143v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16143v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04656v2",
                "updated": "2025-09-09T13:49:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    49,
                    46,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-04T20:57:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    20,
                    57,
                    35,
                    3,
                    247,
                    0
                ],
                "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for\n  Arabic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for\n  Arabic LLMs"
                },
                "summary": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\nhttps://github.com/aishaalansari57/AraHalluEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\nhttps://github.com/aishaalansari57/AraHalluEval"
                },
                "authors": [
                    {
                        "name": "Aisha Alansari"
                    },
                    {
                        "name": "Hamzah Luqman"
                    }
                ],
                "author_detail": {
                    "name": "Hamzah Luqman"
                },
                "author": "Hamzah Luqman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16661v2",
                "updated": "2025-09-09T13:48:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    48,
                    45,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-22T13:27:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    27,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP"
                },
                "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."
                },
                "authors": [
                    {
                        "name": "Shinnosuke Ono"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Takuro Fujii"
                    },
                    {
                        "name": "Kosei Buma"
                    },
                    {
                        "name": "Shunsuke Sasaki"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Sasaki"
                },
                "author": "Shunsuke Sasaki",
                "arxiv_comment": "15 pages, 9 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01542v2",
                "updated": "2025-09-09T13:40:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    40,
                    40,
                    1,
                    252,
                    0
                ],
                "published": "2025-04-02T09:30:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    30,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Always Matters: Analysis of LLM Pretraining Data Through the\n  Lens of Language Variation"
                },
                "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices."
                },
                "authors": [
                    {
                        "name": "Amanda Myntti"
                    },
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Veronika Laippala"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07730v2",
                "updated": "2025-09-10T04:50:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    4,
                    50,
                    56,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T13:32:29Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    32,
                    29,
                    1,
                    252,
                    0
                ],
                "title": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models"
                },
                "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE."
                },
                "authors": [
                    {
                        "name": "Zexuan Li"
                    },
                    {
                        "name": "Hongliang Dai"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "arxiv_comment": "Accepted by EMNLP2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07727v1",
                "updated": "2025-09-09T13:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    28,
                    41,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:28:41Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    28,
                    41,
                    1,
                    252,
                    0
                ],
                "title": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?"
                },
                "summary": "With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy."
                },
                "authors": [
                    {
                        "name": "Songkai Ma"
                    },
                    {
                        "name": "Zhaorui Zhang"
                    },
                    {
                        "name": "Sheng Di"
                    },
                    {
                        "name": "Benben Liu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Xiaoyi Lu"
                    },
                    {
                        "name": "Dan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dan Wang"
                },
                "author": "Dan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07711v1",
                "updated": "2025-09-09T13:13:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    13,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T13:13:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    13,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced\n  Mathematical Reasoning"
                },
                "summary": "As large language models (LLMs) reach high scores on established mathematical\nbenchmarks, such as GSM8K and MATH, the research community has turned to\nInternational Mathematical Olympiad (IMO) problems to push the evaluation\nfrontier. However, existing Olympiad-level benchmarks suffer from practical\nconstraints that introduce grading noise and potential bias, such as\nheterogeneous answer formats requiring model-based judges and a reliance on\npotentially flawed solutions. We introduce RIMO, a two-track benchmark designed\nto preserve peak Olympiad difficulty while eliminating this evaluation noise.\nThe first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique\ninteger answer, allowing for deterministic correctness checking. The second\ntrack, RIMO-P, features 456 proof problems with expert-checked solutions, which\nare decomposed into a sequence of sub-problems to evaluate the step-by-step\nreasoning process via an automated grading system. Our benchmarking of ten\nfrontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these\nsystems excel on older benchmarks, their performance drops sharply on RIMO.\nThese results highlight a substantial gap between current LLM capabilities and\nactual Olympiad-level reasoning. By providing a challenging yet\neasy-to-evaluate suite, RIMO offers a high-resolution yardstick for future\nresearch, presenting a clear target for closing the profound reasoning gap our\nfindings expose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) reach high scores on established mathematical\nbenchmarks, such as GSM8K and MATH, the research community has turned to\nInternational Mathematical Olympiad (IMO) problems to push the evaluation\nfrontier. However, existing Olympiad-level benchmarks suffer from practical\nconstraints that introduce grading noise and potential bias, such as\nheterogeneous answer formats requiring model-based judges and a reliance on\npotentially flawed solutions. We introduce RIMO, a two-track benchmark designed\nto preserve peak Olympiad difficulty while eliminating this evaluation noise.\nThe first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique\ninteger answer, allowing for deterministic correctness checking. The second\ntrack, RIMO-P, features 456 proof problems with expert-checked solutions, which\nare decomposed into a sequence of sub-problems to evaluate the step-by-step\nreasoning process via an automated grading system. Our benchmarking of ten\nfrontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these\nsystems excel on older benchmarks, their performance drops sharply on RIMO.\nThese results highlight a substantial gap between current LLM capabilities and\nactual Olympiad-level reasoning. By providing a challenging yet\neasy-to-evaluate suite, RIMO offers a high-resolution yardstick for future\nresearch, presenting a clear target for closing the profound reasoning gap our\nfindings expose."
                },
                "authors": [
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Yao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Shu"
                },
                "author": "Yao Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v4",
                "updated": "2025-09-09T13:01:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    1,
                    7,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Xiaohua Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Jia"
                },
                "author": "Xiaohua Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06591v3",
                "updated": "2025-09-10T11:32:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    32,
                    27,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-08T12:02:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    2,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising"
                },
                "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications."
                },
                "authors": [
                    {
                        "name": "Yichao Liu"
                    },
                    {
                        "name": "Hengzhi Xue"
                    },
                    {
                        "name": "YueYang Teng"
                    }
                ],
                "author_detail": {
                    "name": "YueYang Teng"
                },
                "author": "YueYang Teng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16665v3",
                "updated": "2025-09-09T12:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    54,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-20T22:27:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    22,
                    27,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust but Verify! A Survey on Verification Design for Test-time Scaling"
                },
                "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io."
                },
                "authors": [
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07676v1",
                "updated": "2025-09-09T12:43:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    43,
                    28,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:43:28Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    43,
                    28,
                    1,
                    252,
                    0
                ],
                "title": "Unleashing the True Potential of LLMs: A Feedback-Triggered\n  Self-Correction with Long-Term Multipath Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the True Potential of LLMs: A Feedback-Triggered\n  Self-Correction with Long-Term Multipath Decoding"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods."
                },
                "authors": [
                    {
                        "name": "Jipeng Li"
                    },
                    {
                        "name": "Zeyu Gao"
                    },
                    {
                        "name": "Yubin Qi"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Weijian Chen"
                    },
                    {
                        "name": "Qiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Lin"
                },
                "author": "Qiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07671v1",
                "updated": "2025-09-09T12:37:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    37,
                    7,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:37:07Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    37,
                    7,
                    1,
                    252,
                    0
                ],
                "title": "Full-scale Microwave SQUID Multiplexer Readout System for Magnetic\n  Microcalorimeters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-scale Microwave SQUID Multiplexer Readout System for Magnetic\n  Microcalorimeters"
                },
                "summary": "The deployment of large cryogenic detector arrays, comprising hundreds to\nthousands of individual detectors, is highly beneficial for various\ncutting-edge applications, requiring large statistics, angular resolution or\nimaging capabilities. The readout of such arrays, however, presents a major\nchallenge in terms of system complexity, parasitic heat load, and cost, which\ncan be overcome only through multiplexing. Among the various multiplexing\napproaches, microwave SQUID multiplexing currently represents the state of the\nart, in particular for magnetic microcalorimeter (MMC) readout. In this work,\nwe demonstrate the successful operation of the latest generation of our\nmicrowave SQUID multiplexer-based readout system, based on a SQUID multiplexer\ntailored for MMC readout and a custom full-scale software-defined radio (SDR)\nelectronics, capable of handling up to 400 channels. The system operates\nreliably across the entire 4-8 GHz frequency band and achieves sufficiently low\nflux noise levels in flux-ramp-demodulated readout. Our results confirm that\nour system is fully functional and provides a scalable path towards future\nlarge-scale, high-resolution MMC experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large cryogenic detector arrays, comprising hundreds to\nthousands of individual detectors, is highly beneficial for various\ncutting-edge applications, requiring large statistics, angular resolution or\nimaging capabilities. The readout of such arrays, however, presents a major\nchallenge in terms of system complexity, parasitic heat load, and cost, which\ncan be overcome only through multiplexing. Among the various multiplexing\napproaches, microwave SQUID multiplexing currently represents the state of the\nart, in particular for magnetic microcalorimeter (MMC) readout. In this work,\nwe demonstrate the successful operation of the latest generation of our\nmicrowave SQUID multiplexer-based readout system, based on a SQUID multiplexer\ntailored for MMC readout and a custom full-scale software-defined radio (SDR)\nelectronics, capable of handling up to 400 channels. The system operates\nreliably across the entire 4-8 GHz frequency band and achieves sufficiently low\nflux noise levels in flux-ramp-demodulated readout. Our results confirm that\nour system is fully functional and provides a scalable path towards future\nlarge-scale, high-resolution MMC experiments."
                },
                "authors": [
                    {
                        "name": "M. Neidig"
                    },
                    {
                        "name": "T. Muscheid"
                    },
                    {
                        "name": "R. Gartmann"
                    },
                    {
                        "name": "L. E. Ardila Perez"
                    },
                    {
                        "name": "M. Wegner"
                    },
                    {
                        "name": "O. Sander"
                    },
                    {
                        "name": "S. Kempf"
                    }
                ],
                "author_detail": {
                    "name": "S. Kempf"
                },
                "author": "S. Kempf",
                "arxiv_comment": "Conference: LTD2025 (submitted to IEEE Transaction on Applied\n  Superconductivity)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06011v2",
                "updated": "2025-09-09T12:22:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    22,
                    18,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-07T10:59:02Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    10,
                    59,
                    2,
                    6,
                    250,
                    0
                ],
                "title": "Light-Weight Cross-Modal Enhancement Method with Benchmark Construction\n  for UAV-based Open-Vocabulary Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-Weight Cross-Modal Enhancement Method with Benchmark Construction\n  for UAV-based Open-Vocabulary Object Detection"
                },
                "summary": "Open-Vocabulary Object Detection (OVD) faces severe performance degradation\nwhen applied to UAV imagery due to the domain gap from ground-level datasets.\nTo address this challenge, we propose a complete UAV-oriented solution that\ncombines both dataset construction and model innovation. First, we design a\nrefined UAV-Label Engine, which efficiently resolves annotation redundancy,\ninconsistency, and ambiguity, enabling the generation of largescale UAV\ndatasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with\nover 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich\nimage-text pairs for vision-language pretraining. Second, we introduce the\nCross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion\ndesign that integrates cross-attention, adaptive gating, and global FiLM\nmodulation for robust textvision alignment. By embedding CAGE into the\nYOLO-World-v2 framework, our method achieves significant gains in both accuracy\nand efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP\nwhile reducing parameters and GFLOPs, and demonstrating strong cross-domain\ngeneralization on SIMD. Extensive experiments and real-world UAV deployment\nconfirm the effectiveness and practicality of our proposed solution for\nUAV-based OVD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Object Detection (OVD) faces severe performance degradation\nwhen applied to UAV imagery due to the domain gap from ground-level datasets.\nTo address this challenge, we propose a complete UAV-oriented solution that\ncombines both dataset construction and model innovation. First, we design a\nrefined UAV-Label Engine, which efficiently resolves annotation redundancy,\ninconsistency, and ambiguity, enabling the generation of largescale UAV\ndatasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with\nover 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich\nimage-text pairs for vision-language pretraining. Second, we introduce the\nCross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion\ndesign that integrates cross-attention, adaptive gating, and global FiLM\nmodulation for robust textvision alignment. By embedding CAGE into the\nYOLO-World-v2 framework, our method achieves significant gains in both accuracy\nand efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP\nwhile reducing parameters and GFLOPs, and demonstrating strong cross-domain\ngeneralization on SIMD. Extensive experiments and real-world UAV deployment\nconfirm the effectiveness and practicality of our proposed solution for\nUAV-based OVD"
                },
                "authors": [
                    {
                        "name": "Zhenhai Weng"
                    },
                    {
                        "name": "Xinjie Li"
                    },
                    {
                        "name": "Can Wu"
                    },
                    {
                        "name": "Weijie He"
                    },
                    {
                        "name": "Jianfeng Lv"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Zhongliang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Yu"
                },
                "author": "Zhongliang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07642v1",
                "updated": "2025-09-09T12:10:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    10,
                    14,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T12:10:14Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    10,
                    14,
                    1,
                    252,
                    0
                ],
                "title": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment"
                },
                "summary": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space."
                },
                "authors": [
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Oliver Müller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Müller"
                },
                "author": "Oliver Müller",
                "arxiv_comment": "Presented at the 19th International Conference on\n  Wirtschaftsinformatik 2024, W\\\"urzburg, Germany\n  https://aisel.aisnet.org/wi2024/91/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07622v1",
                "updated": "2025-09-09T11:52:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    52,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:52:16Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    52,
                    16,
                    1,
                    252,
                    0
                ],
                "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs"
                },
                "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians."
                },
                "authors": [
                    {
                        "name": "Libo Ren"
                    },
                    {
                        "name": "Yee Man Ng"
                    },
                    {
                        "name": "Lifeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Han"
                },
                "author": "Lifeng Han",
                "arxiv_comment": "system paper at CLEF 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07617v1",
                "updated": "2025-09-09T11:42:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    42,
                    6,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:42:06Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    42,
                    6,
                    1,
                    252,
                    0
                ],
                "title": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling"
                },
                "summary": "Direct Prompt Injection (DPI) attacks pose a critical security threat to\nLarge Language Models (LLMs) due to their low barrier of execution and high\npotential damage. To address the impracticality of existing white-box/gray-box\nmethods and the poor transferability of black-box methods, we propose an\nactivations-guided prompt injection attack framework. We first construct an\nEnergy-based Model (EBM) using activations from a surrogate model to evaluate\nthe quality of adversarial prompts. Guided by the trained EBM, we employ the\ntoken-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize\nadversarial prompts, thereby enabling gradient-free black-box attacks.\nExperimental results demonstrate our superior cross-model transferability,\nachieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%\nimprovement over human-crafted prompts, and maintaining 36.6% ASR on unseen\ntask scenarios. Interpretability analysis reveals a correlation between\nactivations and attack effectiveness, highlighting the critical role of\nsemantic patterns in transferable vulnerability exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Prompt Injection (DPI) attacks pose a critical security threat to\nLarge Language Models (LLMs) due to their low barrier of execution and high\npotential damage. To address the impracticality of existing white-box/gray-box\nmethods and the poor transferability of black-box methods, we propose an\nactivations-guided prompt injection attack framework. We first construct an\nEnergy-based Model (EBM) using activations from a surrogate model to evaluate\nthe quality of adversarial prompts. Guided by the trained EBM, we employ the\ntoken-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize\nadversarial prompts, thereby enabling gradient-free black-box attacks.\nExperimental results demonstrate our superior cross-model transferability,\nachieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%\nimprovement over human-crafted prompts, and maintaining 36.6% ASR on unseen\ntask scenarios. Interpretability analysis reveals a correlation between\nactivations and attack effectiveness, highlighting the critical role of\nsemantic patterns in transferable vulnerability exploitation."
                },
                "authors": [
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Yechao Zhang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "pei Xiaobing"
                    },
                    {
                        "name": "Jing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Wang"
                },
                "author": "Jing Wang",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07596v1",
                "updated": "2025-09-09T11:14:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    14,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:14:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    14,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation"
                },
                "summary": "Gender bias in vision-language foundation models (VLMs) raises concerns about\ntheir safe deployment and is typically evaluated using benchmarks with gender\nannotations on real-world images. However, as these benchmarks often contain\nspurious correlations between gender and non-gender features, such as objects\nand backgrounds, we identify a critical oversight in gender bias evaluation: Do\nspurious features distort gender bias evaluation? To address this question, we\nsystematically perturb non-gender features across four widely used benchmarks\n(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact\non bias evaluation. Our findings reveal that even minimal perturbations, such\nas masking just 10% of objects or weakly blurring backgrounds, can dramatically\nalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in\nCLIP variants. This suggests that current bias evaluations often reflect model\nresponses to spurious features rather than gender bias, undermining their\nreliability. Since creating spurious feature-free benchmarks is fundamentally\nchallenging, we recommend reporting bias metrics alongside feature-sensitivity\nmeasurements to enable a more reliable bias assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender bias in vision-language foundation models (VLMs) raises concerns about\ntheir safe deployment and is typically evaluated using benchmarks with gender\nannotations on real-world images. However, as these benchmarks often contain\nspurious correlations between gender and non-gender features, such as objects\nand backgrounds, we identify a critical oversight in gender bias evaluation: Do\nspurious features distort gender bias evaluation? To address this question, we\nsystematically perturb non-gender features across four widely used benchmarks\n(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact\non bias evaluation. Our findings reveal that even minimal perturbations, such\nas masking just 10% of objects or weakly blurring backgrounds, can dramatically\nalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in\nCLIP variants. This suggests that current bias evaluations often reflect model\nresponses to spurious features rather than gender bias, undermining their\nreliability. Since creating spurious feature-free benchmarks is fundamentally\nchallenging, we recommend reporting bias metrics alongside feature-sensitivity\nmeasurements to enable a more reliable bias assessment."
                },
                "authors": [
                    {
                        "name": "Yusuke Hirota"
                    },
                    {
                        "name": "Ryo Hachiuma"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Michael Ross Boone"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Noa Garcia"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chao-Han Huck Yang"
                },
                "author": "Chao-Han Huck Yang",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07595v1",
                "updated": "2025-09-09T11:07:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    7,
                    50,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:07:50Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    7,
                    50,
                    1,
                    252,
                    0
                ],
                "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\n  FaaS-hosted MCP Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\n  FaaS-hosted MCP Services"
                },
                "summary": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows."
                },
                "authors": [
                    {
                        "name": "Shiva Sai Krishna Anand Tokal"
                    },
                    {
                        "name": "Vaibhav Jha"
                    },
                    {
                        "name": "Anand Eswaran"
                    },
                    {
                        "name": "Praveen Jayachandran"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07594v1",
                "updated": "2025-09-09T11:06:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    6,
                    37,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T11:06:37Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    6,
                    37,
                    1,
                    252,
                    0
                ],
                "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction"
                },
                "summary": "Click-through rate (CTR) prediction plays an important role in online\nadvertising systems. On the one hand, traditional CTR prediction models capture\nthe collaborative signals in tabular data via feature interaction modeling, but\nthey lose semantics in text. On the other hand, Large Language Models (LLMs)\nexcel in understanding the context and meaning behind text, but they face\nchallenges in capturing collaborative signals and they have long inference\nlatency. In this paper, we aim to leverage the benefits of both types of models\nand pursue collaboration, semantics and efficiency. We present ELEC, which is\nan Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for\nthe CTR prediction task. In order to leverage the ability of the LLM but\nsimultaneously keep efficiency, we utilize the pseudo-siamese network which\ncontains a gain network and a vanilla network. We inject the high-level\nrepresentation vector generated by the LLM into a collaborative CTR model to\nform the gain network such that it can take advantage of both tabular modeling\nand textual modeling. However, its reliance on the LLM limits its efficiency.\nWe then distill the knowledge from the gain network to the vanilla network on\nboth the score level and the representation level, such that the vanilla\nnetwork takes only tabular data as input, but can still generate comparable\nperformance as the gain network. Our approach is model-agnostic. It allows for\nthe integration with various existing LLMs and collaborative CTR models.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof ELEC for CTR prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-through rate (CTR) prediction plays an important role in online\nadvertising systems. On the one hand, traditional CTR prediction models capture\nthe collaborative signals in tabular data via feature interaction modeling, but\nthey lose semantics in text. On the other hand, Large Language Models (LLMs)\nexcel in understanding the context and meaning behind text, but they face\nchallenges in capturing collaborative signals and they have long inference\nlatency. In this paper, we aim to leverage the benefits of both types of models\nand pursue collaboration, semantics and efficiency. We present ELEC, which is\nan Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for\nthe CTR prediction task. In order to leverage the ability of the LLM but\nsimultaneously keep efficiency, we utilize the pseudo-siamese network which\ncontains a gain network and a vanilla network. We inject the high-level\nrepresentation vector generated by the LLM into a collaborative CTR model to\nform the gain network such that it can take advantage of both tabular modeling\nand textual modeling. However, its reliance on the LLM limits its efficiency.\nWe then distill the knowledge from the gain network to the vanilla network on\nboth the score level and the representation level, such that the vanilla\nnetwork takes only tabular data as input, but can still generate comparable\nperformance as the gain network. Our approach is model-agnostic. It allows for\nthe integration with various existing LLMs and collaborative CTR models.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof ELEC for CTR prediction."
                },
                "authors": [
                    {
                        "name": "Rui Dong"
                    },
                    {
                        "name": "Wentao Ouyang"
                    },
                    {
                        "name": "Xiangzheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzheng Liu"
                },
                "author": "Xiangzheng Liu",
                "arxiv_doi": "10.1145/3726302.3730188",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730188",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.07594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03303v2",
                "updated": "2025-09-09T11:02:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    2,
                    55,
                    1,
                    252,
                    0
                ],
                "published": "2025-05-06T08:36:01Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "title": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices"
                },
                "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms."
                },
                "authors": [
                    {
                        "name": "Tasnim Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Shahriar"
                },
                "author": "Tasnim Shahriar",
                "arxiv_comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-XX (Primary) 68Txx, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07588v1",
                "updated": "2025-09-09T10:59:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    59,
                    47,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T10:59:47Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    59,
                    47,
                    1,
                    252,
                    0
                ],
                "title": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment"
                },
                "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts."
                },
                "authors": [
                    {
                        "name": "Andrey Sakhovskiy"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_doi": "10.1145/3726302.3729901",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729901",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.07588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"",
                "arxiv_journal_ref": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (2025). Association for\n  Computing Machinery, 1152-1164",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12791v2",
                "updated": "2025-09-09T10:55:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    55,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2024-06-18T09:15:46Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    15,
                    46,
                    1,
                    170,
                    0
                ],
                "title": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour."
                },
                "authors": [
                    {
                        "name": "Qikai Wei"
                    },
                    {
                        "name": "Mingzhi Yang"
                    },
                    {
                        "name": "Jinqiang Wang"
                    },
                    {
                        "name": "Wenwei Mao"
                    },
                    {
                        "name": "Jiabo Xu"
                    },
                    {
                        "name": "Huansheng Ning"
                    }
                ],
                "author_detail": {
                    "name": "Huansheng Ning"
                },
                "author": "Huansheng Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06469v2",
                "updated": "2025-09-09T10:50:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    50,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-08T09:30:22Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    30,
                    22,
                    0,
                    251,
                    0
                ],
                "title": "Interactive Shaping of Granular Media Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Shaping of Granular Media Using Reinforcement Learning"
                },
                "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy."
                },
                "authors": [
                    {
                        "name": "Benedikt Kreis"
                    },
                    {
                        "name": "Malte Mosbach"
                    },
                    {
                        "name": "Anny Ripke"
                    },
                    {
                        "name": "Muhammad Ehsan Ullah"
                    },
                    {
                        "name": "Sven Behnke"
                    },
                    {
                        "name": "Maren Bennewitz"
                    }
                ],
                "author_detail": {
                    "name": "Maren Bennewitz"
                },
                "author": "Maren Bennewitz",
                "arxiv_comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19667v3",
                "updated": "2025-09-09T10:38:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    38,
                    54,
                    1,
                    252,
                    0
                ],
                "published": "2025-04-28T10:43:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "Tripartite-GraphRAG via Plugin Ontologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tripartite-GraphRAG via Plugin Ontologies"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs."
                },
                "authors": [
                    {
                        "name": "Michael Banf"
                    },
                    {
                        "name": "Johannes Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Kuhn"
                },
                "author": "Johannes Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07391v3",
                "updated": "2025-09-09T10:16:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    16,
                    0,
                    1,
                    252,
                    0
                ],
                "published": "2024-12-10T10:33:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "A Data-Free Analytical Quantization Scheme for Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-Free Analytical Quantization Scheme for Deep Learning Models"
                },
                "summary": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource-constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on real-world datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource-constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on real-world datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy."
                },
                "authors": [
                    {
                        "name": "Ahmed Luqman"
                    },
                    {
                        "name": "Khuzemah Qazi"
                    },
                    {
                        "name": "Murray Patterson"
                    },
                    {
                        "name": "Malik Jahan Khan"
                    },
                    {
                        "name": "Imdadullah Khan"
                    }
                ],
                "author_detail": {
                    "name": "Imdadullah Khan"
                },
                "author": "Imdadullah Khan",
                "arxiv_comment": "Accepted for publication in IEEE International Conference on Data\n  Mining (ICDM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07571v1",
                "updated": "2025-09-09T10:15:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    15,
                    42,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T10:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    10,
                    15,
                    42,
                    1,
                    252,
                    0
                ],
                "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive\n  and Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive\n  and Efficient Inference"
                },
                "summary": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches."
                },
                "authors": [
                    {
                        "name": "Xiyu Guo"
                    },
                    {
                        "name": "Shan Wang"
                    },
                    {
                        "name": "Chunfang Ji"
                    },
                    {
                        "name": "Xuefeng Zhao"
                    },
                    {
                        "name": "Wenhao Xi"
                    },
                    {
                        "name": "Yaoyao Liu"
                    },
                    {
                        "name": "Qinglan Li"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21141v2",
                "updated": "2025-09-09T09:54:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    54,
                    15,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-28T18:18:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    18,
                    18,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Adaptive LLM Routing under Budget Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive LLM Routing under Budget Constraints"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their varying capabilities and costs pose challenges in practical\napplications. LLM routing addresses this by dynamically selecting the most\nsuitable LLM for each query/task. Previous approaches treat this as a\nsupervised learning problem, assuming complete knowledge of optimal query-LLM\npairings. However, real-world scenarios lack such comprehensive mappings and\nface evolving user queries. We thus propose to study LLM routing as a\ncontextual bandit problem, enabling adaptive decision-making using bandit\nfeedback without requiring exhaustive inference across all LLMs for all queries\n(in contrast to supervised routing). To address this problem, we develop a\nshared embedding space for queries and LLMs, where query and LLM embeddings are\naligned to reflect their affinity. This space is initially learned from offline\nhuman preference data and refined through online bandit feedback. We\ninstantiate this idea through Preference-prior Informed Linucb fOr adaptive\nrouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets\nfor model routing, we introduce an online cost policy modeled as a multi-choice\nknapsack problem, ensuring resource-efficient routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their varying capabilities and costs pose challenges in practical\napplications. LLM routing addresses this by dynamically selecting the most\nsuitable LLM for each query/task. Previous approaches treat this as a\nsupervised learning problem, assuming complete knowledge of optimal query-LLM\npairings. However, real-world scenarios lack such comprehensive mappings and\nface evolving user queries. We thus propose to study LLM routing as a\ncontextual bandit problem, enabling adaptive decision-making using bandit\nfeedback without requiring exhaustive inference across all LLMs for all queries\n(in contrast to supervised routing). To address this problem, we develop a\nshared embedding space for queries and LLMs, where query and LLM embeddings are\naligned to reflect their affinity. This space is initially learned from offline\nhuman preference data and refined through online bandit feedback. We\ninstantiate this idea through Preference-prior Informed Linucb fOr adaptive\nrouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets\nfor model routing, we introduce an online cost policy modeled as a multi-choice\nknapsack problem, ensuring resource-efficient routing."
                },
                "authors": [
                    {
                        "name": "Pranoy Panda"
                    },
                    {
                        "name": "Raghav Magazine"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    },
                    {
                        "name": "Sho Takemori"
                    },
                    {
                        "name": "Vishal Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Sharma"
                },
                "author": "Vishal Sharma",
                "arxiv_comment": "Accepted at EMNLP 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07558v1",
                "updated": "2025-09-09T09:52:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    52,
                    34,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:52:34Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    52,
                    34,
                    1,
                    252,
                    0
                ],
                "title": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR"
                },
                "summary": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization."
                },
                "authors": [
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07555v1",
                "updated": "2025-09-09T09:49:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    49,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:49:23Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    49,
                    23,
                    1,
                    252,
                    0
                ],
                "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition"
                },
                "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xiangrong Zhu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Wei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hu"
                },
                "author": "Wei Hu",
                "arxiv_comment": "Accepted in EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02659v2",
                "updated": "2025-09-09T09:31:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    31,
                    59,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-03T09:12:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    12,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Are Economists Always More Introverted? Analyzing Consistency in\n  Persona-Assigned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Economists Always More Introverted? Analyzing Consistency in\n  Persona-Assigned LLMs"
                },
                "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub."
                },
                "authors": [
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Bart Baesens"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "Accepted to EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19548v2",
                "updated": "2025-09-09T09:27:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    27,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-02-26T20:40:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    40,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "When Large Language Models Meet Speech: A Survey on Integration\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models Meet Speech: A Survey on Integration\n  Approaches"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor"
                },
                "authors": [
                    {
                        "name": "Zhengdong Yang"
                    },
                    {
                        "name": "Shuichiro Shimizu"
                    },
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Chenhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Chu"
                },
                "author": "Chenhui Chu",
                "arxiv_comment": "Accepted at Findings of ACL 2025 (Long Paper)",
                "arxiv_journal_ref": "Findings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07540v1",
                "updated": "2025-09-09T09:16:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    16,
                    45,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:16:45Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    16,
                    45,
                    1,
                    252,
                    0
                ],
                "title": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits\n  with LLM Generated Commits and Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits\n  with LLM Generated Commits and Embeddings"
                },
                "summary": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work."
                },
                "authors": [
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Yide Yin"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Shar Lwin Khin"
                    },
                    {
                        "name": "Ouh Eng Lieh"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07526v1",
                "updated": "2025-09-09T09:01:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    1,
                    1,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T09:01:01Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    9,
                    1,
                    1,
                    1,
                    252,
                    0
                ],
                "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage\n  Training on Public Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Audio-Language Models with Data-Efficient Single-Stage\n  Training on Public Data"
                },
                "summary": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data."
                },
                "authors": [
                    {
                        "name": "Gokul Karthik Kumar"
                    },
                    {
                        "name": "Rishabh Saraf"
                    },
                    {
                        "name": "Ludovick Lepauloux"
                    },
                    {
                        "name": "Abdul Muneer"
                    },
                    {
                        "name": "Billel Mokeddem"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "arxiv_comment": "Accepted at ASRU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06472v2",
                "updated": "2025-09-09T08:54:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    54,
                    11,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-08T09:37:20Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    37,
                    20,
                    0,
                    251,
                    0
                ],
                "title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for\n  Dynamic Retrieval and Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for\n  Dynamic Retrieval and Reranking"
                },
                "summary": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy."
                },
                "authors": [
                    {
                        "name": "Haoxiang Jin"
                    },
                    {
                        "name": "Ronghan Li"
                    },
                    {
                        "name": "Zixiang Lu"
                    },
                    {
                        "name": "Qiguang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qiguang Miao"
                },
                "author": "Qiguang Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07512v1",
                "updated": "2025-09-09T08:47:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    47,
                    13,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:47:13Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    47,
                    13,
                    1,
                    252,
                    0
                ],
                "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval"
                },
                "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal."
                },
                "authors": [
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Weize Wu"
                    },
                    {
                        "name": "Qiji Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07506v1",
                "updated": "2025-09-09T08:39:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    39,
                    50,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:39:50Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    39,
                    50,
                    1,
                    252,
                    0
                ],
                "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization"
                },
                "summary": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Tianran Sun"
                    },
                    {
                        "name": "Yogesh Seenichamy"
                    },
                    {
                        "name": "Hang Song"
                    },
                    {
                        "name": "Anne Ouyang"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09553v3",
                "updated": "2025-09-09T08:36:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    36,
                    52,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-14T16:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "OOD-SEG: Exploiting out-of-distribution detection techniques for\n  learning image segmentation from sparse multi-class positive-only annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOD-SEG: Exploiting out-of-distribution detection techniques for\n  learning image segmentation from sparse multi-class positive-only annotations"
                },
                "summary": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach which broadly falls within the\npositive-unlabelled (PU) learning paradigm and exploits tools from OOD\ndetection techniques. Our framework learns only from sparsely annotated pixels\nfrom multiple positive-only classes and does not use any annotation for the\nbackground class. These multi-class positive annotations naturally fall within\nthe in-distribution (ID) set. Unlabelled pixels may contain positive classes\nbut also negative ones, including what is typically referred to as\n\\emph{background} in standard segmentation formulations. Here, we forgo the\nneed for background annotation and consider these together with any other\nunseen classes as part of the OOD set. Our framework can integrate, at a\npixel-level, any OOD detection approaches designed for classification tasks. To\naddress the lack of existing OOD datasets and established evaluation metric for\nmedical image segmentation, we propose a cross-validation strategy that treats\nheld-out labelled classes as OOD. Extensive experiments on both multi-class\nhyperspectral and RGB surgical imaging datasets demonstrate the robustness and\ngeneralisation capability of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach which broadly falls within the\npositive-unlabelled (PU) learning paradigm and exploits tools from OOD\ndetection techniques. Our framework learns only from sparsely annotated pixels\nfrom multiple positive-only classes and does not use any annotation for the\nbackground class. These multi-class positive annotations naturally fall within\nthe in-distribution (ID) set. Unlabelled pixels may contain positive classes\nbut also negative ones, including what is typically referred to as\n\\emph{background} in standard segmentation formulations. Here, we forgo the\nneed for background annotation and consider these together with any other\nunseen classes as part of the OOD set. Our framework can integrate, at a\npixel-level, any OOD detection approaches designed for classification tasks. To\naddress the lack of existing OOD datasets and established evaluation metric for\nmedical image segmentation, we propose a cross-validation strategy that treats\nheld-out labelled classes as OOD. Extensive experiments on both multi-class\nhyperspectral and RGB surgical imaging datasets demonstrate the robustness and\ngeneralisation capability of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Junwen Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Oscar MacCormac"
                    },
                    {
                        "name": "Jonathan Shapey"
                    },
                    {
                        "name": "Tom Vercauteren"
                    }
                ],
                "author_detail": {
                    "name": "Tom Vercauteren"
                },
                "author": "Tom Vercauteren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07497v1",
                "updated": "2025-09-09T08:23:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    23,
                    12,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:23:12Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    23,
                    12,
                    1,
                    252,
                    0
                ],
                "title": "DREAMS: Decentralized Resource Allocation and Service Management across\n  the Compute Continuum Using Service Affinity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DREAMS: Decentralized Resource Allocation and Service Management across\n  the Compute Continuum Using Service Affinity"
                },
                "summary": "Modern manufacturing systems require adaptive computing infrastructures that\ncan respond to highly dynamic workloads and increasingly customized production\ndemands. The compute continuum emerges as a promising solution, enabling\nflexible deployment of microservices across distributed, heterogeneous domains.\nHowever, this paradigm also requires a novel approach to resource allocation\nand service placement, as traditional centralized solutions struggle to scale\neffectively, suffer from latency bottlenecks, and introduce single points of\nfailure. In this paper, we present DREAMS, a decentralized framework that\noptimizes microservice placement decisions collaboratively across different\ncomputational domains. At its core, DREAMS introduces agents that operate\nautonomously within each domain while coordinating globally through a\nRaft-based consensus algorithm and cost-benefit voting. This decentralized\narchitecture enables responsive, privacy-preserving, and fault-tolerant\ncoordination, making it particularly suitable given the growing prevalence of\nmulti-stakeholder scenarios across the compute continuum. In particular, within\nmodern manufacturing environments, DREAMS achieves globally optimized service\nplacements while maintaining high fault tolerance. Further evaluations\ndemonstrate that key coordination operations, such as Local Domain Manager\n(LDM) registration and migration voting, scale sub-linearly with the number of\ndomains, confirming the efficiency and scalability of our proposal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern manufacturing systems require adaptive computing infrastructures that\ncan respond to highly dynamic workloads and increasingly customized production\ndemands. The compute continuum emerges as a promising solution, enabling\nflexible deployment of microservices across distributed, heterogeneous domains.\nHowever, this paradigm also requires a novel approach to resource allocation\nand service placement, as traditional centralized solutions struggle to scale\neffectively, suffer from latency bottlenecks, and introduce single points of\nfailure. In this paper, we present DREAMS, a decentralized framework that\noptimizes microservice placement decisions collaboratively across different\ncomputational domains. At its core, DREAMS introduces agents that operate\nautonomously within each domain while coordinating globally through a\nRaft-based consensus algorithm and cost-benefit voting. This decentralized\narchitecture enables responsive, privacy-preserving, and fault-tolerant\ncoordination, making it particularly suitable given the growing prevalence of\nmulti-stakeholder scenarios across the compute continuum. In particular, within\nmodern manufacturing environments, DREAMS achieves globally optimized service\nplacements while maintaining high fault tolerance. Further evaluations\ndemonstrate that key coordination operations, such as Local Domain Manager\n(LDM) registration and migration voting, scale sub-linearly with the number of\ndomains, confirming the efficiency and scalability of our proposal."
                },
                "authors": [
                    {
                        "name": "Hai Dinh-Tuan"
                    },
                    {
                        "name": "Tien Hung Nguyen"
                    },
                    {
                        "name": "Sanjeet Raj Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeet Raj Pandey"
                },
                "author": "Sanjeet Raj Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07495v1",
                "updated": "2025-09-09T08:20:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    20,
                    19,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:20:19Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    20,
                    19,
                    1,
                    252,
                    0
                ],
                "title": "Generating Transferrable Adversarial Examples via Local Mixing and\n  Logits Optimization for Remote Sensing Object Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Transferrable Adversarial Examples via Local Mixing and\n  Logits Optimization for Remote Sensing Object Recognition"
                },
                "summary": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing\nsignificant security threats to their deployment in remote sensing\napplications. Research on adversarial attacks not only reveals model\nvulnerabilities but also provides critical insights for enhancing robustness.\nAlthough current mixing-based strategies have been proposed to increase the\ntransferability of adversarial examples, they either perform global blending or\ndirectly exchange a region in the images, which may destroy global semantic\nfeatures and mislead the optimization of adversarial examples. Furthermore,\ntheir reliance on cross-entropy loss for perturbation optimization leads to\ngradient diminishing during iterative updates, compromising adversarial example\nquality. To address these limitations, we focus on non-targeted attacks and\npropose a novel framework via local mixing and logits optimization. First, we\npresent a local mixing strategy to generate diverse yet semantically consistent\ninputs. Different from MixUp, which globally blends two images, and MixCut,\nwhich stitches images together, our method merely blends local regions to\npreserve global semantic information. Second, we adapt the logit loss from\ntargeted attacks to non-targeted scenarios, mitigating the gradient vanishing\nproblem of cross-entropy loss. Third, a perturbation smoothing loss is applied\nto suppress high-frequency noise and enhance transferability. Extensive\nexperiments on FGSCR-42 and MTARSI datasets demonstrate superior performance\nover 12 state-of-the-art methods across 6 surrogate models. Notably, with\nResNet as the surrogate on MTARSI, our method achieves a 17.28% average\nimprovement in black-box attack success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing\nsignificant security threats to their deployment in remote sensing\napplications. Research on adversarial attacks not only reveals model\nvulnerabilities but also provides critical insights for enhancing robustness.\nAlthough current mixing-based strategies have been proposed to increase the\ntransferability of adversarial examples, they either perform global blending or\ndirectly exchange a region in the images, which may destroy global semantic\nfeatures and mislead the optimization of adversarial examples. Furthermore,\ntheir reliance on cross-entropy loss for perturbation optimization leads to\ngradient diminishing during iterative updates, compromising adversarial example\nquality. To address these limitations, we focus on non-targeted attacks and\npropose a novel framework via local mixing and logits optimization. First, we\npresent a local mixing strategy to generate diverse yet semantically consistent\ninputs. Different from MixUp, which globally blends two images, and MixCut,\nwhich stitches images together, our method merely blends local regions to\npreserve global semantic information. Second, we adapt the logit loss from\ntargeted attacks to non-targeted scenarios, mitigating the gradient vanishing\nproblem of cross-entropy loss. Third, a perturbation smoothing loss is applied\nto suppress high-frequency noise and enhance transferability. Extensive\nexperiments on FGSCR-42 and MTARSI datasets demonstrate superior performance\nover 12 state-of-the-art methods across 6 surrogate models. Notably, with\nResNet as the surrogate on MTARSI, our method achieves a 17.28% average\nimprovement in black-box attack success rate."
                },
                "authors": [
                    {
                        "name": "Chun Liu"
                    },
                    {
                        "name": "Hailong Wang"
                    },
                    {
                        "name": "Bingqian Zhu"
                    },
                    {
                        "name": "Panpan Ding"
                    },
                    {
                        "name": "Zheng Zheng"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Zhigang Han"
                    },
                    {
                        "name": "Jiayao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayao Wang"
                },
                "author": "Jiayao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07492v1",
                "updated": "2025-09-09T08:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    16,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    16,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "Constraint-Compliant Network Optimization through Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Compliant Network Optimization through Large Language Models"
                },
                "summary": "This work develops an LLM-based optimization framework ensuring strict\nconstraint satisfaction in network optimization. While LLMs possess contextual\nreasoning capabilities, existing approaches often fail to enforce constraints,\ncausing infeasible solutions. Unlike conventional methods that address average\nconstraints, the proposed framework integrates a natural language-based input\nencoding strategy to restrict the solution space and guarantee feasibility. For\nmulti-access edge computing networks, task allocation is optimized while\nminimizing worst-case latency. Numerical evaluations demonstrate LLMs as a\npromising tool for constraint-aware network optimization, offering insights\ninto their inference capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work develops an LLM-based optimization framework ensuring strict\nconstraint satisfaction in network optimization. While LLMs possess contextual\nreasoning capabilities, existing approaches often fail to enforce constraints,\ncausing infeasible solutions. Unlike conventional methods that address average\nconstraints, the proposed framework integrates a natural language-based input\nencoding strategy to restrict the solution space and guarantee feasibility. For\nmulti-access edge computing networks, task allocation is optimized while\nminimizing worst-case latency. Numerical evaluations demonstrate LLMs as a\npromising tool for constraint-aware network optimization, offering insights\ninto their inference capabilities."
                },
                "authors": [
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Wookjin Lee"
                    },
                    {
                        "name": "Hong Ki Kim"
                    },
                    {
                        "name": "Sang Hyun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Hyun Lee"
                },
                "author": "Sang Hyun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07485v1",
                "updated": "2025-09-09T08:05:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    5,
                    16,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T08:05:16Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    5,
                    16,
                    1,
                    252,
                    0
                ],
                "title": "Multi-view-guided Passage Reranking with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view-guided Passage Reranking with Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have shown impressive\nperformance in passage reranking tasks. Despite their success, LLM-based\nmethods still face challenges in efficiency and sensitivity to external biases.\n(1) Existing models rely mostly on autoregressive generation and sliding window\nstrategies to rank passages, which incur heavy computational overhead as the\nnumber of passages increases. (2) External biases, such as position or\nselection bias, hinder the model's ability to accurately represent passages and\nincrease input-order sensitivity. To address these limitations, we introduce a\nnovel passage reranking model, called Multi-View-guided Passage Reranking\n(MVP). MVP is a non-generative LLM-based reranking method that encodes\nquery-passage information into diverse view embeddings without being influenced\nby external biases. For each view, it combines query-aware passage embeddings\nto produce a distinct anchor vector, which is then used to directly compute\nrelevance scores in a single decoding step. In addition, it employs an\northogonal loss to make the views more distinctive. Extensive experiments\ndemonstrate that MVP, with just 220M parameters, matches the performance of\nmuch larger 7B-scale fine-tuned models while achieving a 100x reduction in\ninference latency. Notably, the 3B-parameter variant of MVP achieves\nstate-of-the-art performance on both in-domain and out-of-domain benchmarks.\nThe source code is available at: https://github.com/bulbna/MVP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown impressive\nperformance in passage reranking tasks. Despite their success, LLM-based\nmethods still face challenges in efficiency and sensitivity to external biases.\n(1) Existing models rely mostly on autoregressive generation and sliding window\nstrategies to rank passages, which incur heavy computational overhead as the\nnumber of passages increases. (2) External biases, such as position or\nselection bias, hinder the model's ability to accurately represent passages and\nincrease input-order sensitivity. To address these limitations, we introduce a\nnovel passage reranking model, called Multi-View-guided Passage Reranking\n(MVP). MVP is a non-generative LLM-based reranking method that encodes\nquery-passage information into diverse view embeddings without being influenced\nby external biases. For each view, it combines query-aware passage embeddings\nto produce a distinct anchor vector, which is then used to directly compute\nrelevance scores in a single decoding step. In addition, it employs an\northogonal loss to make the views more distinctive. Extensive experiments\ndemonstrate that MVP, with just 220M parameters, matches the performance of\nmuch larger 7B-scale fine-tuned models while achieving a 100x reduction in\ninference latency. Notably, the 3B-parameter variant of MVP achieves\nstate-of-the-art performance on both in-domain and out-of-domain benchmarks.\nThe source code is available at: https://github.com/bulbna/MVP"
                },
                "authors": [
                    {
                        "name": "Jeongwoo Na"
                    },
                    {
                        "name": "Jun Kwon"
                    },
                    {
                        "name": "Eunseong Choi"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15512v3",
                "updated": "2025-09-09T08:04:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    4,
                    9,
                    1,
                    252,
                    0
                ],
                "published": "2025-07-21T11:28:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    28,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models"
                },
                "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Yonghao Shi"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by EMNLP 2025. Code: https://github.com/Lucky-259/Hybrid_TTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04431v3",
                "updated": "2025-09-09T08:03:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    3,
                    49,
                    1,
                    252,
                    0
                ],
                "published": "2025-07-06T15:31:01Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    31,
                    1,
                    6,
                    187,
                    0
                ],
                "title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedGellan: LLM-Generated Medical Guidance to Support Physicians"
                },
                "summary": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score."
                },
                "authors": [
                    {
                        "name": "Debodeep Banerjee"
                    },
                    {
                        "name": "Burcu Sayin"
                    },
                    {
                        "name": "Stefano Teso"
                    },
                    {
                        "name": "Andrea Passerini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Passerini"
                },
                "author": "Andrea Passerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07475v1",
                "updated": "2025-09-09T07:58:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T07:58:46Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    58,
                    46,
                    1,
                    252,
                    0
                ],
                "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with\n  Calibrated NLI Ensembles and Abstention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with\n  Calibrated NLI Ensembles and Abstention"
                },
                "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements."
                },
                "authors": [
                    {
                        "name": "Saumya Goswami"
                    },
                    {
                        "name": "Siddharth Kurra"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Kurra"
                },
                "author": "Siddharth Kurra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03020v3",
                "updated": "2025-09-09T07:39:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    39,
                    23,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-03T04:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    55,
                    26,
                    2,
                    246,
                    0
                ],
                "title": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction"
                },
                "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales."
                },
                "authors": [
                    {
                        "name": "Chang Su"
                    },
                    {
                        "name": "Dengliang Shi"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Jintao Du"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "accepted by EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05062v2",
                "updated": "2025-09-09T07:31:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    31,
                    56,
                    1,
                    252,
                    0
                ],
                "published": "2025-06-05T14:06:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    6,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation"
                },
                "summary": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task."
                },
                "authors": [
                    {
                        "name": "Noy Sternlicht"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Noam Slonim"
                    }
                ],
                "author_detail": {
                    "name": "Noam Slonim"
                },
                "author": "Noam Slonim",
                "arxiv_comment": "EMNLP 2025. Code:\n  https://github.com/noy-sternlicht/Debatable-Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05602v2",
                "updated": "2025-09-09T07:15:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    15,
                    51,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-06T05:33:17Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    33,
                    17,
                    5,
                    249,
                    0
                ],
                "title": "Mitigating Spurious Correlations Between Question and Answer via\n  Chain-of-Thought Correctness Perception Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Spurious Correlations Between Question and Answer via\n  Chain-of-Thought Correctness Perception Distillation"
                },
                "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Hongyan Xie"
                    },
                    {
                        "name": "Yitong Yao"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Zixuan Huang"
                    },
                    {
                        "name": "Deqing Wang"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Haoxiang Su"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shuangyong Song"
                    }
                ],
                "author_detail": {
                    "name": "Shuangyong Song"
                },
                "author": "Shuangyong Song",
                "arxiv_comment": "PrePrint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07445v1",
                "updated": "2025-09-09T07:10:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    10,
                    39,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T07:10:39Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    7,
                    10,
                    39,
                    1,
                    252,
                    0
                ],
                "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward\n  Functions"
                },
                "summary": "Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website"
                },
                "authors": [
                    {
                        "name": "Harrison Field"
                    },
                    {
                        "name": "Max Yang"
                    },
                    {
                        "name": "Yijiong Lin"
                    },
                    {
                        "name": "Efi Psomopoulou"
                    },
                    {
                        "name": "David Barton"
                    },
                    {
                        "name": "Nathan F. Lepora"
                    }
                ],
                "author_detail": {
                    "name": "Nathan F. Lepora"
                },
                "author": "Nathan F. Lepora",
                "arxiv_comment": "Accepted at CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07436v1",
                "updated": "2025-09-09T06:48:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    6,
                    48,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T06:48:05Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    6,
                    48,
                    5,
                    1,
                    252,
                    0
                ],
                "title": "SA-OOSC: A Multimodal LLM-Distilled Semantic Communication Framework for\n  Enhanced Coding Efficiency with Scenario Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SA-OOSC: A Multimodal LLM-Distilled Semantic Communication Framework for\n  Enhanced Coding Efficiency with Scenario Understanding"
                },
                "summary": "This paper introduces SA-OOSC, a multimodal large language models\n(MLLM)-distilled semantic communication framework that achieves efficient\nsemantic coding with scenario-aware importance allocations. This approach\naddresses a critical limitation of existing object-oriented semantic\ncommunication (OOSC) systems - assigning static importance values to specific\nclasses of objects regardless of their contextual relevance. Our framework\nutilizes MLLMs to identify the scenario-augmented (SA) semantic importance for\nobjects within the image. Through knowledge distillation with the\nMLLM-annotated data, our vectorization/de-vectorization networks and JSCC\nencoder/decoder learn to dynamically allocate coding resources based on\ncontextual significance, i.e., distinguishing between high-importance objects\nand low-importance according to the SA scenario information of the task. The\nframework features three core innovations: a MLLM-guided knowledge distillation\npipeline, an importance-weighted variable-length JSCC framework, and novel loss\nfunction designs that facilitate the knowledge distillation within the JSCC\nframework. Experimental validation demonstrates our framework's superior coding\nefficiency over conventional semantic communication systems, with open-sourced\nMLLM-annotated and human-verified datasets established as new benchmarks for\nfuture research in semantic communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SA-OOSC, a multimodal large language models\n(MLLM)-distilled semantic communication framework that achieves efficient\nsemantic coding with scenario-aware importance allocations. This approach\naddresses a critical limitation of existing object-oriented semantic\ncommunication (OOSC) systems - assigning static importance values to specific\nclasses of objects regardless of their contextual relevance. Our framework\nutilizes MLLMs to identify the scenario-augmented (SA) semantic importance for\nobjects within the image. Through knowledge distillation with the\nMLLM-annotated data, our vectorization/de-vectorization networks and JSCC\nencoder/decoder learn to dynamically allocate coding resources based on\ncontextual significance, i.e., distinguishing between high-importance objects\nand low-importance according to the SA scenario information of the task. The\nframework features three core innovations: a MLLM-guided knowledge distillation\npipeline, an importance-weighted variable-length JSCC framework, and novel loss\nfunction designs that facilitate the knowledge distillation within the JSCC\nframework. Experimental validation demonstrates our framework's superior coding\nefficiency over conventional semantic communication systems, with open-sourced\nMLLM-annotated and human-verified datasets established as new benchmarks for\nfuture research in semantic communications."
                },
                "authors": [
                    {
                        "name": "Feifan Zhang"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Yifan Xiang"
                    },
                    {
                        "name": "Xiaoyan Liu"
                    },
                    {
                        "name": "Soung Chang Liew"
                    }
                ],
                "author_detail": {
                    "name": "Soung Chang Liew"
                },
                "author": "Soung Chang Liew",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20368v3",
                "updated": "2025-09-09T06:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    6,
                    38,
                    41,
                    1,
                    252,
                    0
                ],
                "published": "2025-08-28T02:31:17Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    2,
                    31,
                    17,
                    3,
                    240,
                    0
                ],
                "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal\n  Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal\n  Multi-Objective Reinforcement Learning"
                },
                "summary": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains."
                },
                "authors": [
                    {
                        "name": "Lang Mei"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Chong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chong Chen"
                },
                "author": "Chong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07430v1",
                "updated": "2025-09-09T06:34:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    6,
                    34,
                    32,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T06:34:32Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    6,
                    34,
                    32,
                    1,
                    252,
                    0
                ],
                "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward"
                },
                "summary": "A central paradox in fine-tuning Large Language Models (LLMs) with\nReinforcement Learning with Verifiable Reward (RLVR) is the frequent\ndegradation of multi-attempt performance (Pass@k) despite improvements in\nsingle-attempt accuracy (Pass@1). This is often accompanied by catastrophic\nforgetting, where models lose previously acquired skills. While various methods\nhave been proposed, the choice and function of the divergence term have been\nsurprisingly unexamined as a proactive solution. We argue that standard RLVR\nobjectives -- both those using the mode-seeking reverse KL-divergence and those\nforgoing a divergence term entirely -- lack a crucial mechanism for knowledge\nretention. The reverse-KL actively accelerates this decay by narrowing the\npolicy, while its absence provides no safeguard against the model drifting from\nits diverse knowledge base. We propose a fundamental shift in perspective:\nusing the divergence term itself as the solution. Our framework,\nDiversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences\n(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By\ncontinuously referencing the initial policy, this approach forces the model to\nmaintain broad solution coverage. Extensive experiments on math and SQL\ngeneration demonstrate that DPH-RL not only resolves the Pass@k degradation but\nimproves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is\nmore training-efficient because it computes f-divergence using generator\nfunctions, requiring only sampling from the initial policy and no online\nreference model. Our work highlights a crucial, overlooked axis for improving\nRLVR, demonstrating that the proper selection of a divergence measure is a\npowerful tool for building more general and diverse reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central paradox in fine-tuning Large Language Models (LLMs) with\nReinforcement Learning with Verifiable Reward (RLVR) is the frequent\ndegradation of multi-attempt performance (Pass@k) despite improvements in\nsingle-attempt accuracy (Pass@1). This is often accompanied by catastrophic\nforgetting, where models lose previously acquired skills. While various methods\nhave been proposed, the choice and function of the divergence term have been\nsurprisingly unexamined as a proactive solution. We argue that standard RLVR\nobjectives -- both those using the mode-seeking reverse KL-divergence and those\nforgoing a divergence term entirely -- lack a crucial mechanism for knowledge\nretention. The reverse-KL actively accelerates this decay by narrowing the\npolicy, while its absence provides no safeguard against the model drifting from\nits diverse knowledge base. We propose a fundamental shift in perspective:\nusing the divergence term itself as the solution. Our framework,\nDiversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences\n(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By\ncontinuously referencing the initial policy, this approach forces the model to\nmaintain broad solution coverage. Extensive experiments on math and SQL\ngeneration demonstrate that DPH-RL not only resolves the Pass@k degradation but\nimproves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is\nmore training-efficient because it computes f-divergence using generator\nfunctions, requiring only sampling from the initial policy and no online\nreference model. Our work highlights a crucial, overlooked axis for improving\nRLVR, demonstrating that the proper selection of a divergence measure is a\npowerful tool for building more general and diverse reasoning models."
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Jiaran Hao"
                    },
                    {
                        "name": "Jason Klein Liu"
                    },
                    {
                        "name": "Zhijian Zhou"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "author": "Yuan Qi",
                "arxiv_comment": "26 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]